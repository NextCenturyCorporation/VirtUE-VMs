diff -urN linux-4.18.20/arch/x86/include/asm/bug.h linux-4.18.20-new/arch/x86/include/asm/bug.h
--- linux-4.18.20/arch/x86/include/asm/bug.h	2019-03-12 13:36:52.522649970 -0400
+++ linux-4.18.20-new/arch/x86/include/asm/bug.h	2019-03-12 13:37:16.958908626 -0400
@@ -40,7 +40,7 @@
 		     "\t.word %c2"        "\t# bug_entry::flags\n"	\
 		     "\t.org 2b+%c3\n"					\
 		     ".popsection"					\
-		     : : "i" (__FILE__), "i" (__LINE__),		\
+		     : : "X" (__FILE__), "i" (__LINE__),		\
 			 "i" (flags),					\
 			 "i" (sizeof(struct bug_entry)));		\
 } while (0)
diff -urN linux-4.18.20/arch/x86/include/asm/elf.h linux-4.18.20-new/arch/x86/include/asm/elf.h
--- linux-4.18.20/arch/x86/include/asm/elf.h	2019-03-12 13:36:57.822706123 -0400
+++ linux-4.18.20-new/arch/x86/include/asm/elf.h	2019-03-12 13:37:16.958908626 -0400
@@ -62,7 +62,8 @@
 #define R_X86_64_PC16		13	/* 16 bit sign extended pc relative */
 #define R_X86_64_8		14	/* Direct 8 bit sign extended  */
 #define R_X86_64_PC8		15	/* 8 bit sign extended pc relative */
-
+#define R_X86_64_GOTOFF64	25
+#define R_X86_64_GOTPC32	26
 #define R_X86_64_GOTPCRELX	41
 #define R_X86_64_REX_GOTPCRELX	42
 
diff -urN linux-4.18.20/arch/x86/include/asm/module.h linux-4.18.20-new/arch/x86/include/asm/module.h
--- linux-4.18.20/arch/x86/include/asm/module.h	2019-03-12 13:36:57.822706123 -0400
+++ linux-4.18.20-new/arch/x86/include/asm/module.h	2019-03-12 13:37:16.958908626 -0400
@@ -4,6 +4,70 @@
 
 #include <asm-generic/module.h>
 #include <asm/orc_types.h>
+#include <smr/smr.h>
+
+/////////////////////////////////////////////////
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+void *module_rerandomize(struct module *mod);
+
+#define __FILENAME__ (strrchr(__FILE__, '/') ? strrchr(__FILE__, '/') + 1 : __FILE__)
+#define printp(x) printk("(%s.%03d): " #x " = 0x%lx\n", __FILENAME__, __LINE__, (unsigned long)(x))
+#define INC_BY_DELTA(x, delta) ( x = (typeof((x))) ((unsigned long)(x) + (unsigned long)(delta)) )
+
+
+/* Places the variable in a special section
+ * Makes visibility default */
+#define SPECIAL_VAR(x) x __attribute__ ((section (".fixed.data")))
+#define SPECIAL_CONST_VAR(x) x __attribute__ ((section (".fixed.rodata")))
+
+
+#define SPECIAL_FUNCTION_PROTO(ret, name, args...)  \
+	noinline ret __attribute__ ((section (".fixed.text"))) __attribute__((naked)) name(args)
+
+#define SPECIAL_FUNCTION(ret, name, args...) \
+_Pragma("GCC diagnostic push") \
+_Pragma("GCC diagnostic ignored \"-Wreturn-type\"") \
+_Pragma("GCC diagnostic ignored \"-Wattributes\"") \
+ret __attribute__ ((visibility("hidden"))) name## _ ##real(args);\
+SPECIAL_FUNCTION_PROTO(ret, name, args) {  \
+	/* Save Args*/ \
+	asm ("push %rdi"); \
+	asm ("push %rsi"); \
+	asm ("push %rdx"); \
+	asm ("push %rcx"); \
+	asm ("push %r8"); \
+	asm ("push %r9"); \
+	asm ("call smr_enter;");                      \
+    asm ("mov %rdx, %r11"); \
+	/* Restore Args*/ \
+	asm ("pop %r9"); \
+	asm ("pop %r8"); \
+	asm ("pop %rcx"); \
+	asm ("pop %rdx"); \
+	asm ("pop %rsi"); \
+	asm ("pop %rdi"); \
+	/* Save smr_enter return value*/ \
+	asm ("push %rax"); \
+    asm ("push %r11"); \
+	asm ("movabs $%c[addr], %%rax;" : : [addr] "i" (name## _ ##real)); \
+    asm ("call *%rax;");                      \
+    asm ("pop %rsi"); \
+    asm ("pop %rdi"); \
+    asm ("push %rax"); \
+    asm ("call smr_leave;");                      \
+	asm ("pop %rax"); \
+    asm ("ret");  \
+    return; \
+} \
+_Pragma("GCC diagnostic pop") \
+ret name## _ ##real(args)
+#else
+#define SPECIAL_VAR(x) x
+#define SPECIAL_CONST_VAR(x) x
+#define SPECIAL_FUNCTION_PROTO(ret, name, args...) ret name (args)
+#define SPECIAL_FUNCTION(ret, name, args...) ret name (args)
+#endif
+/////////////////////////////////////////////////
 
 extern const char __THUNK_FOR_PLT[];
 extern const unsigned int __THUNK_FOR_PLT_SIZE;
@@ -20,14 +84,11 @@
 #endif
 } __packed __aligned(PLT_ENTRY_ALIGNMENT);
 
-struct mod_got_sec {
+struct mod_sec {
 	struct elf64_shdr	*got;
+	struct elf64_shdr	*plt;
 	int			got_num_entries;
 	int			got_max_entries;
-};
-
-struct mod_plt_sec {
-	struct elf64_shdr	*plt;
 	int			plt_num_entries;
 	int			plt_max_entries;
 };
@@ -38,8 +99,8 @@
 	int *orc_unwind_ip;
 	struct orc_entry *orc_unwind;
 #endif
-	struct mod_got_sec	core;
-	struct mod_plt_sec	core_plt;
+	struct mod_sec	core;
+	struct mod_sec	rand;
 };
 
 #ifdef CONFIG_X86_64
diff -urN linux-4.18.20/arch/x86/Kconfig linux-4.18.20-new/arch/x86/Kconfig
--- linux-4.18.20/arch/x86/Kconfig	2019-03-12 13:36:57.818706079 -0400
+++ linux-4.18.20-new/arch/x86/Kconfig	2019-03-12 13:37:16.958908626 -0400
@@ -2238,6 +2238,22 @@
 	select DYNAMIC_MODULE_BASE
 	select MODULE_REL_CRCS if MODVERSIONS
 
+config X86_MODULE_RERANDOMIZE
+	bool
+	prompt "Enable X86 modules rerandomization"
+	depends on X86_PIC && KALLSYMS_ALL && RANDOMIZE_BASE
+	default y
+	---help---
+	  Allow runtime rerandomization of modules.
+
+config X86_MODULE_RERANDOMIZER
+	tristate
+	prompt "Module Rerandomization Trigger"
+	depends on X86_MODULE_RERANDOMIZE
+	default m
+	---help---
+	  Creates a thread for module randomization.
+
 config X86_PIC
 	bool
 	prompt "Enable PIC modules"
diff -urN linux-4.18.20/arch/x86/kernel/module.c linux-4.18.20-new/arch/x86/kernel/module.c
--- linux-4.18.20/arch/x86/kernel/module.c	2019-03-12 13:36:57.822706123 -0400
+++ linux-4.18.20-new/arch/x86/kernel/module.c	2019-03-12 13:37:16.958908626 -0400
@@ -41,6 +41,13 @@
 
 static unsigned int module_plt_size;
 
+static int apply_relocate_add__(Elf64_Shdr *sechdrs,
+		   const char *strtab,
+		   unsigned int symindex,
+		   unsigned int relsec,
+		   struct module *me,
+		   bool check);
+
 #if 0
 #define DEBUGP(fmt, ...)				\
 	printk(KERN_DEBUG fmt, ##__VA_ARGS__)
@@ -63,11 +70,12 @@
 	if (kaslr_enabled()) {
 		mutex_lock(&module_kaslr_mutex);
 		/*
-		 * Calculate the module_load_offset the first time this
-		 * code is called. Once calculated it stays the same until
-		 * reboot.
+		 * Recalculate the module_load_offset only when
+		 * rerandomization is enabled.
 		 */
+#ifndef CONFIG_X86_MODULE_RERANDOMIZE
 		if (module_load_offset == 0)
+#endif
 			module_load_offset =
 				(get_random_int() % 1024 + 1) * PAGE_SIZE;
 		mutex_unlock(&module_kaslr_mutex);
@@ -105,10 +113,308 @@
 	return sym->st_shndx != SHN_UNDEF;
 }
 
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+static void module_print_addresses(struct module *mod);
+
+static char *module_get_section_name(struct module *mod, unsigned int shnum)
+{
+	if (shnum == SHN_UNDEF || shnum > mod->klp_info->hdr.e_shnum)
+		return "";
+
+	return mod->klp_info->secstrings +
+				mod->klp_info->sechdrs[shnum].sh_name;
+}
+
+bool module_is_fixed_section_name(const char *sname){
+	return (strstarts(sname, ".fixed")
+			|| strstarts(sname, ".gnu.linkonce.this_module")
+			|| strstarts(sname, "__param")
+			|| strstarts(sname, ".rodata")
+			|| strstarts(sname, ".data.rel.ro")
+		);
+}
+
+bool module_is_fixed_section(struct module *mod, unsigned int shnum)
+{
+	char *sname;
+
+	if (shnum == SHN_UNDEF || shnum > mod->klp_info->hdr.e_shnum)
+		return true;
+
+	sname = mod->klp_info->secstrings +
+				mod->klp_info->sechdrs[shnum].sh_name;
+
+	return module_is_fixed_section_name(sname);
+}
+
+static inline bool is_rand_symbol(struct module *mod, Elf64_Sym *sym)
+{
+	if(!is_randomizable_module(mod) || !is_local_symbol(sym))
+		return false;
+
+	return !module_is_fixed_section(mod, sym->st_shndx);
+}
+
+static int nullify_relocations_rel(unsigned int relsec, struct module *mod)
+{
+	unsigned int i;
+	Elf_Shdr *sechdrs = mod->klp_info->sechdrs;
+	Elf64_Rela *rel = (void *)sechdrs[relsec].sh_addr;
+	unsigned int symindex = mod->klp_info->symndx;
+	unsigned int symsec;
+	bool relsecType = module_is_fixed_section(mod, mod->klp_info->sechdrs[relsec].sh_info);
+	bool symsecType;
+	Elf64_Sym *sym;
+	int relocations_removed = 0;
+
+	for (i = 0; i < sechdrs[relsec].sh_size / sizeof(*rel); i++) {
+		sym = (Elf64_Sym *)sechdrs[symindex].sh_addr
+			+ ELF64_R_SYM(rel[i].r_info);
+		symsec = sym->st_shndx;
+		symsecType = module_is_fixed_section(mod, symsec);
+
+		switch (ELF64_R_TYPE(rel[i].r_info)) {
+			case R_X86_64_REX_GOTPCRELX:
+			case R_X86_64_GOTPCRELX:
+			case R_X86_64_GOTPCREL:
+			case R_X86_64_PLT32:
+				relocations_removed++;
+				rel[i].r_info = 0;
+				break;
+		}
+	}
+
+	return relocations_removed;
+}
+
+/* Remove relocation that are not needed in the
+ * re-randomization process */
+static void nullify_relocations(struct module *mod)
+{
+	unsigned int i;
+	char *sname;
+	int relocations_removed = 0;
+
+	printk("nullify_relocations\n");
+
+	for (i = 1; i < mod->klp_info->hdr.e_shnum; i++) {
+		unsigned int infosec = mod->klp_info->sechdrs[i].sh_info;
+		sname = mod->klp_info->secstrings +
+						mod->klp_info->sechdrs[i].sh_name;
+
+		/* Not a valid relocation section */
+		if (infosec >= mod->klp_info->hdr.e_shnum)
+			continue;
+
+		/* Don't bother with non-allocated sections */
+		if (!(mod->klp_info->sechdrs[infosec].sh_flags & SHF_ALLOC))
+			continue;
+
+		/* Livepatch relocation sections are applied by livepatch */
+		if (mod->klp_info->sechdrs[i].sh_flags & SHF_RELA_LIVEPATCH)
+			continue;
+
+		if (strstarts(sname, ".rela.init")) {
+			printk("%s nullified\n", sname);
+			mod->klp_info->sechdrs[i].sh_type = SHT_NULL;
+			continue;
+		}
+
+
+		if (mod->klp_info->sechdrs[i].sh_type == SHT_RELA) {
+			relocations_removed += nullify_relocations_rel(i, mod);
+		}
+	}
+
+	printk("relocations_removed = %d\n", relocations_removed);
+}
+
+int module_arch_preinit(struct module *mod)
+{
+	Elf_Shdr *sechdrs;
+	char *secstrings;
+	unsigned int i;
+
+	if(!is_randomizable_module(mod)) return 0;
+
+	sechdrs = mod->klp_info->sechdrs;
+	secstrings = mod->klp_info->secstrings;
+
+	/* Find .got.rand */
+	for (i = 0; i < mod->klp_info->hdr.e_shnum; i++) {
+		if (!strcmp(secstrings + sechdrs[i].sh_name, ".got.rand")) {
+			mod->arch.rand.got = sechdrs + i;
+		}
+	}
+
+	if (!mod->arch.core.got)
+		return -ENOEXEC;
+
+	module_disable_ro(mod);
+		nullify_relocations(mod);
+	module_enable_ro(mod, false);
+
+	/* TODO: Remove */
+//	module_print_addresses(mod);
+
+	return 0;
+}
+
+static void module_reapply_relocations(struct module *mod, unsigned long delta)
+{
+	unsigned int i;
+//	unsigned int num_sec = mod->klp_info->hdr.e_shnum;
+
+//	printk("module_reapply_relocations\n");
+
+	for (i = 1; i < mod->klp_info->hdr.e_shnum; i++) {
+		unsigned int infosec = mod->klp_info->sechdrs[i].sh_info;
+
+		/* Not a valid relocation section */
+		if (infosec >= mod->klp_info->hdr.e_shnum)
+			continue;
+
+		/* Don't bother with non-allocated sections */
+		if (!(mod->klp_info->sechdrs[infosec].sh_flags & SHF_ALLOC))
+			continue;
+
+		/* Livepatch relocation sections are applied by livepatch */
+		if (mod->klp_info->sechdrs[i].sh_flags & SHF_RELA_LIVEPATCH)
+			continue;
+
+		/* Don't do fixed section */
+		if (module_is_fixed_section(mod, infosec))
+			continue;
+
+		if (mod->klp_info->sechdrs[i].sh_type == SHT_RELA) {
+			apply_relocate_add__(mod->klp_info->sechdrs, mod->kallsyms->strtab,
+					mod->klp_info->symndx, i, mod, false);
+		}
+	}
+
+	for (i = 1; i < mod->klp_info->hdr.e_shnum; i++) {
+		unsigned int infosec = mod->klp_info->sechdrs[i].sh_info;
+
+		/* Not a valid relocation section */
+		if (infosec >= mod->klp_info->hdr.e_shnum)
+			continue;
+
+		/* Don't bother with non-allocated sections */
+		if (!(mod->klp_info->sechdrs[infosec].sh_flags & SHF_ALLOC))
+			continue;
+
+		/* Livepatch relocation sections are applied by livepatch */
+		if (mod->klp_info->sechdrs[i].sh_flags & SHF_RELA_LIVEPATCH)
+			continue;
+
+		/* Do fixed section */
+		if (!module_is_fixed_section(mod, infosec))
+			continue;
+
+		if (mod->klp_info->sechdrs[i].sh_type == SHT_RELA) {
+			apply_relocate_add__(mod->klp_info->sechdrs, mod->kallsyms->strtab,
+					mod->klp_info->symndx, i, mod, false);
+		}
+	}
+}
+
+/* Update all randomized symbols in the symbol table. */
+static void module_update_symbols(struct module *mod, unsigned long delta)
+{
+	unsigned int i;
+	Elf64_Shdr *sym_sechdr = mod->klp_info->sechdrs + mod->klp_info->symndx;
+	Elf64_Sym *syms = (Elf64_Sym *)sym_sechdr->sh_addr;
+	unsigned int num_syms = sym_sechdr->sh_size / sizeof(*syms);
+
+	for (i = 0; i < num_syms; i++) {
+		if (is_rand_symbol(mod, &syms[i])) {
+//			printk("  i=%u, sec=%u\n", i, syms[i].st_shndx);
+			INC_BY_DELTA(syms[i].st_value, delta);
+		}
+	}
+}
+
+/* Update all randomized symbols in .got.rand */
+static void module_update_got(struct module *mod, unsigned long delta)
+{
+	unsigned int i;
+	struct mod_sec *gotsec = &mod->arch.rand;
+	u64 *got = (u64*)gotsec->got->sh_addr;
+
+	for(i=0; i < gotsec->got_num_entries; i++){
+		INC_BY_DELTA(got[i], delta);
+	}
+}
+
+static void module_print_addresses(struct module *mod){
+	unsigned int i;
+
+	printk("core_layout.base  = 0lx%x | 0x%x\n",
+			mod->core_layout.base, mod->core_layout.size);
+	printk("fixed_layout.base = 0lx%x | 0x%x\n",
+			mod->fixed_layout.base,mod->fixed_layout.size);
+	printk("init_layout.base  = 0lx%x | 0x%x\n",
+			mod->init_layout.base, mod->init_layout.size);
+
+	for (i = 1; i < mod->klp_info->hdr.e_shnum; i++) {
+		printk("%s\t= 0x%lx | 0x%x\n", mod->klp_info->secstrings +
+				mod->klp_info->sechdrs[i].sh_name, mod->klp_info->sechdrs[i].sh_addr,
+				mod->klp_info->sechdrs[i].sh_size);
+	}
+}
+
+void *module_rerandomize(struct module *mod)
+{
+	unsigned long delta;
+	void *p;
+	unsigned long size = mod->core_layout.size;
+	unsigned long addr = (unsigned long) mod->core_layout.base;
+
+	if(!is_randomizable_module(mod)) return NULL;
+
+	p = module_alloc(size);
+	memcpy(p, addr, size);
+
+	// Clear permission of old address space
+	module_disable_ro(mod);
+	module_disable_nx(mod);
+
+	delta = (unsigned long)p - (unsigned long)addr;
+//	printk("delta = %ld = %lu = 0x%lx\n", delta, delta, delta);
+
+	// Update kernel's pointer to this module
+	update_module_ref(mod, delta);
+
+	// Set permission of new address space
+	module_enable_nx(mod);
+
+//	module_print_addresses(mod);
+
+	module_disable_ro(mod);
+		module_update_symbols(mod, delta);
+		module_update_got(mod, delta);
+		module_reapply_relocations(mod, delta);
+	module_enable_ro(mod, true);
+
+	if (mod->rerandomize)
+		mod->rerandomize(delta);
+
+	smr_retire(addr);
+
+	return p;
+}
+EXPORT_SYMBOL_GPL(module_rerandomize);
+#else /* !CONFIG_X86_MODULE_RERANDOMIZE */
+static inline bool is_rand_symbol(struct module *mod, Elf64_Sym *sym)
+{
+	return false;
+}
+#endif
+
 static u64 module_emit_got_entry(struct module *mod, void *loc,
 				 const Elf64_Rela *rela, Elf64_Sym *sym)
 {
-	struct mod_got_sec *gotsec = &mod->arch.core;
+	struct mod_sec *gotsec = is_rand_symbol(mod, sym)?&mod->arch.rand:&mod->arch.core;
 	u64 *got = (u64*)gotsec->got->sh_addr;
 	int i = gotsec->got_num_entries;
 	u64 ret;
@@ -161,7 +467,7 @@
 static u64 module_emit_plt_entry(struct module *mod, void *loc,
 				 const Elf64_Rela *rela, Elf64_Sym *sym)
 {
-	struct mod_plt_sec *pltsec = &mod->arch.core_plt;
+	struct mod_sec *pltsec = &mod->arch.core;
 	int i = pltsec->plt_num_entries;
 	void *plt = (void *)pltsec->plt->sh_addr + (u64)i * module_plt_size;
 
@@ -207,8 +513,9 @@
 	return num > 0 && cmp_rela(rela + num, rela + num - 1) == 0;
 }
 
-static void count_gots_plts(unsigned long *num_got, unsigned long *num_plt,
-		Elf64_Sym *syms, Elf64_Rela *rela, int num)
+static void count_gots_plts(unsigned long *num_got,
+		unsigned long *num_rand_got, unsigned long *num_plt,
+		Elf64_Sym *syms, Elf64_Rela *rela, int num, struct module *mod)
 {
 	Elf64_Sym *s;
 	int i;
@@ -227,7 +534,11 @@
 			 */
 			if (!duplicate_rel(rela, i) &&
 			    !find_got_kernel_entry(s, rela + i)) {
-				(*num_got)++;
+				if (is_rand_symbol(mod, s))
+					(*num_rand_got)++;
+				else
+					(*num_got)++;
+
 				if (ELF64_R_TYPE(rela[i].r_info) ==
 				    R_X86_64_PLT32 && !is_local_symbol(s))
 					(*num_plt)++;
@@ -363,12 +674,15 @@
 {
 	unsigned long num_got = 0;
 	unsigned long num_plt = 0;
+	unsigned long num_rand_got = 0;
 	Elf_Shdr *symtab = NULL;
 	Elf64_Sym *syms = NULL;
 	char *strings, *name;
 	int i, got_idx = -1;
 
-	apply_relaxations(ehdr, sechdrs, mod);
+	// TODO: allow for randomizable after testing
+	if (!is_randomizable_module(mod))
+		apply_relaxations(ehdr, sechdrs, mod);
 
 	/*
 	 * Find the empty .got and .plt sections so we can expand it
@@ -378,8 +692,10 @@
 	for (i = 0; i < ehdr->e_shnum; i++) {
 		if (!strcmp(secstrings + sechdrs[i].sh_name, ".got")) {
 			got_idx = i;
+		} else if (!strcmp(secstrings + sechdrs[i].sh_name, ".got.rand")) {
+			mod->arch.rand.got = sechdrs + i;
 		} else if (!strcmp(secstrings + sechdrs[i].sh_name, ".plt")) {
-			mod->arch.core_plt.plt = sechdrs + i;
+			mod->arch.core.plt = sechdrs + i;
 		} else if (sechdrs[i].sh_type == SHT_SYMTAB) {
 			symtab = sechdrs + i;
 			syms = (Elf64_Sym *)symtab->sh_addr;
@@ -393,7 +709,11 @@
 
 	mod->arch.core.got = sechdrs + got_idx;
 
-	if (!mod->arch.core_plt.plt) {
+	if (!mod->arch.rand.got) {
+		pr_err("%s: module .got.rand section missing\n", mod->name);
+		return -ENOEXEC;
+	}
+	if (!mod->arch.core.plt) {
 		pr_err("%s: module PLT section missing\n", mod->name);
 		return -ENOEXEC;
 	}
@@ -412,7 +732,8 @@
 		/* sort by type, symbol index and addend */
 		sort(rels, numrels, sizeof(Elf64_Rela), cmp_rela, NULL);
 
-		count_gots_plts(&num_got, &num_plt, syms, rels, numrels);
+		count_gots_plts(&num_got, &num_rand_got,
+				&num_plt, syms, rels, numrels, mod);
 	}
 
 	mod->arch.core.got->sh_type = SHT_NOBITS;
@@ -422,13 +743,20 @@
 	mod->arch.core.got_num_entries = 0;
 	mod->arch.core.got_max_entries = num_got;
 
+	mod->arch.rand.got->sh_type = SHT_NOBITS;
+	mod->arch.rand.got->sh_flags = SHF_ALLOC;
+	mod->arch.rand.got->sh_addralign = L1_CACHE_BYTES;
+	mod->arch.rand.got->sh_size = (num_rand_got + 1) * sizeof(u64);
+	mod->arch.rand.got_num_entries = 0;
+	mod->arch.rand.got_max_entries = num_rand_got;
+
 	module_plt_size = ALIGN(__THUNK_FOR_PLT_SIZE, PLT_ENTRY_ALIGNMENT);
-	mod->arch.core_plt.plt->sh_type = SHT_NOBITS;
-	mod->arch.core_plt.plt->sh_flags = SHF_EXECINSTR | SHF_ALLOC;
-	mod->arch.core_plt.plt->sh_addralign = L1_CACHE_BYTES;
-	mod->arch.core_plt.plt->sh_size = (num_plt + 1) * module_plt_size;
-	mod->arch.core_plt.plt_num_entries = 0;
-	mod->arch.core_plt.plt_max_entries = num_plt;
+	mod->arch.core.plt->sh_type = SHT_NOBITS;
+	mod->arch.core.plt->sh_flags = SHF_EXECINSTR | SHF_ALLOC;
+	mod->arch.core.plt->sh_addralign = L1_CACHE_BYTES;
+	mod->arch.core.plt->sh_size = (num_plt + 1) * module_plt_size;
+	mod->arch.core.plt_num_entries = 0;
+	mod->arch.core.plt_max_entries = num_plt;
 
 	strings = (void *) ehdr + sechdrs[symtab->sh_link].sh_offset;
 	for (i = 0; i < symtab->sh_size/sizeof(Elf_Sym); i++) {
@@ -525,7 +853,16 @@
 		   const char *strtab,
 		   unsigned int symindex,
 		   unsigned int relsec,
-		   struct module *me)
+		   struct module *me){
+	return apply_relocate_add__(sechdrs, strtab, symindex, relsec, me, true);
+}
+
+static int apply_relocate_add__(Elf64_Shdr *sechdrs,
+		   const char *strtab,
+		   unsigned int symindex,
+		   unsigned int relsec,
+		   struct module *me,
+		   bool check)
 {
 	unsigned int i;
 	Elf64_Rela *rel = (void *)sechdrs[relsec].sh_addr;
@@ -546,7 +883,9 @@
 			+ ELF64_R_SYM(rel[i].r_info);
 
 #ifdef CONFIG_X86_PIC
-		BUG_ON(check_relocation_pic_safe(&rel[i], sym));
+/* FIXME: REMOVE this comment */
+// hxn
+//		BUG_ON(check_relocation_pic_safe(&rel[i], sym));
 #endif
 
 		DEBUGP("type %d st_value %Lx r_addend %Lx loc %Lx\n",
@@ -558,20 +897,23 @@
 		switch (ELF64_R_TYPE(rel[i].r_info)) {
 		case R_X86_64_NONE:
 			break;
+		case R_X86_64_GOTOFF64:
+			val -= me->arch.core.got->sh_addr;
+			/* fallthrough */
 		case R_X86_64_64:
-			if (*(u64 *)loc != 0)
+			if (check && *(u64 *)loc != 0)
 				goto invalid_relocation;
 			*(u64 *)loc = val;
 			break;
 		case R_X86_64_32:
-			if (*(u32 *)loc != 0)
+			if (check && *(u32 *)loc != 0)
 				goto invalid_relocation;
 			*(u32 *)loc = val;
 			if (val != *(u32 *)loc)
 				goto overflow;
 			break;
 		case R_X86_64_32S:
-			if (*(s32 *)loc != 0)
+			if (check && *(s32 *)loc != 0)
 				goto invalid_relocation;
 			*(s32 *)loc = val;
 			if ((s64)val != *(s32 *)loc)
@@ -588,9 +930,11 @@
 			val = module_emit_got_entry(me, loc, rel + i, sym)
 				+ rel[i].r_addend;
 			/* fallthrough */
+		case R_X86_64_GOTPC32:
+			/* symbol = _GLOBAL_OFFSET_TABLE_ */
 		case R_X86_64_PC32:
 pc32_reloc:
-			if (*(u32 *)loc != 0)
+			if (check && *(u32 *)loc != 0)
 				goto invalid_relocation;
 			val -= (u64)loc;
 			*(u32 *)loc = val;
diff -urN linux-4.18.20/arch/x86/kernel/module.lds linux-4.18.20-new/arch/x86/kernel/module.lds
--- linux-4.18.20/arch/x86/kernel/module.lds	2019-03-12 13:36:57.822706123 -0400
+++ linux-4.18.20-new/arch/x86/kernel/module.lds	2019-03-12 13:37:16.958908626 -0400
@@ -1,4 +1,5 @@
 SECTIONS {
 	.got (NOLOAD) : { BYTE(0) }
 	.plt (NOLOAD) : { BYTE(0) }
+	.got.rand (NOLOAD) : { BYTE(0) }
 }
diff -urN linux-4.18.20/arch/x86/module-lib/retpoline.S linux-4.18.20-new/arch/x86/module-lib/retpoline.S
--- linux-4.18.20/arch/x86/module-lib/retpoline.S	2019-03-12 13:36:57.826706164 -0400
+++ linux-4.18.20-new/arch/x86/module-lib/retpoline.S	2019-03-12 13:37:16.958908626 -0400
@@ -45,3 +45,12 @@
 GENERATE_THUNK(r15)
 #endif
 
+.section .fixed.text, "ax"
+ENTRY(__FIXED_JMP_RETPOLINE)
+	JMP_NOSPEC %rax
+ENDPROC(__FIXED_JMP_RETPOLINE)
+
+.section .fixed.text, "ax"
+ENTRY(__FIXED_CALL_RETPOLINE)
+	CALL_NOSPEC %rax
+ENDPROC(__FIXED_CALL_RETPOLINE)
diff -urN linux-4.18.20/arch/x86/tools/relocs.h linux-4.18.20-new/arch/x86/tools/relocs.h
--- linux-4.18.20/arch/x86/tools/relocs.h	2019-03-12 13:36:52.542650182 -0400
+++ linux-4.18.20-new/arch/x86/tools/relocs.h	2019-03-12 13:37:16.958908626 -0400
@@ -17,6 +17,14 @@
 #include <regex.h>
 #include <tools/le_byteshift.h>
 
+#ifndef R_X86_64_REX_GOTPCRELX
+	#define R_X86_64_REX_GOTPCRELX	42
+#endif
+
+#ifndef R_X86_64_GOTPCRELX
+	#define R_X86_64_GOTPCRELX	41
+#endif
+
 void die(char *fmt, ...) __attribute__((noreturn));
 
 #define ARRAY_SIZE(x) (sizeof(x) / sizeof((x)[0]))
diff -urN linux-4.18.20/drivers/net/ethernet/amazon/ena/ena_com.h linux-4.18.20-new/drivers/net/ethernet/amazon/ena/ena_com.h
--- linux-4.18.20/drivers/net/ethernet/amazon/ena/ena_com.h	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/drivers/net/ethernet/amazon/ena/ena_com.h	2019-03-12 13:37:16.958908626 -0400
@@ -42,12 +42,22 @@
 #include <linux/spinlock.h>
 #include <linux/types.h>
 #include <linux/wait.h>
+#include <linux/module.h>
 
 #include "ena_common_defs.h"
 #include "ena_admin_defs.h"
 #include "ena_eth_io_defs.h"
 #include "ena_regs_defs.h"
 
+#ifndef SPECIAL_VAR
+/* FIXME: REMOVE - these are for IDE */
+#define SPECIAL_VAR(x) x
+#define SPECIAL_CONST_VAR(x) x
+#define SPECIAL_FUNCTION_PROTO(ret, name, args...) ret name (args)
+#define SPECIAL_FUNCTION(ret, name, args...) ret name (args)
+#error "Could not find wrappers"
+#endif
+
 #undef pr_fmt
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
diff -urN linux-4.18.20/drivers/net/ethernet/amazon/ena/ena_ethtool.c linux-4.18.20-new/drivers/net/ethernet/amazon/ena/ena_ethtool.c
--- linux-4.18.20/drivers/net/ethernet/amazon/ena/ena_ethtool.c	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/drivers/net/ethernet/amazon/ena/ena_ethtool.c	2019-03-12 13:37:16.958908626 -0400
@@ -173,7 +173,7 @@
 	}
 }
 
-static void ena_get_ethtool_stats(struct net_device *netdev,
+SPECIAL_FUNCTION(static void, ena_get_ethtool_stats, struct net_device *netdev,
 				  struct ethtool_stats *stats,
 				  u64 *data)
 {
@@ -195,7 +195,7 @@
 	ena_dev_admin_queue_stats(adapter, &data);
 }
 
-int ena_get_sset_count(struct net_device *netdev, int sset)
+SPECIAL_FUNCTION(int, ena_get_sset_count, struct net_device *netdev, int sset)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
 
@@ -245,7 +245,7 @@
 	}
 }
 
-static void ena_get_strings(struct net_device *netdev, u32 sset, u8 *data)
+SPECIAL_FUNCTION(static void, ena_get_strings, struct net_device *netdev, u32 sset, u8 *data)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
 	const struct ena_stats *ena_stats;
@@ -265,7 +265,7 @@
 	ena_com_dev_strings(&data);
 }
 
-static int ena_get_link_ksettings(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, ena_get_link_ksettings, struct net_device *netdev,
 				  struct ethtool_link_ksettings *link_ksettings)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
@@ -297,7 +297,7 @@
 	return 0;
 }
 
-static int ena_get_coalesce(struct net_device *net_dev,
+SPECIAL_FUNCTION(static int, ena_get_coalesce, struct net_device *net_dev,
 			    struct ethtool_coalesce *coalesce)
 {
 	struct ena_adapter *adapter = netdev_priv(net_dev);
@@ -345,7 +345,7 @@
 		adapter->tx_ring[i].smoothed_interval = val;
 }
 
-static int ena_set_coalesce(struct net_device *net_dev,
+SPECIAL_FUNCTION(static int, ena_set_coalesce, struct net_device *net_dev,
 			    struct ethtool_coalesce *coalesce)
 {
 	struct ena_adapter *adapter = netdev_priv(net_dev);
@@ -416,21 +416,21 @@
 	return 0;
 }
 
-static u32 ena_get_msglevel(struct net_device *netdev)
+SPECIAL_FUNCTION(static u32, ena_get_msglevel, struct net_device *netdev)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
 
 	return adapter->msg_enable;
 }
 
-static void ena_set_msglevel(struct net_device *netdev, u32 value)
+SPECIAL_FUNCTION(static void, ena_set_msglevel, struct net_device *netdev, u32 value)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
 
 	adapter->msg_enable = value;
 }
 
-static void ena_get_drvinfo(struct net_device *dev,
+SPECIAL_FUNCTION(static void, ena_get_drvinfo, struct net_device *dev,
 			    struct ethtool_drvinfo *info)
 {
 	struct ena_adapter *adapter = netdev_priv(dev);
@@ -441,7 +441,7 @@
 		sizeof(info->bus_info));
 }
 
-static void ena_get_ringparam(struct net_device *netdev,
+SPECIAL_FUNCTION(static void, ena_get_ringparam, struct net_device *netdev,
 			      struct ethtool_ringparam *ring)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
@@ -593,7 +593,7 @@
 	return ena_com_fill_hash_ctrl(ena_dev, proto, hash_fields);
 }
 
-static int ena_set_rxnfc(struct net_device *netdev, struct ethtool_rxnfc *info)
+SPECIAL_FUNCTION(static int, ena_set_rxnfc, struct net_device *netdev, struct ethtool_rxnfc *info)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
 	int rc = 0;
@@ -613,7 +613,7 @@
 	return rc;
 }
 
-static int ena_get_rxnfc(struct net_device *netdev, struct ethtool_rxnfc *info,
+SPECIAL_FUNCTION(static int, ena_get_rxnfc, struct net_device *netdev, struct ethtool_rxnfc *info,
 			 u32 *rules)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
@@ -639,17 +639,17 @@
 	return rc;
 }
 
-static u32 ena_get_rxfh_indir_size(struct net_device *netdev)
+SPECIAL_FUNCTION(static u32, ena_get_rxfh_indir_size, struct net_device *netdev)
 {
 	return ENA_RX_RSS_TABLE_SIZE;
 }
 
-static u32 ena_get_rxfh_key_size(struct net_device *netdev)
+SPECIAL_FUNCTION(static u32, ena_get_rxfh_key_size, struct net_device *netdev)
 {
 	return ENA_HASH_KEY_SIZE;
 }
 
-static int ena_get_rxfh(struct net_device *netdev, u32 *indir, u8 *key,
+SPECIAL_FUNCTION(static int, ena_get_rxfh, struct net_device *netdev, u32 *indir, u8 *key,
 			u8 *hfunc)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
@@ -684,7 +684,7 @@
 	return rc;
 }
 
-static int ena_set_rxfh(struct net_device *netdev, const u32 *indir,
+SPECIAL_FUNCTION(static int, ena_set_rxfh, struct net_device *netdev, const u32 *indir,
 			const u8 *key, const u8 hfunc)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
@@ -738,7 +738,7 @@
 	return 0;
 }
 
-static void ena_get_channels(struct net_device *netdev,
+SPECIAL_FUNCTION(static void, ena_get_channels, struct net_device *netdev,
 			     struct ethtool_channels *channels)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
@@ -753,7 +753,7 @@
 	channels->combined_count = 0;
 }
 
-static int ena_get_tunable(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, ena_get_tunable, struct net_device *netdev,
 			   const struct ethtool_tunable *tuna, void *data)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
@@ -771,7 +771,7 @@
 	return ret;
 }
 
-static int ena_set_tunable(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, ena_set_tunable, struct net_device *netdev,
 			   const struct ethtool_tunable *tuna,
 			   const void *data)
 {
@@ -796,7 +796,7 @@
 	return ret;
 }
 
-static const struct ethtool_ops ena_ethtool_ops = {
+SPECIAL_CONST_VAR(static const struct ethtool_ops ena_ethtool_ops) = {
 	.get_link_ksettings	= ena_get_link_ksettings,
 	.get_drvinfo		= ena_get_drvinfo,
 	.get_msglevel		= ena_get_msglevel,
diff -urN linux-4.18.20/drivers/net/ethernet/amazon/ena/ena_netdev.c linux-4.18.20-new/drivers/net/ethernet/amazon/ena/ena_netdev.c
--- linux-4.18.20/drivers/net/ethernet/amazon/ena/ena_netdev.c	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/drivers/net/ethernet/amazon/ena/ena_netdev.c	2019-03-12 13:37:16.958908626 -0400
@@ -52,6 +52,7 @@
 
 static char version[] = DEVICE_NAME " v" DRV_MODULE_VERSION "\n";
 
+MODULE_INFO(randomizable, "Y");
 MODULE_AUTHOR("Amazon.com, Inc. or its affiliates");
 MODULE_DESCRIPTION(DEVICE_NAME);
 MODULE_LICENSE("GPL");
@@ -68,7 +69,7 @@
 module_param(debug, int, 0);
 MODULE_PARM_DESC(debug, "Debug level (0=none,...,16=all)");
 
-static struct ena_aenq_handlers aenq_handlers;
+SPECIAL_VAR(static struct ena_aenq_handlers) aenq_handlers;
 
 static struct workqueue_struct *ena_wq;
 
@@ -79,7 +80,7 @@
 static void ena_destroy_device(struct ena_adapter *adapter, bool graceful);
 static int ena_restore_device(struct ena_adapter *adapter);
 
-static void ena_tx_timeout(struct net_device *dev)
+SPECIAL_FUNCTION(static void, ena_tx_timeout, struct net_device *dev)
 {
 	struct ena_adapter *adapter = netdev_priv(dev);
 
@@ -106,7 +107,7 @@
 		adapter->rx_ring[i].mtu = mtu;
 }
 
-static int ena_change_mtu(struct net_device *dev, int new_mtu)
+SPECIAL_FUNCTION(static int, ena_change_mtu, struct net_device *dev, int new_mtu)
 {
 	struct ena_adapter *adapter = netdev_priv(dev);
 	int ret;
@@ -1197,7 +1198,7 @@
 	put_cpu();
 }
 
-static int ena_io_poll(struct napi_struct *napi, int budget)
+SPECIAL_FUNCTION(static int, ena_io_poll, struct napi_struct *napi, int budget)
 {
 	struct ena_napi *ena_napi = container_of(napi, struct ena_napi, napi);
 	struct ena_ring *tx_ring, *rx_ring;
@@ -1259,7 +1260,7 @@
 	return ret;
 }
 
-static irqreturn_t ena_intr_msix_mgmnt(int irq, void *data)
+SPECIAL_FUNCTION(static irqreturn_t, ena_intr_msix_mgmnt, int irq, void *data)
 {
 	struct ena_adapter *adapter = (struct ena_adapter *)data;
 
@@ -1276,7 +1277,7 @@
  * @irq: interrupt number
  * @data: pointer to a network interface private napi device structure
  */
-static irqreturn_t ena_intr_msix_io(int irq, void *data)
+SPECIAL_FUNCTION(static irqreturn_t, ena_intr_msix_io, int irq, void *data)
 {
 	struct ena_napi *ena_napi = data;
 
@@ -1854,7 +1855,7 @@
  * handler is registered with the OS, the watchdog timer is started,
  * and the stack is notified that the interface is ready.
  */
-static int ena_open(struct net_device *netdev)
+SPECIAL_FUNCTION(static int, ena_open, struct net_device *netdev)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
 	int rc;
@@ -1889,7 +1890,7 @@
  * needs to be disabled.  A global MAC reset is issued to stop the
  * hardware, and all transmit and receive resources are freed.
  */
-static int ena_close(struct net_device *netdev)
+SPECIAL_FUNCTION(static int, ena_close, struct net_device *netdev)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
 
@@ -1992,7 +1993,7 @@
 }
 
 /* Called with netif_tx_lock. */
-static netdev_tx_t ena_start_xmit(struct sk_buff *skb, struct net_device *dev)
+SPECIAL_FUNCTION(static netdev_tx_t, ena_start_xmit, struct sk_buff *skb, struct net_device *dev)
 {
 	struct ena_adapter *adapter = netdev_priv(dev);
 	struct ena_tx_buffer *tx_info;
@@ -2199,7 +2200,7 @@
 }
 
 #ifdef CONFIG_NET_POLL_CONTROLLER
-static void ena_netpoll(struct net_device *netdev)
+SPECIAL_FUNCTION(static void, ena_netpoll, struct net_device *netdev)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
 	int i;
@@ -2217,7 +2218,7 @@
 }
 #endif /* CONFIG_NET_POLL_CONTROLLER */
 
-static u16 ena_select_queue(struct net_device *dev, struct sk_buff *skb,
+SPECIAL_FUNCTION(static u16, ena_select_queue, struct net_device *dev, struct sk_buff *skb,
 			    void *accel_priv, select_queue_fallback_t fallback)
 {
 	u16 qid;
@@ -2312,7 +2313,7 @@
 	ena_com_delete_debug_area(adapter->ena_dev);
 }
 
-static void ena_get_stats64(struct net_device *netdev,
+SPECIAL_FUNCTION(static void, ena_get_stats64, struct net_device *netdev,
 			    struct rtnl_link_stats64 *stats)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
@@ -2371,7 +2372,7 @@
 	stats->tx_errors = 0;
 }
 
-static const struct net_device_ops ena_netdev_ops = {
+SPECIAL_CONST_VAR(static const struct net_device_ops ena_netdev_ops) = {
 	.ndo_open		= ena_open,
 	.ndo_stop		= ena_close,
 	.ndo_start_xmit		= ena_start_xmit,
@@ -2667,7 +2668,7 @@
 	return rc;
 }
 
-static void ena_fw_reset_device(struct work_struct *work)
+SPECIAL_FUNCTION(static void, ena_fw_reset_device, struct work_struct *work)
 {
 	struct ena_adapter *adapter =
 		container_of(work, struct ena_adapter, reset_task);
@@ -2944,7 +2945,7 @@
 		(netdev->features & GENMASK_ULL(63, 32)) >> 32;
 }
 
-static void ena_timer_service(struct timer_list *t)
+SPECIAL_FUNCTION(static void, ena_timer_service, struct timer_list *t)
 {
 	struct ena_adapter *adapter = from_timer(adapter, t, timer_service);
 	u8 *debug_area = adapter->ena_dev->host_attr.debug_area_virt_addr;
@@ -3184,7 +3185,7 @@
  * The OS initialization, configuring of the adapter private structure,
  * and a hardware reset occur.
  */
-static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+SPECIAL_FUNCTION(static int, ena_probe, struct pci_dev *pdev, const struct pci_device_id *ent)
 {
 	struct ena_com_dev_get_features_ctx get_feat_ctx;
 	static int version_printed;
@@ -3401,7 +3402,7 @@
  * ena_remove is called by the PCI subsystem to alert the driver
  * that it should release a PCI device.
  */
-static void ena_remove(struct pci_dev *pdev)
+SPECIAL_FUNCTION(static void, ena_remove, struct pci_dev *pdev)
 {
 	struct ena_adapter *adapter = pci_get_drvdata(pdev);
 	struct ena_com_dev *ena_dev;
@@ -3454,7 +3455,7 @@
  * @pdev: PCI device information struct
  * @state:power state
  */
-static int ena_suspend(struct pci_dev *pdev,  pm_message_t state)
+SPECIAL_FUNCTION(static int, ena_suspend, struct pci_dev *pdev,  pm_message_t state)
 {
 	struct ena_adapter *adapter = pci_get_drvdata(pdev);
 
@@ -3477,7 +3478,7 @@
  * @pdev: PCI device information struct
  *
  */
-static int ena_resume(struct pci_dev *pdev)
+SPECIAL_FUNCTION(static int, ena_resume, struct pci_dev *pdev)
 {
 	struct ena_adapter *adapter = pci_get_drvdata(pdev);
 	int rc;
@@ -3493,7 +3494,8 @@
 }
 #endif
 
-static struct pci_driver ena_pci_driver = {
+
+SPECIAL_VAR(static struct pci_driver ena_pci_driver) = {
 	.name		= DRV_MODULE_NAME,
 	.id_table	= ena_pci_tbl,
 	.probe		= ena_probe,
@@ -3605,7 +3607,7 @@
 		  "Unknown event was received or event with unimplemented handler\n");
 }
 
-static struct ena_aenq_handlers aenq_handlers = {
+SPECIAL_VAR(static struct ena_aenq_handlers aenq_handlers) = {
 	.handlers = {
 		[ENA_ADMIN_LINK_CHANGE] = ena_update_on_link_change,
 		[ENA_ADMIN_NOTIFICATION] = ena_notification,
diff -urN linux-4.18.20/drivers/net/ethernet/amazon/ena/ena_pci_id_tbl.h linux-4.18.20-new/drivers/net/ethernet/amazon/ena/ena_pci_id_tbl.h
--- linux-4.18.20/drivers/net/ethernet/amazon/ena/ena_pci_id_tbl.h	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/drivers/net/ethernet/amazon/ena/ena_pci_id_tbl.h	2019-03-12 13:37:16.958908626 -0400
@@ -56,7 +56,7 @@
 #define ENA_PCI_ID_TABLE_ENTRY(devid) \
 	{PCI_DEVICE(PCI_VENDOR_ID_AMAZON, devid)},
 
-static const struct pci_device_id ena_pci_tbl[] = {
+SPECIAL_CONST_VAR(static const struct pci_device_id ena_pci_tbl[]) = {
 	ENA_PCI_ID_TABLE_ENTRY(PCI_DEV_ID_ENA_PF)
 	ENA_PCI_ID_TABLE_ENTRY(PCI_DEV_ID_ENA_LLQ_PF)
 	ENA_PCI_ID_TABLE_ENTRY(PCI_DEV_ID_ENA_VF)
diff -urN linux-4.18.20/drivers/net/ethernet/intel/e1000e/e1000.h linux-4.18.20-new/drivers/net/ethernet/intel/e1000e/e1000.h
--- linux-4.18.20/drivers/net/ethernet/intel/e1000e/e1000.h	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/drivers/net/ethernet/intel/e1000e/e1000.h	2019-03-12 13:37:16.958908626 -0400
@@ -23,8 +23,19 @@
 #include <linux/mii.h>
 #include <linux/mdio.h>
 #include <linux/pm_qos.h>
+#include <linux/module.h>
 #include "hw.h"
 
+// TODO: Remove
+#ifndef SPECIAL_VAR
+/* These are for IDE */
+#define SPECIAL_VAR(x) x
+#define SPECIAL_CONST_VAR(x) x
+#define SPECIAL_FUNCTION_PROTO(ret, name, args...) ret name (args)
+#define SPECIAL_FUNCTION(ret, name, args...) ret name (args)
+#error "Could not find wrappers"
+#endif
+
 struct e1000_info;
 
 #define e_dbg(format, arg...) \
@@ -466,8 +477,8 @@
 void e1000e_check_options(struct e1000_adapter *adapter);
 void e1000e_set_ethtool_ops(struct net_device *netdev);
 
-int e1000e_open(struct net_device *netdev);
-int e1000e_close(struct net_device *netdev);
+SPECIAL_FUNCTION_PROTO(int, e1000e_open, struct net_device *netdev);
+SPECIAL_FUNCTION_PROTO(int, e1000e_close, struct net_device *netdev);
 void e1000e_up(struct e1000_adapter *adapter);
 void e1000e_down(struct e1000_adapter *adapter, bool reset);
 void e1000e_reinit_locked(struct e1000_adapter *adapter);
diff -urN linux-4.18.20/drivers/net/ethernet/intel/e1000e/ethtool.c linux-4.18.20-new/drivers/net/ethernet/intel/e1000e/ethtool.c
--- linux-4.18.20/drivers/net/ethernet/intel/e1000e/ethtool.c	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/drivers/net/ethernet/intel/e1000e/ethtool.c	2019-03-12 13:37:16.958908626 -0400
@@ -100,7 +100,7 @@
 
 #define E1000_TEST_LEN ARRAY_SIZE(e1000_gstrings_test)
 
-static int e1000_get_link_ksettings(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_get_link_ksettings, struct net_device *netdev,
 				    struct ethtool_link_ksettings *cmd)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -248,7 +248,7 @@
 	return -EINVAL;
 }
 
-static int e1000_set_link_ksettings(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_set_link_ksettings, struct net_device *netdev,
 				    const struct ethtool_link_ksettings *cmd)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -337,7 +337,7 @@
 	return ret_val;
 }
 
-static void e1000_get_pauseparam(struct net_device *netdev,
+SPECIAL_FUNCTION(static void, e1000_get_pauseparam, struct net_device *netdev,
 				 struct ethtool_pauseparam *pause)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -356,7 +356,7 @@
 	}
 }
 
-static int e1000_set_pauseparam(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_set_pauseparam, struct net_device *netdev,
 				struct ethtool_pauseparam *pause)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -407,25 +407,25 @@
 	return retval;
 }
 
-static u32 e1000_get_msglevel(struct net_device *netdev)
+SPECIAL_FUNCTION(static u32, e1000_get_msglevel, struct net_device *netdev)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 	return adapter->msg_enable;
 }
 
-static void e1000_set_msglevel(struct net_device *netdev, u32 data)
+SPECIAL_FUNCTION(static void, e1000_set_msglevel, struct net_device *netdev, u32 data)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 	adapter->msg_enable = data;
 }
 
-static int e1000_get_regs_len(struct net_device __always_unused *netdev)
+SPECIAL_FUNCTION(static int, e1000_get_regs_len, struct net_device __always_unused *netdev)
 {
 #define E1000_REGS_LEN 32	/* overestimate */
 	return E1000_REGS_LEN * sizeof(u32);
 }
 
-static void e1000_get_regs(struct net_device *netdev,
+SPECIAL_FUNCTION(static void, e1000_get_regs, struct net_device *netdev,
 			   struct ethtool_regs *regs, void *p)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -484,13 +484,13 @@
 	pm_runtime_put_sync(netdev->dev.parent);
 }
 
-static int e1000_get_eeprom_len(struct net_device *netdev)
+SPECIAL_FUNCTION(static int, e1000_get_eeprom_len, struct net_device *netdev)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 	return adapter->hw.nvm.word_size * 2;
 }
 
-static int e1000_get_eeprom(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_get_eeprom, struct net_device *netdev,
 			    struct ethtool_eeprom *eeprom, u8 *bytes)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -547,7 +547,7 @@
 	return ret_val;
 }
 
-static int e1000_set_eeprom(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_set_eeprom, struct net_device *netdev,
 			    struct ethtool_eeprom *eeprom, u8 *bytes)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -627,7 +627,7 @@
 	return ret_val;
 }
 
-static void e1000_get_drvinfo(struct net_device *netdev,
+SPECIAL_FUNCTION(static void, e1000_get_drvinfo, struct net_device *netdev,
 			      struct ethtool_drvinfo *drvinfo)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -649,7 +649,7 @@
 		sizeof(drvinfo->bus_info));
 }
 
-static void e1000_get_ringparam(struct net_device *netdev,
+SPECIAL_FUNCTION(static void, e1000_get_ringparam, struct net_device *netdev,
 				struct ethtool_ringparam *ring)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -660,7 +660,7 @@
 	ring->tx_pending = adapter->tx_ring_count;
 }
 
-static int e1000_set_ringparam(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_set_ringparam, struct net_device *netdev,
 			       struct ethtool_ringparam *ring)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -968,7 +968,7 @@
 	return *data;
 }
 
-static irqreturn_t e1000_test_intr(int __always_unused irq, void *data)
+SPECIAL_FUNCTION(static irqreturn_t, e1000_test_intr, int __always_unused irq, void *data)
 {
 	struct net_device *netdev = (struct net_device *)data;
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -1767,7 +1767,7 @@
 	return *data;
 }
 
-static int e1000e_get_sset_count(struct net_device __always_unused *netdev,
+SPECIAL_FUNCTION(static int, e1000e_get_sset_count, struct net_device __always_unused *netdev,
 				 int sset)
 {
 	switch (sset) {
@@ -1780,7 +1780,7 @@
 	}
 }
 
-static void e1000_diag_test(struct net_device *netdev,
+SPECIAL_FUNCTION(static void, e1000_diag_test, struct net_device *netdev,
 			    struct ethtool_test *eth_test, u64 *data)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -1880,7 +1880,7 @@
 	pm_runtime_put_sync(netdev->dev.parent);
 }
 
-static void e1000_get_wol(struct net_device *netdev,
+SPECIAL_FUNCTION(static void, e1000_get_wol, struct net_device *netdev,
 			  struct ethtool_wolinfo *wol)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -1915,7 +1915,7 @@
 		wol->wolopts |= WAKE_PHY;
 }
 
-static int e1000_set_wol(struct net_device *netdev, struct ethtool_wolinfo *wol)
+SPECIAL_FUNCTION(static int, e1000_set_wol, struct net_device *netdev, struct ethtool_wolinfo *wol)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 
@@ -1944,7 +1944,7 @@
 	return 0;
 }
 
-static int e1000_set_phys_id(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_set_phys_id, struct net_device *netdev,
 			     enum ethtool_phys_id_state state)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -1980,7 +1980,7 @@
 	return 0;
 }
 
-static int e1000_get_coalesce(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_get_coalesce, struct net_device *netdev,
 			      struct ethtool_coalesce *ec)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -1993,7 +1993,7 @@
 	return 0;
 }
 
-static int e1000_set_coalesce(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_set_coalesce, struct net_device *netdev,
 			      struct ethtool_coalesce *ec)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -2027,7 +2027,7 @@
 	return 0;
 }
 
-static int e1000_nway_reset(struct net_device *netdev)
+SPECIAL_FUNCTION(static int, e1000_nway_reset, struct net_device *netdev)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 
@@ -2044,7 +2044,7 @@
 	return 0;
 }
 
-static void e1000_get_ethtool_stats(struct net_device *netdev,
+SPECIAL_FUNCTION(static void, e1000_get_ethtool_stats, struct net_device *netdev,
 				    struct ethtool_stats __always_unused *stats,
 				    u64 *data)
 {
@@ -2079,7 +2079,7 @@
 	}
 }
 
-static void e1000_get_strings(struct net_device __always_unused *netdev,
+SPECIAL_FUNCTION(static void, e1000_get_strings, struct net_device __always_unused *netdev,
 			      u32 stringset, u8 *data)
 {
 	u8 *p = data;
@@ -2099,7 +2099,7 @@
 	}
 }
 
-static int e1000_get_rxnfc(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_get_rxnfc, struct net_device *netdev,
 			   struct ethtool_rxnfc *info,
 			   u32 __always_unused *rule_locs)
 {
@@ -2151,7 +2151,7 @@
 	}
 }
 
-static int e1000e_get_eee(struct net_device *netdev, struct ethtool_eee *edata)
+SPECIAL_FUNCTION(static int, e1000e_get_eee, struct net_device *netdev, struct ethtool_eee *edata)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
@@ -2227,7 +2227,7 @@
 	return ret_val;
 }
 
-static int e1000e_set_eee(struct net_device *netdev, struct ethtool_eee *edata)
+SPECIAL_FUNCTION(static int, e1000e_set_eee, struct net_device *netdev, struct ethtool_eee *edata)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
@@ -2270,7 +2270,7 @@
 	return 0;
 }
 
-static int e1000e_get_ts_info(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000e_get_ts_info, struct net_device *netdev,
 			      struct ethtool_ts_info *info)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -2304,7 +2304,8 @@
 	return 0;
 }
 
-static const struct ethtool_ops e1000_ethtool_ops = {
+// hxn
+SPECIAL_CONST_VAR(static const struct ethtool_ops e1000_ethtool_ops) = {
 	.get_drvinfo		= e1000_get_drvinfo,
 	.get_regs_len		= e1000_get_regs_len,
 	.get_regs		= e1000_get_regs,
diff -urN linux-4.18.20/drivers/net/ethernet/intel/e1000e/netdev.c linux-4.18.20-new/drivers/net/ethernet/intel/e1000e/netdev.c
--- linux-4.18.20/drivers/net/ethernet/intel/e1000e/netdev.c	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/drivers/net/ethernet/intel/e1000e/netdev.c	2019-03-12 13:37:16.962908667 -0400
@@ -28,10 +28,12 @@
 
 #include "e1000.h"
 
+MODULE_INFO(randomizable, "Y");
+
 #define DRV_EXTRAVERSION "-k"
 
 #define DRV_VERSION "3.2.6" DRV_EXTRAVERSION
-char e1000e_driver_name[] = "e1000e";
+SPECIAL_VAR(char e1000e_driver_name[]) = "e1000e";
 const char e1000e_driver_version[] = DRV_VERSION;
 
 #define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV|NETIF_MSG_PROBE|NETIF_MSG_LINK)
@@ -641,7 +643,7 @@
  * e1000_alloc_rx_buffers - Replace used receive buffers
  * @rx_ring: Rx descriptor ring
  **/
-static void e1000_alloc_rx_buffers(struct e1000_ring *rx_ring,
+SPECIAL_FUNCTION(static void, e1000_alloc_rx_buffers, struct e1000_ring *rx_ring,
 				   int cleaned_count, gfp_t gfp)
 {
 	struct e1000_adapter *adapter = rx_ring->adapter;
@@ -709,7 +711,7 @@
  * e1000_alloc_rx_buffers_ps - Replace used receive buffers; packet split
  * @rx_ring: Rx descriptor ring
  **/
-static void e1000_alloc_rx_buffers_ps(struct e1000_ring *rx_ring,
+SPECIAL_FUNCTION(static void, e1000_alloc_rx_buffers_ps, struct e1000_ring *rx_ring,
 				      int cleaned_count, gfp_t gfp)
 {
 	struct e1000_adapter *adapter = rx_ring->adapter;
@@ -813,7 +815,7 @@
  * @cleaned_count: number of buffers to allocate this pass
  **/
 
-static void e1000_alloc_jumbo_rx_buffers(struct e1000_ring *rx_ring,
+SPECIAL_FUNCTION(static void, e1000_alloc_jumbo_rx_buffers, struct e1000_ring *rx_ring,
 					 int cleaned_count, gfp_t gfp)
 {
 	struct e1000_adapter *adapter = rx_ring->adapter;
@@ -904,7 +906,7 @@
  * the return value indicates whether actual cleaning was done, there
  * is no guarantee that everything was cleaned
  **/
-static bool e1000_clean_rx_irq(struct e1000_ring *rx_ring, int *work_done,
+SPECIAL_FUNCTION(static bool, e1000_clean_rx_irq, struct e1000_ring *rx_ring, int *work_done,
 			       int work_to_do)
 {
 	struct e1000_adapter *adapter = rx_ring->adapter;
@@ -1077,7 +1079,7 @@
 	buffer_info->time_stamp = 0;
 }
 
-static void e1000_print_hw_hang(struct work_struct *work)
+SPECIAL_FUNCTION(static void, e1000_print_hw_hang, struct work_struct *work)
 {
 	struct e1000_adapter *adapter = container_of(work,
 						     struct e1000_adapter,
@@ -1162,7 +1164,7 @@
  * timestamp has been taken for the current stored skb.  The timestamp must
  * be for this skb because only one such packet is allowed in the queue.
  */
-static void e1000e_tx_hwtstamp_work(struct work_struct *work)
+SPECIAL_FUNCTION(static void, e1000e_tx_hwtstamp_work, struct work_struct *work)
 {
 	struct e1000_adapter *adapter = container_of(work, struct e1000_adapter,
 						     tx_hwtstamp_work);
@@ -1298,7 +1300,7 @@
  * the return value indicates whether actual cleaning was done, there
  * is no guarantee that everything was cleaned
  **/
-static bool e1000_clean_rx_irq_ps(struct e1000_ring *rx_ring, int *work_done,
+SPECIAL_FUNCTION(static bool, e1000_clean_rx_irq_ps, struct e1000_ring *rx_ring, int *work_done,
 				  int work_to_do)
 {
 	struct e1000_adapter *adapter = rx_ring->adapter;
@@ -1503,7 +1505,7 @@
  * the return value indicates whether actual cleaning was done, there
  * is no guarantee that everything was cleaned
  **/
-static bool e1000_clean_jumbo_rx_irq(struct e1000_ring *rx_ring, int *work_done,
+SPECIAL_FUNCTION(static bool, e1000_clean_jumbo_rx_irq, struct e1000_ring *rx_ring, int *work_done,
 				     int work_to_do)
 {
 	struct e1000_adapter *adapter = rx_ring->adapter;
@@ -1732,7 +1734,7 @@
 	adapter->flags2 &= ~FLAG2_IS_DISCARDING;
 }
 
-static void e1000e_downshift_workaround(struct work_struct *work)
+SPECIAL_FUNCTION(static void, e1000e_downshift_workaround, struct work_struct *work)
 {
 	struct e1000_adapter *adapter = container_of(work,
 						     struct e1000_adapter,
@@ -1749,7 +1751,7 @@
  * @irq: interrupt number
  * @data: pointer to a network interface device structure
  **/
-static irqreturn_t e1000_intr_msi(int __always_unused irq, void *data)
+SPECIAL_FUNCTION(static irqreturn_t, e1000_intr_msi, int __always_unused irq, void *data)
 {
 	struct net_device *netdev = data;
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -1816,7 +1818,7 @@
  * @irq: interrupt number
  * @data: pointer to a network interface device structure
  **/
-static irqreturn_t e1000_intr(int __always_unused irq, void *data)
+SPECIAL_FUNCTION(static irqreturn_t, e1000_intr, int __always_unused irq, void *data)
 {
 	struct net_device *netdev = data;
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -1891,7 +1893,7 @@
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t e1000_msix_other(int __always_unused irq, void *data)
+SPECIAL_FUNCTION(static irqreturn_t, e1000_msix_other, int __always_unused irq, void *data)
 {
 	struct net_device *netdev = data;
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -1914,7 +1916,7 @@
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t e1000_intr_msix_tx(int __always_unused irq, void *data)
+SPECIAL_FUNCTION(static irqreturn_t, e1000_intr_msix_tx, int __always_unused irq, void *data)
 {
 	struct net_device *netdev = data;
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -1934,7 +1936,7 @@
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t e1000_intr_msix_rx(int __always_unused irq, void *data)
+SPECIAL_FUNCTION(static irqreturn_t, e1000_intr_msix_rx, int __always_unused irq, void *data)
 {
 	struct net_device *netdev = data;
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -2653,7 +2655,7 @@
  * @napi: struct associated with this polling callback
  * @weight: number of packets driver is allowed to process this poll
  **/
-static int e1000e_poll(struct napi_struct *napi, int weight)
+SPECIAL_FUNCTION(static int, e1000e_poll, struct napi_struct *napi, int weight)
 {
 	struct e1000_adapter *adapter = container_of(napi, struct e1000_adapter,
 						     napi);
@@ -2688,7 +2690,7 @@
 	return work_done;
 }
 
-static int e1000_vlan_rx_add_vid(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_vlan_rx_add_vid, struct net_device *netdev,
 				 __always_unused __be16 proto, u16 vid)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -2714,7 +2716,7 @@
 	return 0;
 }
 
-static int e1000_vlan_rx_kill_vid(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_vlan_rx_kill_vid, struct net_device *netdev,
 				  __always_unused __be16 proto, u16 vid)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -3393,7 +3395,7 @@
  * responsible for configuring the hardware for proper unicast, multicast,
  * promiscuous mode, and all-multi behavior.
  **/
-static void e1000e_set_rx_mode(struct net_device *netdev)
+SPECIAL_FUNCTION(static void, e1000e_set_rx_mode, struct net_device *netdev)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
@@ -4440,7 +4442,7 @@
  * @irq: interrupt number
  * @data: pointer to a network interface device structure
  **/
-static irqreturn_t e1000_intr_msi_test(int __always_unused irq, void *data)
+SPECIAL_FUNCTION(static irqreturn_t, e1000_intr_msi_test,int __always_unused irq, void *data)
 {
 	struct net_device *netdev = data;
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -4570,7 +4572,7 @@
  * handler is registered with the OS, the watchdog timer is started,
  * and the stack is notified that the interface is ready.
  **/
-int e1000e_open(struct net_device *netdev)
+SPECIAL_FUNCTION(int, e1000e_open, struct net_device *netdev)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
@@ -4678,7 +4680,7 @@
  * needs to be disabled.  A global MAC reset is issued to stop the
  * hardware, and all transmit and receive resources are freed.
  **/
-int e1000e_close(struct net_device *netdev)
+SPECIAL_FUNCTION(int, e1000e_close, struct net_device *netdev)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct pci_dev *pdev = adapter->pdev;
@@ -4732,7 +4734,7 @@
  *
  * Returns 0 on success, negative on failure
  **/
-static int e1000_set_mac(struct net_device *netdev, void *p)
+SPECIAL_FUNCTION(static int, e1000_set_mac, struct net_device *netdev, void *p)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 	struct e1000_hw *hw = &adapter->hw;
@@ -4772,7 +4774,7 @@
  * semaphore to read the phy, which we could msleep while
  * waiting for it, and we can't msleep in a timer.
  **/
-static void e1000e_update_phy_task(struct work_struct *work)
+SPECIAL_FUNCTION(static void, e1000e_update_phy_task, struct work_struct *work)
 {
 	struct e1000_adapter *adapter = container_of(work,
 						     struct e1000_adapter,
@@ -4796,7 +4798,7 @@
  * Need to wait a few seconds after link up to get diagnostic information from
  * the phy
  **/
-static void e1000_update_phy_info(struct timer_list *t)
+SPECIAL_FUNCTION(static void, e1000_update_phy_info, struct timer_list *t)
 {
 	struct e1000_adapter *adapter = from_timer(adapter, t, phy_info_timer);
 
@@ -5132,7 +5134,7 @@
  * e1000_watchdog - Timer Call-back
  * @data: pointer to adapter cast into an unsigned long
  **/
-static void e1000_watchdog(struct timer_list *t)
+SPECIAL_FUNCTION(static void, e1000_watchdog, struct timer_list *t)
 {
 	struct e1000_adapter *adapter = from_timer(adapter, t, watchdog_timer);
 
@@ -5142,7 +5144,7 @@
 	/* TODO: make this use queue_delayed_work() */
 }
 
-static void e1000_watchdog_task(struct work_struct *work)
+SPECIAL_FUNCTION(static void, e1000_watchdog_task, struct work_struct *work)
 {
 	struct e1000_adapter *adapter = container_of(work,
 						     struct e1000_adapter,
@@ -5744,7 +5746,7 @@
 	return __e1000_maybe_stop_tx(tx_ring, size);
 }
 
-static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb,
+SPECIAL_FUNCTION(static netdev_tx_t, e1000_xmit_frame, struct sk_buff *skb,
 				    struct net_device *netdev)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -5904,7 +5906,7 @@
  * e1000_tx_timeout - Respond to a Tx Hang
  * @netdev: network interface device structure
  **/
-static void e1000_tx_timeout(struct net_device *netdev)
+SPECIAL_FUNCTION(static void, e1000_tx_timeout, struct net_device *netdev)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 
@@ -5913,7 +5915,7 @@
 	schedule_work(&adapter->reset_task);
 }
 
-static void e1000_reset_task(struct work_struct *work)
+SPECIAL_FUNCTION(static void, e1000_reset_task, struct work_struct *work)
 {
 	struct e1000_adapter *adapter;
 	adapter = container_of(work, struct e1000_adapter, reset_task);
@@ -5936,7 +5938,7 @@
  *
  * Returns the address of the device statistics structure.
  **/
-void e1000e_get_stats64(struct net_device *netdev,
+SPECIAL_FUNCTION(void, e1000e_get_stats64, struct net_device *netdev,
 			struct rtnl_link_stats64 *stats)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -5982,7 +5984,7 @@
  *
  * Returns 0 on success, negative on failure
  **/
-static int e1000_change_mtu(struct net_device *netdev, int new_mtu)
+SPECIAL_FUNCTION(static int, e1000_change_mtu, struct net_device *netdev, int new_mtu)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 	int max_frame = new_mtu + VLAN_ETH_HLEN + ETH_FCS_LEN;
@@ -6160,7 +6162,7 @@
 			    sizeof(adapter->hwtstamp_config)) ? -EFAULT : 0;
 }
 
-static int e1000_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)
+SPECIAL_FUNCTION(static int, e1000_ioctl, struct net_device *netdev, struct ifreq *ifr, int cmd)
 {
 	switch (cmd) {
 	case SIOCGMIIPHY:
@@ -6272,7 +6274,7 @@
 	pm_runtime_put_sync(netdev->dev.parent);
 }
 
-static int e1000e_pm_freeze(struct device *dev)
+SPECIAL_FUNCTION(static int, e1000e_pm_freeze, struct device *dev)
 {
 	struct net_device *netdev = pci_get_drvdata(to_pci_dev(dev));
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -6605,7 +6607,7 @@
 }
 
 #ifdef CONFIG_PM_SLEEP
-static int e1000e_pm_thaw(struct device *dev)
+SPECIAL_FUNCTION(static int, e1000e_pm_thaw, struct device *dev)
 {
 	struct net_device *netdev = pci_get_drvdata(to_pci_dev(dev));
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -6625,7 +6627,7 @@
 	return 0;
 }
 
-static int e1000e_pm_suspend(struct device *dev)
+SPECIAL_FUNCTION(static int, e1000e_pm_suspend, struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
 	int rc;
@@ -6641,7 +6643,7 @@
 	return rc;
 }
 
-static int e1000e_pm_resume(struct device *dev)
+SPECIAL_FUNCTION(static int, e1000e_pm_resume, struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
 	int rc;
@@ -6654,7 +6656,7 @@
 }
 #endif /* CONFIG_PM_SLEEP */
 
-static int e1000e_pm_runtime_idle(struct device *dev)
+SPECIAL_FUNCTION(static int, e1000e_pm_runtime_idle, struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
 	struct net_device *netdev = pci_get_drvdata(pdev);
@@ -6671,7 +6673,7 @@
 	return -EBUSY;
 }
 
-static int e1000e_pm_runtime_resume(struct device *dev)
+SPECIAL_FUNCTION(static int, e1000e_pm_runtime_resume, struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
 	struct net_device *netdev = pci_get_drvdata(pdev);
@@ -6688,7 +6690,7 @@
 	return rc;
 }
 
-static int e1000e_pm_runtime_suspend(struct device *dev)
+SPECIAL_FUNCTION(static int, e1000e_pm_runtime_suspend, struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
 	struct net_device *netdev = pci_get_drvdata(pdev);
@@ -6715,7 +6717,7 @@
 }
 #endif /* CONFIG_PM */
 
-static void e1000_shutdown(struct pci_dev *pdev)
+SPECIAL_FUNCTION(static void, e1000_shutdown, struct pci_dev *pdev)
 {
 	e1000e_flush_lpic(pdev);
 
@@ -6726,7 +6728,7 @@
 
 #ifdef CONFIG_NET_POLL_CONTROLLER
 
-static irqreturn_t e1000_intr_msix(int __always_unused irq, void *data)
+SPECIAL_FUNCTION(static irqreturn_t, e1000_intr_msix, int __always_unused irq, void *data)
 {
 	struct net_device *netdev = data;
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -6764,7 +6766,7 @@
  * without having to re-enable interrupts. It's not called while
  * the interrupt routine is executing.
  */
-static void e1000_netpoll(struct net_device *netdev)
+SPECIAL_FUNCTION(static void, e1000_netpoll, struct net_device *netdev)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 
@@ -6794,7 +6796,7 @@
  * This function is called after a PCI bus error affecting
  * this device has been detected.
  */
-static pci_ers_result_t e1000_io_error_detected(struct pci_dev *pdev,
+SPECIAL_FUNCTION(static pci_ers_result_t, e1000_io_error_detected, struct pci_dev *pdev,
 						pci_channel_state_t state)
 {
 	struct net_device *netdev = pci_get_drvdata(pdev);
@@ -6820,7 +6822,7 @@
  * Restart the card from scratch, as if from a cold-boot. Implementation
  * resembles the first-half of the e1000e_pm_resume routine.
  */
-static pci_ers_result_t e1000_io_slot_reset(struct pci_dev *pdev)
+SPECIAL_FUNCTION(static pci_ers_result_t, e1000_io_slot_reset, struct pci_dev *pdev)
 {
 	struct net_device *netdev = pci_get_drvdata(pdev);
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -6867,7 +6869,7 @@
  * its OK to resume normal operation. Implementation resembles the
  * second-half of the e1000e_pm_resume routine.
  */
-static void e1000_io_resume(struct pci_dev *pdev)
+SPECIAL_FUNCTION(static void, e1000_io_resume, struct pci_dev *pdev)
 {
 	struct net_device *netdev = pci_get_drvdata(pdev);
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -6929,7 +6931,7 @@
 	}
 }
 
-static netdev_features_t e1000_fix_features(struct net_device *netdev,
+SPECIAL_FUNCTION(static netdev_features_t, e1000_fix_features, struct net_device *netdev,
 					    netdev_features_t features)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -6950,7 +6952,7 @@
 	return features;
 }
 
-static int e1000_set_features(struct net_device *netdev,
+SPECIAL_FUNCTION(static int, e1000_set_features, struct net_device *netdev,
 			      netdev_features_t features)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -6988,7 +6990,8 @@
 	return 0;
 }
 
-static const struct net_device_ops e1000e_netdev_ops = {
+
+SPECIAL_CONST_VAR(static const struct net_device_ops e1000e_netdev_ops) = {
 	.ndo_open		= e1000e_open,
 	.ndo_stop		= e1000e_close,
 	.ndo_start_xmit		= e1000_xmit_frame,
@@ -7010,6 +7013,9 @@
 	.ndo_features_check	= passthru_features_check,
 };
 
+struct pci_dev *global_pdev_1 = NULL;
+struct pci_dev *global_pdev_2 = NULL;
+
 /**
  * e1000_probe - Device Initialization Routine
  * @pdev: PCI device information struct
@@ -7021,7 +7027,7 @@
  * The OS initialization, configuring of the adapter private structure,
  * and a hardware reset occur.
  **/
-static int e1000_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+SPECIAL_FUNCTION(static int, e1000_probe, struct pci_dev *pdev, const struct pci_device_id *ent)
 {
 	struct net_device *netdev;
 	struct e1000_adapter *adapter;
@@ -7036,6 +7042,8 @@
 	u16 eeprom_apme_mask = E1000_EEPROM_APME;
 	s32 ret_val = 0;
 
+	printk("XXXXXXXXX: Probe start\n");
+
 	if (ei->flags2 & FLAG2_DISABLE_ASPM_L0S)
 		aspm_disable_flag = PCIE_LINK_STATE_L0S;
 	if (ei->flags2 & FLAG2_DISABLE_ASPM_L1)
@@ -7333,6 +7341,13 @@
 	if (pci_dev_run_wake(pdev))
 		pm_runtime_put_noidle(&pdev->dev);
 
+	if(!global_pdev_1)
+		global_pdev_1 = pdev;
+	else
+		global_pdev_2 = pdev;
+
+	printk("XXXXXXXXX: Probe end\n");
+
 	return 0;
 
 err_register:
@@ -7360,6 +7375,35 @@
 	return err;
 }
 
+static void _e1000e_rerandomize(struct pci_dev *pdev, unsigned long delta)
+{
+	struct net_device *netdev;
+	struct e1000_adapter *adapter;
+	const struct e1000_info *ei;
+	struct e1000_hw *hw;
+
+	if(!pdev) return;
+
+	netdev = pci_get_drvdata(pdev);
+	adapter = netdev_priv(netdev);
+	ei = adapter->ei;
+	hw = &adapter->hw;
+
+	memcpy(&hw->mac.ops, ei->mac_ops, sizeof(hw->mac.ops));
+	memcpy(&hw->nvm.ops, ei->nvm_ops, sizeof(hw->nvm.ops));
+	memcpy(&hw->phy.ops, ei->phy_ops, sizeof(hw->phy.ops));
+
+	ei->get_variants(adapter);
+}
+
+static void e1000e_rerandomize(unsigned long delta)
+{
+	_e1000e_rerandomize(global_pdev_1, delta);
+	_e1000e_rerandomize(global_pdev_2, delta);
+}
+
+module_randomize(e1000e_rerandomize);
+
 /**
  * e1000_remove - Device Removal Routine
  * @pdev: PCI device information struct
@@ -7369,7 +7413,7 @@
  * Hot-Plug event, or because the driver is going to be removed from
  * memory.
  **/
-static void e1000_remove(struct pci_dev *pdev)
+SPECIAL_FUNCTION(static void, e1000_remove, struct pci_dev *pdev)
 {
 	struct net_device *netdev = pci_get_drvdata(pdev);
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -7431,7 +7475,7 @@
 }
 
 /* PCI Error Recovery (ERS) */
-static const struct pci_error_handlers e1000_err_handler = {
+SPECIAL_CONST_VAR(static const struct pci_error_handlers e1000_err_handler) = {
 	.error_detected = e1000_io_error_detected,
 	.slot_reset = e1000_io_slot_reset,
 	.resume = e1000_io_resume,
@@ -7550,7 +7594,7 @@
 };
 
 /* PCI Device API Driver */
-static struct pci_driver e1000_driver = {
+SPECIAL_VAR(static struct pci_driver e1000_driver) = {
 	.name     = e1000e_driver_name,
 	.id_table = e1000_pci_tbl,
 	.probe    = e1000_probe,
diff -urN linux-4.18.20/drivers/net/ethernet/intel/e1000e/ptp.c linux-4.18.20-new/drivers/net/ethernet/intel/e1000e/ptp.c
--- linux-4.18.20/drivers/net/ethernet/intel/e1000e/ptp.c	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/drivers/net/ethernet/intel/e1000e/ptp.c	2019-03-12 13:37:16.962908667 -0400
@@ -226,7 +226,7 @@
 	return -EOPNOTSUPP;
 }
 
-static void e1000e_systim_overflow_work(struct work_struct *work)
+SPECIAL_FUNCTION(static void, e1000e_systim_overflow_work, struct work_struct *work)
 {
 	struct e1000_adapter *adapter = container_of(work, struct e1000_adapter,
 						     systim_overflow_work.work);
diff -urN linux-4.18.20/include/linux/module.h linux-4.18.20-new/include/linux/module.h
--- linux-4.18.20/include/linux/module.h	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/include/linux/module.h	2019-03-12 13:37:16.962908667 -0400
@@ -73,6 +73,9 @@
 /* These are either module local, or the kernel's dummy ones. */
 extern int init_module(void);
 extern void cleanup_module(void);
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+extern void randomize_module(unsigned long);
+#endif
 
 #ifndef MODULE
 /**
@@ -137,6 +140,14 @@
 	{ return exitfn; }					\
 	void cleanup_module(void) __attribute__((alias(#exitfn)));
 
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+typedef void (*randomizecall_t)(unsigned long);
+#define module_randomize(randomizefn)					\
+	static inline randomizecall_t __maybe_unused __randomizetest(void)		\
+	{ return randomizefn; }					\
+	void randomize_module(unsigned long) __attribute__((alias(#randomizefn)));
+#endif
+
 #endif
 
 /* This means "can be init if no module support, otherwise module load
@@ -317,7 +328,7 @@
 	char *strtab;
 };
 
-#ifdef CONFIG_LIVEPATCH
+#if defined(CONFIG_LIVEPATCH) || defined(CONFIG_X86_MODULE_RERANDOMIZE)
 struct klp_modinfo {
 	Elf_Ehdr hdr;
 	Elf_Shdr *sechdrs;
@@ -393,6 +404,7 @@
 	/* Core layout: rbtree is accessed frequently, so keep together. */
 	struct module_layout core_layout __module_layout_align;
 	struct module_layout init_layout;
+	struct module_layout fixed_layout;
 
 	/* Arch-specific module values */
 	struct mod_arch_specific arch;
@@ -451,7 +463,7 @@
 	unsigned long *ftrace_callsites;
 #endif
 
-#ifdef CONFIG_LIVEPATCH
+#if defined(CONFIG_LIVEPATCH) || defined(CONFIG_X86_MODULE_RERANDOMIZE)
 	bool klp; /* Is this a livepatch module? */
 	bool klp_alive;
 
@@ -459,6 +471,11 @@
 	struct klp_modinfo *klp_info;
 #endif
 
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+	bool randomizable;
+	void (*rerandomize)(unsigned long);
+#endif
+
 #ifdef CONFIG_MODULE_UNLOAD
 	/* What modules depend on me? */
 	struct list_head source_list;
@@ -648,6 +665,19 @@
 }
 #endif /* CONFIG_LIVEPATCH */
 
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+void update_module_ref(struct module *mod, unsigned long delta);
+static inline bool is_randomizable_module(struct module *mod)
+{
+	return mod->randomizable;
+}
+#else /* !CONFIG_X86_MODULE_RERANDOMIZE */
+static inline bool is_randomizable_module(struct module *mod)
+{
+	return false;
+}
+#endif /* CONFIG_X86_MODULE_RERANDOMIZE */
+
 bool is_module_sig_enforced(void);
 
 #else /* !CONFIG_MODULES... */
@@ -795,11 +825,15 @@
 extern void set_all_modules_text_ro(void);
 extern void module_enable_ro(const struct module *mod, bool after_init);
 extern void module_disable_ro(const struct module *mod);
+extern void module_enable_nx(const struct module *mod);
+extern void module_disable_nx(const struct module *mod);
 #else
 static inline void set_all_modules_text_rw(void) { }
 static inline void set_all_modules_text_ro(void) { }
 static inline void module_enable_ro(const struct module *mod, bool after_init) { }
 static inline void module_disable_ro(const struct module *mod) { }
+void module_enable_nx(const struct module *mod) { }
+void module_disable_nx(const struct module *mod) { }
 #endif
 
 #ifdef CONFIG_GENERIC_BUG
diff -urN linux-4.18.20/include/linux/moduleloader.h linux-4.18.20-new/include/linux/moduleloader.h
--- linux-4.18.20/include/linux/moduleloader.h	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/include/linux/moduleloader.h	2019-03-12 13:37:16.962908667 -0400
@@ -19,6 +19,10 @@
 			      char *secstrings,
 			      struct module *mod);
 
+int module_arch_preinit(struct module *mod);
+bool module_is_fixed_section(struct module *mod, unsigned int shnum);
+bool module_is_fixed_section_name(const char *sname);
+
 /* Additional bytes needed by arch in front of individual sections */
 unsigned int arch_mod_section_prepend(struct module *mod, unsigned int section);
 
diff -urN linux-4.18.20/include/linux/vmalloc.h linux-4.18.20-new/include/linux/vmalloc.h
--- linux-4.18.20/include/linux/vmalloc.h	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/include/linux/vmalloc.h	2019-03-12 13:37:16.962908667 -0400
@@ -206,4 +206,11 @@
 int register_vmap_purge_notifier(struct notifier_block *nb);
 int unregister_vmap_purge_notifier(struct notifier_block *nb);
 
+void *remap_module(unsigned long addr, unsigned long size, unsigned long align,
+		unsigned long start, unsigned long end, gfp_t gfp_mask,
+		pgprot_t prot, unsigned long vm_flags, int node,
+		const void *caller);
+
+void unmap_module(const void *addr);
+
 #endif /* _LINUX_VMALLOC_H */
diff -urN linux-4.18.20/include/smr/smr.h linux-4.18.20-new/include/smr/smr.h
--- linux-4.18.20/include/smr/smr.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/include/smr/smr.h	2019-03-12 13:37:16.962908667 -0400
@@ -0,0 +1,18 @@
+#pragma once
+
+#define SMR_ORDER	6U /* 64 CPUs */
+#define SMR_NUM		(1U << SMR_ORDER)
+
+typedef struct _smr_header {
+	void *reserved[SMR_NUM+1];
+} smr_header;
+
+typedef struct _smr_handle {
+	unsigned long handle;
+	unsigned long vector;
+} smr_handle;
+
+void smr_init(void);
+smr_handle smr_enter(void);
+void smr_leave(smr_handle);
+int smr_retire(void *address);
diff -urN linux-4.18.20/init/main.c linux-4.18.20-new/init/main.c
--- linux-4.18.20/init/main.c	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/init/main.c	2019-03-12 13:37:16.962908667 -0400
@@ -92,6 +92,7 @@
 #include <linux/rodata_test.h>
 #include <linux/jump_label.h>
 #include <linux/mem_encrypt.h>
+#include <smr/smr.h>
 
 #include <asm/io.h>
 #include <asm/bugs.h>
@@ -533,6 +534,10 @@
 	char *command_line;
 	char *after_dashes;
 
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+	smr_init();
+#endif
+
 	set_task_stack_end_magic(&init_task);
 	smp_setup_processor_id();
 	debug_objects_early_init();
diff -urN linux-4.18.20/kernel/Makefile linux-4.18.20-new/kernel/Makefile
--- linux-4.18.20/kernel/Makefile	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/kernel/Makefile	2019-03-12 13:37:16.962908667 -0400
@@ -12,6 +12,8 @@
 	    notifier.o ksysfs.o cred.o reboot.o \
 	    async.o range.o smpboot.o ucount.o
 
+obj-$(CONFIG_X86_MODULE_RERANDOMIZE) += smr.o
+obj-$(CONFIG_X86_MODULE_RERANDOMIZER) += randmod.o
 obj-$(CONFIG_MODULES) += kmod.o
 obj-$(CONFIG_MULTIUSER) += groups.o
 
diff -urN linux-4.18.20/kernel/module.c linux-4.18.20-new/kernel/module.c
--- linux-4.18.20/kernel/module.c	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/kernel/module.c	2019-03-12 13:37:16.962908667 -0400
@@ -86,7 +86,8 @@
 #endif
 
 /* If this is set, the section belongs in the init part of the module */
-#define INIT_OFFSET_MASK (1UL << (BITS_PER_LONG-1))
+#define INIT_OFFSET_MASK  (1UL << (BITS_PER_LONG-1))
+#define FIXED_OFFSET_MASK (1UL << (BITS_PER_LONG-2))
 
 /*
  * Mutex protects:
@@ -1223,6 +1224,17 @@
 static struct module_attribute modinfo_coresize =
 	__ATTR(coresize, 0444, show_coresize, NULL);
 
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+static ssize_t show_fixedsize(struct module_attribute *mattr,
+			     struct module_kobject *mk, char *buffer)
+{
+	return sprintf(buffer, "%u\n", mk->mod->fixed_layout.size);
+}
+
+static struct module_attribute modinfo_fixedsize =
+	__ATTR(fixedsize, 0444, show_fixedsize, NULL);
+#endif
+
 static ssize_t show_initsize(struct module_attribute *mattr,
 			     struct module_kobject *mk, char *buffer)
 {
@@ -1252,6 +1264,9 @@
 	&modinfo_initstate,
 	&modinfo_coresize,
 	&modinfo_initsize,
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+	&modinfo_fixedsize,
+#endif
 	&modinfo_taint,
 #ifdef CONFIG_MODULE_UNLOAD
 	&modinfo_refcnt,
@@ -1941,6 +1956,10 @@
 	frob_ro_after_init(&mod->core_layout, set_memory_rw);
 	frob_text(&mod->init_layout, set_memory_rw);
 	frob_rodata(&mod->init_layout, set_memory_rw);
+
+	frob_text(&mod->fixed_layout, set_memory_rw);
+	frob_rodata(&mod->fixed_layout, set_memory_rw);
+	frob_ro_after_init(&mod->fixed_layout, set_memory_rw);
 }
 
 void module_enable_ro(const struct module *mod, bool after_init)
@@ -1953,26 +1972,39 @@
 	frob_text(&mod->init_layout, set_memory_ro);
 	frob_rodata(&mod->init_layout, set_memory_ro);
 
-	if (after_init)
+	frob_text(&mod->fixed_layout, set_memory_ro);
+	frob_rodata(&mod->fixed_layout, set_memory_ro);
+
+	if (after_init) {
 		frob_ro_after_init(&mod->core_layout, set_memory_ro);
+		frob_ro_after_init(&mod->fixed_layout, set_memory_ro);
+	}
 }
 
-static void module_enable_nx(const struct module *mod)
+void module_enable_nx(const struct module *mod)
 {
 	frob_rodata(&mod->core_layout, set_memory_nx);
 	frob_ro_after_init(&mod->core_layout, set_memory_nx);
 	frob_writable_data(&mod->core_layout, set_memory_nx);
 	frob_rodata(&mod->init_layout, set_memory_nx);
 	frob_writable_data(&mod->init_layout, set_memory_nx);
+
+	frob_rodata(&mod->fixed_layout, set_memory_nx);
+	frob_ro_after_init(&mod->fixed_layout, set_memory_nx);
+	frob_writable_data(&mod->fixed_layout, set_memory_nx);
 }
 
-static void module_disable_nx(const struct module *mod)
+void module_disable_nx(const struct module *mod)
 {
 	frob_rodata(&mod->core_layout, set_memory_x);
 	frob_ro_after_init(&mod->core_layout, set_memory_x);
 	frob_writable_data(&mod->core_layout, set_memory_x);
 	frob_rodata(&mod->init_layout, set_memory_x);
 	frob_writable_data(&mod->init_layout, set_memory_x);
+
+	frob_rodata(&mod->fixed_layout, set_memory_x);
+	frob_ro_after_init(&mod->fixed_layout, set_memory_x);
+	frob_writable_data(&mod->fixed_layout, set_memory_x);
 }
 
 /* Iterate through all modules and set each module's text as RW */
@@ -1990,6 +2022,7 @@
 
 		frob_text(&mod->core_layout, set_memory_rw);
 		frob_text(&mod->init_layout, set_memory_rw);
+		frob_text(&mod->fixed_layout, set_memory_rw);
 	}
 	mutex_unlock(&module_mutex);
 }
@@ -2015,6 +2048,7 @@
 
 		frob_text(&mod->core_layout, set_memory_ro);
 		frob_text(&mod->init_layout, set_memory_ro);
+		frob_text(&mod->fixed_layout, set_memory_ro);
 	}
 	mutex_unlock(&module_mutex);
 }
@@ -2033,11 +2067,9 @@
 
 #else
 static void disable_ro_nx(const struct module_layout *layout) { }
-static void module_enable_nx(const struct module *mod) { }
-static void module_disable_nx(const struct module *mod) { }
 #endif
 
-#ifdef CONFIG_LIVEPATCH
+#if defined(CONFIG_LIVEPATCH) || defined(CONFIG_X86_MODULE_RERANDOMIZE)
 /*
  * Persist Elf information about a module. Copy the Elf header,
  * section header table, section string table, and symtab section
@@ -2048,10 +2080,13 @@
 	unsigned int size, symndx;
 	int ret;
 
-	size = sizeof(*mod->klp_info);
-	mod->klp_info = kmalloc(size, GFP_KERNEL);
-	if (mod->klp_info == NULL)
-		return -ENOMEM;
+	if (is_randomizable_module(mod)) {
+		/* klp_info is already allocated */
+		size = sizeof(*mod->klp_info);
+		mod->klp_info = kmalloc(size, GFP_KERNEL);
+		if (mod->klp_info == NULL)
+			return -ENOMEM;
+	}
 
 	/* Elf header */
 	size = sizeof(mod->klp_info->hdr);
@@ -2094,6 +2129,7 @@
 	kfree(mod->klp_info->sechdrs);
 free_info:
 	kfree(mod->klp_info);
+	mod->klp_info = NULL;
 	return ret;
 }
 
@@ -2152,7 +2188,7 @@
 	/* Free any allocated parameters. */
 	destroy_params(mod->kp, mod->num_kp);
 
-	if (is_livepatch_module(mod))
+	if (is_livepatch_module(mod) || is_randomizable_module(mod))
 		free_module_elf(mod);
 
 	/* Now we can delete it from the lists */
@@ -2178,7 +2214,17 @@
 
 	/* Finally, free the core (containing the module structure) */
 	disable_ro_nx(&mod->core_layout);
-	module_memfree(mod->core_layout.base);
+	disable_ro_nx(&mod->fixed_layout);
+	/* struct module could either be in fixed or core
+	 * Free struct module at the end */
+	/* TODO: Find better way to do this */
+	if (is_randomizable_module(mod)) {
+		module_memfree(mod->core_layout.base);
+		module_memfree(mod->fixed_layout.base);
+	} else {
+		module_memfree(mod->fixed_layout.base);
+		module_memfree(mod->core_layout.base);
+	}
 }
 
 void *__symbol_get(const char *symbol)
@@ -2372,42 +2418,51 @@
 	};
 	unsigned int m, i;
 
-	for (i = 0; i < info->hdr->e_shnum; i++)
+	for (i = 0; i < info->hdr->e_shnum; i++) {
 		info->sechdrs[i].sh_entsize = ~0UL;
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+		if (is_randomizable_module(mod) &&
+				info->sechdrs[i].sh_type == SHT_RELA) {
+			info->sechdrs[i].sh_flags |= SHF_ALLOC;
+		}
+#endif
+	}
 
-	pr_debug("Core section allocation order:\n");
+if(is_randomizable_module(mod)) {
+	/* Fixed sections allocation */
+	pr_debug("Fixed section allocation order:\n");
 	for (m = 0; m < ARRAY_SIZE(masks); ++m) {
-		for (i = 0; i < info->hdr->e_shnum; ++i) {
+		for (i = 0; i < info->hdr->e_shnum; ++i){
 			Elf_Shdr *s = &info->sechdrs[i];
 			const char *sname = info->secstrings + s->sh_name;
-
-			if ((s->sh_flags & masks[m][0]) != masks[m][0]
+			if((s->sh_flags & masks[m][0]) != masks[m][0]
 			    || (s->sh_flags & masks[m][1])
 			    || s->sh_entsize != ~0UL
-			    || strstarts(sname, ".init"))
+			    || !module_is_fixed_section_name(sname))
 				continue;
-			s->sh_entsize = get_offset(mod, &mod->core_layout.size, s, i);
+			s->sh_entsize = (get_offset(mod, &mod->fixed_layout.size, s, i)
+								 | FIXED_OFFSET_MASK);
 			pr_debug("\t%s\n", sname);
 		}
 		switch (m) {
 		case 0: /* executable */
-			mod->core_layout.size = debug_align(mod->core_layout.size);
-			mod->core_layout.text_size = mod->core_layout.size;
+			mod->fixed_layout.size = debug_align(mod->fixed_layout.size);
+			mod->fixed_layout.text_size = mod->fixed_layout.size;
 			break;
 		case 1: /* RO: text and ro-data */
-			mod->core_layout.size = debug_align(mod->core_layout.size);
-			mod->core_layout.ro_size = mod->core_layout.size;
+			mod->fixed_layout.size = debug_align(mod->fixed_layout.size);
+			mod->fixed_layout.ro_size = mod->fixed_layout.size;
 			break;
 		case 2: /* RO after init */
-			mod->core_layout.size = debug_align(mod->core_layout.size);
-			mod->core_layout.ro_after_init_size = mod->core_layout.size;
+			mod->fixed_layout.size = debug_align(mod->fixed_layout.size);
+			mod->fixed_layout.ro_after_init_size = mod->fixed_layout.size;
 			break;
-		case 4: /* whole core */
-			mod->core_layout.size = debug_align(mod->core_layout.size);
+		case 4: /* whole init */
+			mod->fixed_layout.size = debug_align(mod->fixed_layout.size);
 			break;
 		}
 	}
-
+} else {
 	pr_debug("Init section allocation order:\n");
 	for (m = 0; m < ARRAY_SIZE(masks); ++m) {
 		for (i = 0; i < info->hdr->e_shnum; ++i) {
@@ -2444,6 +2499,39 @@
 			break;
 		}
 	}
+} /* is_randomizable_module(mod) */
+
+	pr_debug("Core section allocation order:\n");
+	for (m = 0; m < ARRAY_SIZE(masks); ++m) {
+		for (i = 0; i < info->hdr->e_shnum; ++i) {
+			Elf_Shdr *s = &info->sechdrs[i];
+			const char *sname = info->secstrings + s->sh_name;
+
+			if ((s->sh_flags & masks[m][0]) != masks[m][0]
+			    || (s->sh_flags & masks[m][1])
+			    || s->sh_entsize != ~0UL)
+				continue;
+			s->sh_entsize = get_offset(mod, &mod->core_layout.size, s, i);
+			pr_debug("\t%s\n", sname);
+		}
+		switch (m) {
+		case 0: /* executable */
+			mod->core_layout.size = debug_align(mod->core_layout.size);
+			mod->core_layout.text_size = mod->core_layout.size;
+			break;
+		case 1: /* RO: text and ro-data */
+			mod->core_layout.size = debug_align(mod->core_layout.size);
+			mod->core_layout.ro_size = mod->core_layout.size;
+			break;
+		case 2: /* RO after init */
+			mod->core_layout.size = debug_align(mod->core_layout.size);
+			mod->core_layout.ro_after_init_size = mod->core_layout.size;
+			break;
+		case 4: /* whole core */
+			mod->core_layout.size = debug_align(mod->core_layout.size);
+			break;
+		}
+	}
 }
 
 static void set_license(struct module *mod, const char *license)
@@ -2630,6 +2718,7 @@
 	/* Compute total space required for the core symbols' strtab. */
 	for (ndst = i = 0; i < nsrc; i++) {
 		if (i == 0 || is_livepatch_module(mod) ||
+		    is_randomizable_module(mod) ||
 		    is_core_symbol(src+i, info->sechdrs, info->hdr->e_shnum,
 				   info->index.pcpu)) {
 			strtab_size += strlen(&info->strtab[src[i].st_name])+1;
@@ -2689,6 +2778,7 @@
 	src = mod->kallsyms->symtab;
 	for (ndst = i = 0; i < mod->kallsyms->num_symtab; i++) {
 		if (i == 0 || is_livepatch_module(mod) ||
+		    is_randomizable_module(mod) ||
 		    is_core_symbol(src+i, info->sechdrs, info->hdr->e_shnum,
 				   info->index.pcpu)) {
 			dst[ndst] = src[i];
@@ -3044,6 +3134,12 @@
 	if (err)
 		return err;
 
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+	if (get_modinfo(info, "randomizable")) {
+		mod->randomizable = true;
+	}
+#endif
+
 	/* Set up license info based on the info section */
 	set_license(mod, get_modinfo(info, "license"));
 
@@ -3159,6 +3255,21 @@
 	memset(ptr, 0, mod->core_layout.size);
 	mod->core_layout.base = ptr;
 
+	/* Allocate fixed section */
+	if (mod->fixed_layout.size) {
+		ptr = module_alloc(mod->fixed_layout.size);
+		kmemleak_not_leak(ptr);
+		if (!ptr) {
+			module_memfree(mod->core_layout.base);
+			return -ENOMEM;
+		}
+
+		memset(ptr, 0, mod->fixed_layout.size);
+		mod->fixed_layout.base = ptr;
+	} else {
+		mod->fixed_layout.base = NULL;
+	}
+
 	if (mod->init_layout.size) {
 		ptr = module_alloc(mod->init_layout.size);
 		/*
@@ -3169,6 +3280,9 @@
 		 */
 		kmemleak_ignore(ptr);
 		if (!ptr) {
+			if (mod->fixed_layout.base) {
+				module_memfree(mod->fixed_layout.base);
+			}
 			module_memfree(mod->core_layout.base);
 			return -ENOMEM;
 		}
@@ -3189,6 +3303,9 @@
 		if (shdr->sh_entsize & INIT_OFFSET_MASK)
 			dest = mod->init_layout.base
 				+ (shdr->sh_entsize & ~INIT_OFFSET_MASK);
+		else if (shdr->sh_entsize & FIXED_OFFSET_MASK)
+			dest = mod->fixed_layout.base
+				+ (shdr->sh_entsize & ~FIXED_OFFSET_MASK);
 		else
 			dest = mod->core_layout.base + shdr->sh_entsize;
 
@@ -3263,6 +3380,9 @@
 				   + mod->init_layout.size);
 	flush_icache_range((unsigned long)mod->core_layout.base,
 			   (unsigned long)mod->core_layout.base + mod->core_layout.size);
+	if (mod->fixed_layout.base)
+		flush_icache_range((unsigned long)mod->fixed_layout.base,
+			   (unsigned long)mod->fixed_layout.base + mod->fixed_layout.size);
 
 	set_fs(old_fs);
 }
@@ -3275,6 +3395,11 @@
 	return 0;
 }
 
+int __weak module_arch_preinit(struct module *mod)
+{
+	return 0;
+}
+
 /* module_blacklist is a comma-separated list of module names */
 static char *module_blacklist;
 static bool blacklisted(const char *module_name)
@@ -3314,11 +3439,22 @@
 	if (err)
 		return ERR_PTR(err);
 
+	if (is_randomizable_module(mod)) {
+		/* Early set up klp_info with temporary data */
+		mod->klp_info = kmalloc(sizeof(*mod->klp_info), GFP_KERNEL);
+		if (mod->klp_info == NULL)
+			return ERR_PTR(-ENOMEM);
+		memcpy(&mod->klp_info->hdr, info->hdr, sizeof(mod->klp_info->hdr));
+		mod->klp_info->symndx = info->index.sym;
+		mod->klp_info->secstrings = info->secstrings;
+		mod->klp_info->sechdrs = info->sechdrs;
+	}
+
 	/* Allow arches to frob section contents and sizes.  */
 	err = module_frob_arch_sections(info->hdr, info->sechdrs,
 					info->secstrings, mod);
 	if (err < 0)
-		return ERR_PTR(err);
+		goto free_klp;
 
 	/* We will do a special allocation for per-cpu sections later. */
 	info->sechdrs[info->index.pcpu].sh_flags &= ~(unsigned long)SHF_ALLOC;
@@ -3341,12 +3477,17 @@
 	/* Allocate and move to the final place */
 	err = move_module(mod, info);
 	if (err)
-		return ERR_PTR(err);
+		goto free_klp;
 
 	/* Module has been copied to its final place now: return it. */
 	mod = (void *)info->sechdrs[info->index.mod].sh_addr;
 	kmemleak_load_module(mod, info);
 	return mod;
+
+ free_klp:
+	if (is_randomizable_module(mod))
+		kfree(mod->klp_info);
+	return ERR_PTR(err);
 }
 
 /* mod is no longer valid after this! */
@@ -3355,7 +3496,16 @@
 	percpu_modfree(mod);
 	module_arch_freeing_init(mod);
 	module_memfree(mod->init_layout.base);
-	module_memfree(mod->core_layout.base);
+	/* struct module could either be in fixed or core
+	 * Free struct module at the end */
+	/* TODO: Find better way to do this */
+	if (is_randomizable_module(mod)) {
+		module_memfree(mod->core_layout.base);
+		module_memfree(mod->fixed_layout.base);
+	} else {
+		module_memfree(mod->fixed_layout.base);
+		module_memfree(mod->core_layout.base);
+	}
 }
 
 int __weak module_finalize(const Elf_Ehdr *hdr,
@@ -3770,7 +3920,7 @@
 	if (err < 0)
 		goto coming_cleanup;
 
-	if (is_livepatch_module(mod)) {
+	if (is_livepatch_module(mod) || is_randomizable_module(mod)) {
 		err = copy_module_elf(mod, info);
 		if (err < 0)
 			goto sysfs_cleanup;
@@ -3782,6 +3932,8 @@
 	/* Done! */
 	trace_module_load(mod);
 
+	module_arch_preinit(mod);
+
 	return do_init_module(mod);
 
  sysfs_cleanup:
@@ -4379,6 +4531,40 @@
 	pr_cont("\n");
 }
 
+#ifdef CONFIG_X86_MODULE_RERANDOMIZE
+static void update_module_klp(struct module *mod, unsigned long delta)
+{
+	unsigned int i;
+	Elf_Shdr *shdrs = mod->klp_info->sechdrs;
+
+//	printk("update_module_klp\n");
+
+	/* Update Elf_Shdr.sh_addr of core sections */
+	for (i = 1; i < mod->klp_info->hdr.e_shnum; i++) {
+		if (!module_is_fixed_section(mod, i))
+			INC_BY_DELTA(shdrs[i].sh_addr, delta);
+	}
+
+}
+
+void update_module_ref(struct module *mod, unsigned long delta){
+//	printk("update_module_ref\n");
+
+	mutex_lock(&module_mutex);
+
+	mod_tree_remove(mod);
+
+	INC_BY_DELTA(mod->core_layout.base, delta);
+
+	mod_update_bounds(mod);
+	mod_tree_insert(mod);
+
+	update_module_klp(mod, delta);
+
+	mutex_unlock(&module_mutex);
+}
+#endif /* CONFIG_X86_MODULE_RERANDOMIZE */
+
 #ifdef CONFIG_MODVERSIONS
 /* Generate the signature for all relevant module structures here.
  * If these change, we don't want to try to parse the module. */
diff -urN linux-4.18.20/kernel/randmod.c linux-4.18.20-new/kernel/randmod.c
--- linux-4.18.20/kernel/randmod.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/kernel/randmod.c	2019-03-12 13:37:16.962908667 -0400
@@ -0,0 +1,161 @@
+#include <linux/module.h>	/* Needed by all modules */
+#include <linux/kernel.h>	/* Needed for KERN_INFO */
+#include <linux/moduleparam.h>
+#include <linux/delay.h>
+#include <linux/list.h>
+#include <linux/debugfs.h>
+#include <linux/uaccess.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+
+// Macros
+#define printi(x) printk("%d." #x " = %d\n", __LINE__, x)
+
+static char *module_name = "ioctrl";
+module_param(module_name, charp, 0000);
+MODULE_PARM_DESC(module_name, "Name of module to relocate");
+
+static int manual_unmap = false;
+module_param(manual_unmap, int, 0);
+MODULE_PARM_DESC(manual_unmap, "Unmap old memory after relocation?");
+
+static int rand_period = 20;
+module_param(rand_period, int, 0);
+MODULE_PARM_DESC(rand_period, "Randomization Period in ms");
+
+static struct workqueue_struct *my_wq = NULL;
+
+typedef struct {
+    struct delayed_work my_work;
+    void *address;
+} UnmapWork;
+
+static void delayed_unmap_cb(struct work_struct *work)
+{
+    UnmapWork *my_work = (UnmapWork *)work;
+
+	printk("Manual Memory Freed %lx\n", (unsigned long) my_work->address);
+	unmap_module(my_work->address);
+	kfree( (void *)work );
+}
+
+int delayed_unmap(void *address, int delay)
+{
+	UnmapWork *work = kzalloc(sizeof(*work), GFP_KERNEL);
+	if(!work)
+		return -1;
+
+	work->address = address;
+	INIT_DELAYED_WORK((struct delayed_work *)work, delayed_unmap_cb);
+	queue_delayed_work(my_wq, (struct delayed_work *)work, msecs_to_jiffies(delay));
+	
+	return 0;
+}
+
+int randomize(const char *name){
+	void *oldAddr, *newAddr;
+	struct module *mod;
+
+	// printk(KERN_INFO "------ Start Randomization ------\n");
+
+	mod = find_module(name);
+
+	if(mod == NULL){
+		pr_err("Module %s not found\n", name);
+		return -1;
+	}
+
+	if(!is_randomizable_module(mod)){
+		pr_err("Module %s is not randomizable\n", name);
+		return -1;
+	}
+
+	// printk("Randomizing %s\n", mod->name);
+
+	oldAddr = mod->core_layout.base;
+
+	newAddr = module_rerandomize(mod);
+	// printk("Old Address = 0x%lx", (unsigned long)oldAddr);
+	// printk("New Address = 0x%lx", (unsigned long)newAddr);
+
+	if(manual_unmap){
+		delayed_unmap(oldAddr, manual_unmap);
+	}
+
+	// printk(KERN_INFO "------ End Randomization ------\n");
+
+	return 0;
+}
+
+
+static struct task_struct *kthread = NULL;
+int work_func(void *args){
+	int ret;
+	unsigned long min = 1000LU * rand_period;
+	unsigned long max = min + 1000;
+	printk("Randomize: kthread started\n");
+	do{
+		ret = randomize(module_name);
+		if(ret)
+			break;
+
+		if(rand_period == 0)
+			break;
+
+		if(rand_period > 20) {
+			msleep(rand_period);
+		} else {
+			usleep_range(min, max);
+		}
+	}while(!kthread_should_stop());
+
+	kthread = NULL;
+
+	return 0;
+}
+
+
+int init_module(void){
+	printk("Module Name: %s\n", module_name);
+	printk("Period: %d\n", rand_period);
+	printk("Manual Unmap: %d\n", manual_unmap);
+
+	/* Init WorkQueue */
+	if(manual_unmap){
+		my_wq = create_workqueue("unmap_queue");
+	
+		if(!my_wq){
+			pr_err("Could not create workqueue\n");
+			return -1;
+		}
+	}
+
+	/* Start worker kthread */
+	kthread = kthread_run(work_func, NULL, "randomizer");
+	if(kthread == ERR_PTR(-ENOMEM)){
+		pr_err("Could not run kthread\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+void cleanup_module(void){
+	if(kthread){
+		kthread_stop(kthread);
+		printk("Randomize: kthread stopped\n");
+	}
+
+	if(manual_unmap){
+		/* allow delayed unmap */
+		mdelay(manual_unmap);
+		mdelay(500);
+
+		if(my_wq){
+			flush_workqueue( my_wq );
+			destroy_workqueue( my_wq );
+		}
+	}
+}
+
+MODULE_LICENSE("GPL v2");
diff -urN linux-4.18.20/kernel/smr/bits/c11.h linux-4.18.20-new/kernel/smr/bits/c11.h
--- linux-4.18.20/kernel/smr/bits/c11.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/kernel/smr/bits/c11.h	2019-03-12 13:46:02.636367346 -0400
@@ -0,0 +1,153 @@
+/*
+ Copyright (c) 2018, Ruslan Nikolaev
+ All rights reserved.
+
+ Redistribution and use in source and binary forms, with or without
+ modification, are permitted provided that the following conditions are met:
+
+ 1. Redistributions of source code must retain the above copyright notice, this
+    list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright notice,
+    this list of conditions and the following disclaimer in the documentation
+    and/or other materials provided with the distribution.
+
+ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+*/
+
+#ifndef __LF_C11_H
+#define __LF_C11_H 1
+
+#include <stdatomic.h>
+
+#define LFATOMIC(x)				_Atomic(x)
+#define LFATOMIC_VAR_INIT(x)	ATOMIC_VAR_INIT(x)
+
+static inline void __lfaba_init(_Atomic(lfatomic_big_t) * obj,
+		lfatomic_big_t val)
+{
+	atomic_init(obj, val);
+}
+
+static inline lfatomic_big_t __lfaba_load(_Atomic(lfatomic_big_t) * obj,
+		memory_order order)
+{
+#if __LFLOAD_SPLIT(LFATOMIC_BIG_WIDTH) == 1
+	lfatomic_big_t res;
+	_Atomic(lfatomic_t) * hobj = (_Atomic(lfatomic_t) *) obj;
+	lfatomic_t * hres = (lfatomic_t *) &res;
+
+	hres[0] = atomic_load_explicit(hobj, order);
+	hres[1] = atomic_load_explicit(hobj + 1, order);
+	return res;
+#elif __LFLOAD_SPLIT(LFATOMIC_BIG_WIDTH) == 0
+	return atomic_load_explicit(obj, order);
+#endif
+}
+
+static inline bool __lfaba_cmpxchg_weak(_Atomic(lfatomic_big_t) * obj,
+	lfatomic_big_t * expected, lfatomic_big_t desired,
+	memory_order succ, memory_order fail)
+{
+	return atomic_compare_exchange_weak_explicit(obj, expected, desired,
+					succ, fail);
+}
+
+static inline bool __lfaba_cmpxchg_strong(_Atomic(lfatomic_big_t) * obj,
+	lfatomic_big_t * expected, lfatomic_big_t desired,
+	memory_order succ, memory_order fail)
+{
+	return atomic_compare_exchange_strong_explicit(obj, expected, desired,
+					succ, fail);
+}
+
+#define __lfepoch_init			atomic_init
+#define __lfepoch_load			atomic_load_explicit
+#define __lfepoch_cmpxchg_weak	atomic_compare_exchange_weak_explicit
+#define __lfepoch_fetch_add		atomic_fetch_add_explicit
+
+#define __LFREF_CMPXCHG_FULL(dtype_t)	(1)
+
+#define __LFREF_ATOMICS_IMPL(w, type_t, dtype_t)							\
+static inline void __lfref_init##w(_Atomic(dtype_t) * obj, dtype_t val)		\
+{																			\
+	atomic_init(obj, val);													\
+}																			\
+																			\
+static inline dtype_t __lfref_load##w(_Atomic(dtype_t) * obj,				\
+		memory_order order)													\
+{																			\
+	if (!__LFLOAD_SPLIT(sizeof(dtype_t) * 8)) {								\
+		return atomic_load_explicit(obj, order);							\
+	} else {																\
+		dtype_t res;														\
+		_Atomic(type_t) * hobj = (_Atomic(type_t) *) obj;					\
+		type_t * hres = (type_t *) &res;									\
+																			\
+		hres[0] = atomic_load_explicit(hobj, order);						\
+		hres[1] = atomic_load_explicit(hobj + 1, order);					\
+		return res;															\
+	}																		\
+}																			\
+																			\
+static inline type_t __lfref_link##w(_Atomic(dtype_t) * obj,				\
+		memory_order order)													\
+{																			\
+	if (!__LFLOAD_SPLIT(sizeof(dtype_t) * 8)) {								\
+		return (atomic_load_explicit(obj, order) & ~__lfref_mask##w) >>		\
+					__lfrptr_shift##w;										\
+	} else {																\
+		_Atomic(type_t) * hobj = (_Atomic(type_t) *) obj;					\
+		return atomic_load_explicit(&hobj[__LFREF_LINK], order);			\
+	}																		\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgptr_weak##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	return atomic_compare_exchange_weak_explicit(obj, expected, desired,	\
+			succ, fail);													\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgptr_strong##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	return atomic_compare_exchange_strong_explicit(obj, expected, desired,	\
+			succ, fail);													\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgref_weak##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	return atomic_compare_exchange_weak_explicit(obj, expected, desired,	\
+			succ, fail);													\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgref_strong##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	return atomic_compare_exchange_strong_explicit(obj, expected, desired,	\
+			succ, fail);													\
+}																			\
+																			\
+static inline dtype_t __lfref_fetch_add##w(_Atomic(dtype_t) * obj,			\
+		dtype_t arg, memory_order order)									\
+{																			\
+	return atomic_fetch_add_explicit(obj, arg, order);						\
+}
+
+#endif /* !__LF_C11_H */
+
+/* vi: set tabstop=4: */
diff -urN linux-4.18.20/kernel/smr/bits/config.h linux-4.18.20-new/kernel/smr/bits/config.h
--- linux-4.18.20/kernel/smr/bits/config.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/kernel/smr/bits/config.h	2019-03-12 13:46:22.560571498 -0400
@@ -0,0 +1,104 @@
+/*
+ Copyright (c) 2017, Ruslan Nikolaev
+ All rights reserved.
+
+ Redistribution and use in source and binary forms, with or without
+ modification, are permitted provided that the following conditions are met:
+
+ 1. Redistributions of source code must retain the above copyright notice, this
+    list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright notice,
+    this list of conditions and the following disclaimer in the documentation
+    and/or other materials provided with the distribution.
+
+ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+*/
+
+#ifndef __BITS_LFCONFIG_H
+#define __BITS_LFCONFIG_H	1
+
+#ifndef __KERNEL__
+# include <inttypes.h>
+#else
+# include <linux/types.h>
+#endif
+
+/* For the following architectures, it is cheaper to use split (word-atomic)
+   loads whenever possible. */
+#if defined(__i386__) || defined(__x86_64__) || defined(__arm__) ||	\
+	defined(__aarch64__)
+# define __LFLOAD_SPLIT(dtype_width)	(dtype_width > LFATOMIC_WIDTH)
+#else
+# define __LFLOAD_SPLIT(dtype_width)	0
+#endif
+
+/* IA-64 provides a 128-bit single-compare/double-swap instruction, so
+   LFCMPXCHG_SPLIT is true for 128-bit types. */
+#if defined(__ia64__)
+# define __LFCMPXCHG_SPLIT(dtype_width)	(dtype_width > LFATOMIC_WIDTH)
+#else
+# define __LFCMPXCHG_SPLIT(dtype_width)	0
+#endif
+
+#if defined(__x86_64__) || defined (__aarch64__) || defined(__powerpc64__)	\
+	|| (defined(__mips__) && _MIPS_SIM == _MIPS_SIM_ABI64)
+typedef uint64_t lfepoch_t;
+typedef int64_t lfepoch_signed_t;
+typedef uint64_t lfatomic_t;
+typedef __uint128_t lfatomic_big_t;
+# define LFATOMIC_LOG2			3
+# define LFATOMIC_WIDTH			64
+# define LFATOMIC_BIG_WIDTH		128
+#elif defined(__i386__) || defined(__arm__) || defined(__powerpc__)			\
+	|| (defined(__mips__) &&												\
+		(_MIPS_SIM == _MIPS_SIM_ABI32 || _MIPS_SIM == _MIPS_SIM_NABI32))
+# if defined(__i386__)
+typedef uint64_t lfepoch_t; /* Still makes sense to use 64-bit numbers. */
+typedef int64_t lfepoch_signed_t;
+# else
+typedef uint32_t lfepoch_t;
+typedef int32_t lfepoch_signed_t;
+# endif
+typedef uint32_t lfatomic_t;
+typedef uint64_t lfatomic_big_t;
+# define LFATOMIC_LOG2			2
+# define LFATOMIC_WIDTH			32
+# define LFATOMIC_BIG_WIDTH		64
+#else
+typedef uintptr_t lfepoch_t;
+typedef uintptr_t lfatomic_t;
+typedef uintptr_t lfatomic_big_t;
+# if UINTPTR_MAX == UINT32_C(0xFFFFFFFF)
+#  define LFATOMIC_LOG2			2
+#  define LFATOMIC_WIDTH		32
+#  define LFATOMIC_BIG_WIDTH	32
+# elif UINTPTR_MAX == UINT64_C(0xFFFFFFFFFFFFFFFF)
+#  define LFATOMIC_LOG2			3
+#  define LFATOMIC_WIDTH		64
+#  define LFATOMIC_BIG_WIDTH	64
+# endif
+#endif
+
+/* XXX: True for x86/x86-64 but needs to be properly defined for other CPUs. */
+#define LF_CACHE_SHIFT		7U
+#define LF_CACHE_BYTES		(1U << LF_CACHE_SHIFT)
+
+/* Allow to use LEA for x86/x86-64. */
+#if defined(__i386__) || defined(__x86_64__)
+# define __LFMERGE(x,y)	((x) + (y))
+#else
+# define __LFMERGE(x,y)	((x) | (y))
+#endif
+
+#endif /* !__BITS_LFCONFIG_H */
+
+/* vi: set tabstop=4: */
diff -urN linux-4.18.20/kernel/smr/bits/gcc_x86.h linux-4.18.20-new/kernel/smr/bits/gcc_x86.h
--- linux-4.18.20/kernel/smr/bits/gcc_x86.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/kernel/smr/bits/gcc_x86.h	2019-03-12 13:46:38.364733340 -0400
@@ -0,0 +1,224 @@
+/*
+ Copyright (c) 2018, Ruslan Nikolaev
+ All rights reserved.
+
+ Redistribution and use in source and binary forms, with or without
+ modification, are permitted provided that the following conditions are met:
+
+ 1. Redistributions of source code must retain the above copyright notice, this
+    list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright notice,
+    this list of conditions and the following disclaimer in the documentation
+    and/or other materials provided with the distribution.
+
+ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+*/
+
+#ifndef __LF_GCC_X86_H
+#define __LF_GCC_X86_H 1
+
+#include <stdatomic.h>
+
+#define LFATOMIC(x)				_Atomic(x)
+#define LFATOMIC_VAR_INIT(x)	ATOMIC_VAR_INIT(x)
+
+static inline void __lfbig_init(_Atomic(lfatomic_big_t) * obj,
+		lfatomic_big_t val)
+{
+	*((volatile lfatomic_big_t *) obj) = val;
+}
+
+static inline lfatomic_big_t __lfbig_load(_Atomic(lfatomic_big_t) * obj,
+		memory_order order)
+{
+	return *((volatile lfatomic_big_t *) obj);
+}
+
+static inline bool __lfbig_cmpxchg_strong(_Atomic(lfatomic_big_t) * obj,
+		lfatomic_big_t * expected, lfatomic_big_t desired,
+		memory_order succ, memory_order fail)
+{
+	lfatomic_t low = (lfatomic_t) desired;
+	lfatomic_t high = (lfatomic_t) (desired >> (sizeof(lfatomic_t) * 8));
+	bool result;
+
+#if defined(__x86_64__)
+# define __LFX86_CMPXCHG "cmpxchg16b"
+#elif defined(__i386__)
+# define __LFX86_CMPXCHG "cmpxchg8b"
+#endif
+	__asm__ __volatile__ ("lock " __LFX86_CMPXCHG " %0"
+						  : "+m" (*obj), "=@ccz" (result), "+A" (*expected)
+						  : "b" (low), "c" (high)
+	);
+#undef __LFX86_CMPXCHG
+
+	return result;
+}
+
+static inline bool __lfbig_cmpxchg_weak(_Atomic(lfatomic_big_t) * obj,
+		lfatomic_big_t * expected, lfatomic_big_t desired,
+		memory_order succ, memory_order fail)
+{
+	return __lfbig_cmpxchg_strong(obj, expected, desired, succ, fail);
+}
+
+static inline lfatomic_big_t __lfbig_fetch_add(_Atomic(lfatomic_big_t) * obj,
+		lfatomic_big_t arg, memory_order order)
+{
+	lfatomic_big_t new_val, old_val = __lfbig_load(obj, order);
+	do {
+		new_val = old_val + arg;
+	} while (!__lfbig_cmpxchg_weak(obj, &old_val, new_val, order, order));
+	__LF_ASSUME(new_val == old_val + arg);
+	return old_val;
+}
+
+#define __lfaba_init			__lfbig_init
+#define __lfaba_load			__lfbig_load
+#define __lfaba_cmpxchg_weak	__lfbig_cmpxchg_weak
+#define __lfaba_cmpxchg_strong	__lfbig_cmpxchg_strong
+
+static inline void __lfepoch_init(_Atomic(lfepoch_t) * obj, lfepoch_t val)
+{
+	atomic_init(obj, val);
+}
+
+static inline lfepoch_t __lfepoch_load(_Atomic(lfepoch_t) * obj,
+		memory_order order)
+{
+	/* Already uses atomic 64-bit FPU loads for i386. */
+	return atomic_load_explicit(obj, order);
+}
+
+static inline bool __lfepoch_cmpxchg_weak(_Atomic(lfepoch_t) * obj,
+		lfepoch_t * expected, lfepoch_t desired,
+		memory_order succ, memory_order fail)
+{
+#ifdef __i386__
+	return __lfbig_cmpxchg_weak((_Atomic(lfatomic_big_t) *) obj,
+				(lfatomic_big_t *) expected, desired, succ, fail);
+#else
+	return atomic_compare_exchange_weak_explicit(obj, expected, desired,
+				succ, fail);
+#endif
+}
+
+static inline lfepoch_t __lfepoch_fetch_add(_Atomic(lfepoch_t) * obj,
+		lfepoch_t arg, memory_order order)
+{
+#ifdef __i386__
+	return __lfbig_fetch_add((_Atomic(lfatomic_big_t) *) obj, arg, order);
+#else
+	return atomic_fetch_add_explicit(obj, arg, order);
+#endif
+}
+
+#define __LFREF_CMPXCHG_FULL(dtype_t)	(1)
+
+#define __LFREF_ATOMICS_IMPL(w, type_t, dtype_t)							\
+static inline void __lfref_init##w(_Atomic(dtype_t) * obj, dtype_t val)		\
+{																			\
+	if (sizeof(dtype_t) != sizeof(lfatomic_big_t)) {						\
+		atomic_init(obj, val);												\
+	} else {																\
+		__lfbig_init((_Atomic(lfatomic_big_t) *) obj, val);					\
+	}																		\
+}																			\
+																			\
+static inline dtype_t __lfref_load##w(_Atomic(dtype_t) * obj,				\
+		memory_order order)													\
+{																			\
+	if (sizeof(dtype_t) != sizeof(lfatomic_big_t)) {						\
+		return atomic_load_explicit(obj, order);							\
+	} else {																\
+		return __lfbig_load((_Atomic(lfatomic_big_t) *) obj, order);		\
+	}																		\
+}																			\
+																			\
+static inline type_t __lfref_link##w(_Atomic(dtype_t) * obj,				\
+		memory_order order)													\
+{																			\
+	if (sizeof(dtype_t) != sizeof(lfatomic_big_t)) {						\
+		return (atomic_load_explicit(obj, order) & ~__lfref_mask##w) >>		\
+					__lfrptr_shift##w;										\
+	} else {																\
+		return *((volatile type_t *) obj + __LFREF_LINK);					\
+	}																		\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgptr_weak##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	if (sizeof(dtype_t) != sizeof(lfatomic_big_t)) {						\
+		return atomic_compare_exchange_weak_explicit(obj,					\
+			expected, desired, succ, fail);									\
+	} else {																\
+		return __lfbig_cmpxchg_weak((_Atomic(lfatomic_big_t) *) obj,		\
+				(lfatomic_big_t *) expected, desired, succ, fail);			\
+	}																		\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgptr_strong##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	if (sizeof(dtype_t) != sizeof(lfatomic_big_t)) {						\
+		return atomic_compare_exchange_strong_explicit(obj,					\
+			expected, desired, succ, fail);									\
+	} else {																\
+		return __lfbig_cmpxchg_strong((_Atomic(lfatomic_big_t) *) obj,		\
+				(lfatomic_big_t *) expected, desired, succ, fail);			\
+	}																		\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgref_weak##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	if (sizeof(dtype_t) != sizeof(lfatomic_big_t)) {						\
+		return atomic_compare_exchange_weak_explicit(obj,					\
+			expected, desired, succ, fail);									\
+	} else {																\
+		return __lfbig_cmpxchg_weak((_Atomic(lfatomic_big_t) *) obj,		\
+				(lfatomic_big_t *) expected, desired, succ, fail);			\
+	}																		\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgref_strong##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	if (sizeof(dtype_t) != sizeof(lfatomic_big_t)) {						\
+		return atomic_compare_exchange_strong_explicit(obj,					\
+			expected, desired, succ, fail);									\
+	} else {																\
+		return __lfbig_cmpxchg_strong((_Atomic(lfatomic_big_t) *) obj,		\
+				(lfatomic_big_t *) expected, desired, succ, fail);			\
+	}																		\
+}																			\
+																			\
+static inline dtype_t __lfref_fetch_add##w(_Atomic(dtype_t) * obj,			\
+		dtype_t arg, memory_order order)									\
+{																			\
+	if (sizeof(dtype_t) != sizeof(lfatomic_big_t)) {						\
+		return atomic_fetch_add_explicit(obj, arg, order);					\
+	} else {																\
+		return __lfbig_fetch_add((_Atomic(lfatomic_big_t) *) obj,			\
+				arg, order);												\
+	}																		\
+}
+
+#endif /* !__LF_GGC_X86_H */
+
+/* vi: set tabstop=4: */
diff -urN linux-4.18.20/kernel/smr/bits/lf.h linux-4.18.20-new/kernel/smr/bits/lf.h
--- linux-4.18.20/kernel/smr/bits/lf.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/kernel/smr/bits/lf.h	2019-03-12 13:47:07.537031864 -0400
@@ -0,0 +1,159 @@
+/*
+ Copyright (c) 2017, Ruslan Nikolaev
+ All rights reserved.
+
+ Redistribution and use in source and binary forms, with or without
+ modification, are permitted provided that the following conditions are met:
+
+ 1. Redistributions of source code must retain the above copyright notice, this
+    list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright notice,
+    this list of conditions and the following disclaimer in the documentation
+    and/or other materials provided with the distribution.
+
+ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+*/
+
+#ifndef __BITS_LF_H
+#define __BITS_LF_H	1
+
+#ifndef __KERNEL__
+# include <inttypes.h>
+# include <sys/types.h>
+# include <stddef.h>
+# include <stdbool.h>
+# if UINTPTR_MAX == UINT32_C(0xFFFFFFFF)
+#  define __LFPTR_WIDTH	32
+typedef uint64_t lfref_t;
+# elif UINTPTR_MAX == UINT64_C(0xFFFFFFFFFFFFFFFF)
+#  define __LFPTR_WIDTH	64
+typedef __uint128_t lfref_t;
+# else
+#  error "Unsupported word length."
+# endif
+#else
+#include <linux/types.h>
+# ifdef CONFIG_64BIT
+#  define __LFPTR_WIDTH 64
+typedef __uint128_t lfref_t;
+# else
+#  define __LFPTR_WIDTH 32
+typedef uint64_t lfref_t;
+# endif
+#endif
+
+#include "config.h"
+
+#ifdef __GNUC__
+# define __LF_ASSUME(c) do { if (!(c)) __builtin_unreachable(); } while (0)
+#else
+# define __LF_ASSUME(c)
+#endif
+
+/* GCC does not have a sane implementation of wide atomics for x86-64
+   in recent versions, so use inline assembly workarounds whenever possible.
+   No aarch64 support in GCC for right now. */
+#if (defined(__i386__) || defined(__x86_64__)) && defined(__GNUC__) &&	\
+	!defined(__llvm__) && defined(__GCC_ASM_FLAG_OUTPUTS__)
+# include "gcc_x86.h"
+#elif (defined(__i386__) || defined(__x86_64__)) && defined(__llvm__)
+# include "llvm_x86.h"
+#else
+# include "c11.h"
+#endif
+
+/* Reference counting. */
+#define __LFREF_IMPL(w, dtype_t)											\
+static const size_t __lfref_shift##w = sizeof(dtype_t) * 4;					\
+static const size_t __lfrptr_shift##w = 0;									\
+static const dtype_t __lfref_mask##w =										\
+				~(dtype_t) 0U << (sizeof(dtype_t) * 4);						\
+static const dtype_t __lfref_step##w =										\
+				(dtype_t) 1U << (sizeof(dtype_t) * 4);
+
+/* Pointer index for double-width types. */
+#ifdef __LITTLE_ENDIAN__
+# define __LFREF_LINK	0
+#else
+# define __LFREF_LINK	1
+#endif
+
+/* ABA tagging with split (word-atomic) load/cmpxchg operation. */
+#if __LFLOAD_SPLIT(LFATOMIC_BIG_WIDTH) == 1 ||								\
+		__LFCMPXCHG_SPLIT(LFATOMIC_BIG_WIDTH) == 1
+# define __LFABA_IMPL(w, type_t)											\
+static const size_t __lfaba_shift##w = sizeof(lfatomic_big_t) * 4;			\
+static const size_t __lfaptr_shift##w = 0;									\
+static const lfatomic_big_t __lfaba_mask##w =								\
+				~(lfatomic_big_t) 0U << (sizeof(lfatomic_big_t) * 4);		\
+static const lfatomic_big_t __lfaba_step##w =								\
+				(lfatomic_big_t) 1U << (sizeof(lfatomic_big_t) * 4);
+#endif
+
+/* ABA tagging when load/cmpxchg is not split. Note that unlike previous
+   case, __lfaptr_shift is required to be 0. */
+#if __LFLOAD_SPLIT(LFATOMIC_BIG_WIDTH) == 0 &&								\
+		__LFCMPXCHG_SPLIT(LFATOMIC_BIG_WIDTH) == 0
+# define __LFABA_IMPL(w, type_t)											\
+static const size_t __lfaba_shift##w = sizeof(type_t) * 8;					\
+static const size_t __lfaptr_shift##w = 0;									\
+static const lfatomic_big_t __lfaba_mask##w =								\
+				~(lfatomic_big_t) 0U << (sizeof(type_t) * 8);				\
+static const lfatomic_big_t __lfaba_step##w =								\
+				(lfatomic_big_t) 1U << (sizeof(type_t) * 8);
+#endif
+
+typedef bool (*lf_check_t) (void * data, void * addr, size_t size);
+
+static inline size_t lf_pow2(size_t order)
+{
+	return (size_t) 1U << order;
+}
+
+static inline bool LF_DONTCHECK(void * head, void * addr, size_t size)
+{
+	return true;
+}
+
+#define lf_container_of(addr, type, field)									\
+	(type *) ((char *) (addr) - offsetof(type, field))
+
+#ifdef __cplusplus
+# define LF_ERROR	UINTPTR_MAX
+#else
+# define LF_ERROR	((void *) UINTPTR_MAX)
+#endif
+
+/* Available on all 64-bit and CAS2 32-bit architectures. */
+#if LFATOMIC_BIG_WIDTH >= 64
+__LFABA_IMPL(32, uint32_t)
+__LFREF_IMPL(32, uint64_t)
+__LFREF_ATOMICS_IMPL(32, uint32_t, uint64_t)
+#endif
+
+/* Available on CAS2 64-bit architectures. */
+#if LFATOMIC_BIG_WIDTH >= 128
+__LFABA_IMPL(64, uint64_t)
+__LFREF_IMPL(64, __uint128_t)
+__LFREF_ATOMICS_IMPL(64, uint64_t, __uint128_t)
+#endif
+
+/* Available on CAS2 32/64-bit architectures. */
+#if LFATOMIC_BIG_WIDTH >= 2 * __LFPTR_WIDTH
+__LFABA_IMPL(, uintptr_t)
+__LFREF_IMPL(, lfref_t)
+__LFREF_ATOMICS_IMPL(, uintptr_t, lfref_t)
+#endif
+
+#endif	/* !__BITS_LF_H */
+
+/* vi: set tabstop=4: */
diff -urN linux-4.18.20/kernel/smr/bits/lfsmr_cas2.h linux-4.18.20-new/kernel/smr/bits/lfsmr_cas2.h
--- linux-4.18.20/kernel/smr/bits/lfsmr_cas2.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/kernel/smr/bits/lfsmr_cas2.h	2019-03-12 13:47:47.429439665 -0400
@@ -0,0 +1,172 @@
+/*
+ Copyright (c) 2017-2018, Ruslan Nikolaev
+ All rights reserved.
+
+ Redistribution and use in source and binary forms, with or without
+ modification, are permitted provided that the following conditions are met:
+
+ 1. Redistributions of source code must retain the above copyright notice, this
+    list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright notice,
+    this list of conditions and the following disclaimer in the documentation
+    and/or other materials provided with the distribution.
+
+ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+*/
+
+#ifndef __LFSMR_H
+# error "Do not include bits/lfsmr_cas2.h, use lfsmr.h instead."
+#endif
+
+#include "lfsmr_common.h"
+
+/* Generic implementation that uses CAS2 available on
+   i586+, x86-64 and ARMv6+. It is also adopted for
+   single-width LL/SC on PowerPC and MIPS. */
+
+/********************************************
+ *           head reference format
+ * +---------------------------------------+
+ * |  reference count  |      pointer      |
+ * +---------------------------------------+
+ *       32/64 bits          32/64 bits
+ *
+ ********************************************/
+
+#define __LFSMR_IMPL2(w, type_t, dtype_t)									\
+																			\
+struct lfsmr##w##_vector {													\
+	_Alignas(LF_CACHE_BYTES) LFATOMIC(dtype_t) head;						\
+	_Alignas(LF_CACHE_BYTES) char _pad[0];									\
+};																			\
+																			\
+struct lfsmr##w {															\
+	struct lfsmr##w##_vector vector[0];										\
+};																			\
+																			\
+__LFSMR_COMMON_IMPL(w, type_t)												\
+																			\
+static inline void lfsmr##w##_init(struct lfsmr##w * hdr, size_t order)		\
+{																			\
+	size_t count = (size_t) 1U << order, i = 0;								\
+	do {																	\
+		__lfref_init##w(&hdr->vector[i].head, 0);							\
+	} while (++i != count);													\
+}																			\
+																			\
+static inline type_t __lfsmr##w##_link(struct lfsmr##w * hdr, size_t vec)	\
+{																			\
+	return __lfref_link##w(&hdr->vector[vec].head, memory_order_acquire);	\
+}																			\
+																			\
+static inline bool lfsmr##w##_enter(struct lfsmr##w * hdr, size_t vec,		\
+		lfsmr##w##_handle_t * smr, const void * base, lf_check_t check)		\
+{																			\
+	dtype_t head = __lfref_fetch_add##w(&hdr->vector[vec].head,				\
+						__lfref_step##w, memory_order_acq_rel);				\
+	*smr = (lfsmr##w##_handle_t)											\
+		((head & ~__lfref_mask##w) >> __lfrptr_shift##w);					\
+	return true;															\
+}																			\
+																			\
+static inline bool __lfsmr##w##_leave(struct lfsmr##w * hdr, size_t vec,	\
+		size_t order, lfsmr##w##_handle_t smr, type_t * list,				\
+		const void * base, lf_check_t check)								\
+{																			\
+	const type_t addend = ((~(type_t) 0U) >> order) + 1U;					\
+	struct lfsmr##w##_node * node;											\
+	dtype_t head, last;														\
+	type_t next = next; /* Silence 'uninitialized' warnings. */				\
+	uintptr_t start, curr;													\
+																			\
+	last = __lfref_load##w(&hdr->vector[vec].head, memory_order_acquire);	\
+	do {																	\
+		curr = (uintptr_t) ((last & ~__lfref_mask##w) >>					\
+						__lfrptr_shift##w);									\
+		if (curr != smr) {													\
+			node = lfsmr##w##_addr(curr, base);								\
+			if (!check(hdr, node, sizeof(*node)))							\
+				return false;												\
+			next = node->next[vec];											\
+		}																	\
+		head = (last - __lfref_step##w) & __lfref_mask##w;					\
+		if (!__LFREF_CMPXCHG_FULL(dtype_t) || head)							\
+			head |= last & ~__lfref_mask##w;								\
+	} while (!__lfref_cmpxchgref_weak##w(&hdr->vector[vec].head, &last,		\
+				head, memory_order_acq_rel, memory_order_acquire));			\
+	start = (uintptr_t) ((last & ~__lfref_mask##w) >> __lfrptr_shift##w);	\
+	if (!__LFREF_CMPXCHG_FULL(dtype_t) && start &&							\
+					!((head & __lfref_mask##w) >> __lfref_shift##w)) {		\
+		/* For an incomplete cmpxchg, perform an extra step. */				\
+		if (__lfref_cmpxchgptr_strong##w(&hdr->vector[vec].head, &head, 0,	\
+				memory_order_acq_rel, memory_order_acquire)) {				\
+			if (!__lfsmr##w##_adjust_refs(hdr, list, start, addend, base))	\
+				return false;												\
+		}																	\
+	} else if (!head && start != 0) {										\
+		if (!__lfsmr##w##_adjust_refs(hdr, list, start, addend, base))		\
+			return false;													\
+	}																		\
+	if (curr != smr) {														\
+		size_t threshold = 0;												\
+		return __lfsmr##w##_traverse(hdr, vec, order, NULL, list, base,		\
+					check, &threshold, next, (uintptr_t) smr);				\
+	}																		\
+	return true;															\
+}																			\
+																			\
+static inline bool __lfsmr##w##_retire(struct lfsmr##w * hdr,				\
+		size_t order, type_t first, lfsmr##w##_free_t smr_free,				\
+		const void * base)													\
+{																			\
+	const type_t addend = ((~(type_t) 0U) >> order) + 1U;					\
+	size_t count = (size_t) 1U << order, i = 0;								\
+	struct lfsmr##w##_node * node;											\
+	dtype_t head, last;														\
+	type_t curr = first, prev, refs, adjs = 0, list = 0;					\
+	bool do_adjs = false;													\
+																			\
+	/* Add to the retirement lists. */										\
+	node = lfsmr##w##_addr(curr, base);										\
+	do {																	\
+		last = __lfref_load##w(&hdr->vector[i].head, memory_order_acquire);	\
+		do {																\
+			refs = (type_t) ((last & __lfref_mask##w) >> __lfref_shift##w);	\
+			if (!refs) {													\
+				do_adjs = true;												\
+				adjs += addend;												\
+				goto next;													\
+			}																\
+			prev = (type_t) ((last & ~__lfref_mask##w) >>					\
+							__lfrptr_shift##w);								\
+			node->next[i] = prev;											\
+			head = (last & __lfref_mask##w) |								\
+				(((dtype_t) curr << __lfrptr_shift##w) & ~__lfref_mask##w);	\
+		} while (!__lfref_cmpxchgptr_weak##w(&hdr->vector[i].head, &last,	\
+					head, memory_order_acq_rel, memory_order_acquire));		\
+		/* Adjust the reference counter. */									\
+		if (prev && !__lfsmr##w##_adjust_refs(hdr, &list, prev,				\
+				addend + refs, base))										\
+			return false;													\
+		next: ;																\
+	} while (++i != count);													\
+																			\
+	if (do_adjs) {															\
+		if (!__lfsmr##w##_adjust_refs(hdr, &list, first, adjs, base))		\
+			return false;													\
+	}																		\
+																			\
+	__lfsmr##w##_free(hdr, list, smr_free, base);							\
+	return true;															\
+}
+
+/* vi: set tabstop=4: */
diff -urN linux-4.18.20/kernel/smr/bits/lfsmr_common.h linux-4.18.20-new/kernel/smr/bits/lfsmr_common.h
--- linux-4.18.20/kernel/smr/bits/lfsmr_common.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/kernel/smr/bits/lfsmr_common.h	2019-03-12 13:48:01.277581120 -0400
@@ -0,0 +1,167 @@
+/*
+ Copyright (c) 2017-2018, Ruslan Nikolaev
+ All rights reserved.
+
+ Redistribution and use in source and binary forms, with or without
+ modification, are permitted provided that the following conditions are met:
+
+ 1. Redistributions of source code must retain the above copyright notice, this
+    list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright notice,
+    this list of conditions and the following disclaimer in the documentation
+    and/or other materials provided with the distribution.
+
+ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+*/
+
+#if !defined(__LFSMR_H) && !defined(__LFSMRO_H)
+# error "Do not include bits/lfsmr_common.h, use lfsmr.h instead."
+#endif
+
+#include "lf.h"
+
+#define LFSMR_NUM_CPUS 64
+
+#define __LFSMR_COMMON_IMPL(w, type_t)										\
+typedef uintptr_t lfsmr##w##_handle_t;										\
+struct lfsmr##w;															\
+																			\
+struct lfsmr##w##_node {													\
+	LFATOMIC(type_t) refs;													\
+	type_t next[LFSMR_NUM_CPUS];												\
+};																			\
+																			\
+typedef void (*lfsmr##w##_free_t) (struct lfsmr##w *,						\
+		struct lfsmr##w##_node *);											\
+																			\
+static inline type_t __lfsmr##w##_link(struct lfsmr##w * hdr, size_t vec);	\
+static inline bool __lfsmr##w##_retire(struct lfsmr##w * hdr, size_t order,	\
+		type_t first, lfsmr##w##_free_t smr_free, const void * base);		\
+static inline bool lfsmr##w##_enter(struct lfsmr##w * hdr, size_t vec,		\
+		lfsmr##w##_handle_t * smr, const void * base, lf_check_t check);	\
+static inline bool __lfsmr##w##_leave(struct lfsmr##w * hdr, size_t vec,	\
+		size_t order, lfsmr##w##_handle_t smr, type_t * list,				\
+		const void * base, lf_check_t check);								\
+																			\
+static inline struct lfsmr##w##_node * lfsmr##w##_addr(						\
+	uintptr_t offset, const void * base)									\
+{																			\
+	return (struct lfsmr##w##_node *) ((uintptr_t) base + offset);			\
+}																			\
+																			\
+static inline bool __lfsmr##w##_adjust_refs(struct lfsmr##w * hdr,			\
+		type_t * list, type_t prev, type_t refs, const void * base)			\
+{																			\
+	struct lfsmr##w##_node * node;											\
+	node = lfsmr##w##_addr(prev, base);										\
+	if (atomic_fetch_add_explicit(&node->refs, refs,						\
+							memory_order_acq_rel) == -refs) {				\
+		node->next[0] = *list;												\
+		*list = prev;														\
+	}																		\
+	return true;															\
+}																			\
+																			\
+static inline bool __lfsmr##w##_traverse(struct lfsmr##w * hdr, size_t vec,	\
+	size_t order, lfsmr##w##_handle_t * smr, type_t * list,					\
+	const void * base, lf_check_t check, size_t * threshold,				\
+	uintptr_t next, uintptr_t end)											\
+{																			\
+	struct lfsmr##w##_node * node;											\
+	size_t length = 0;														\
+	uintptr_t curr;															\
+																			\
+	do {																	\
+		curr = next;														\
+		if (!curr)															\
+			break;															\
+		node = lfsmr##w##_addr(curr, base);									\
+		if (!check(hdr, node, sizeof(*node)))								\
+			return false;													\
+		next = node->next[vec];												\
+		/* If the last reference, put into the local list. */				\
+		if (atomic_fetch_sub_explicit(&node->refs, 1, memory_order_acq_rel)	\
+						== 1) {												\
+			node->next[0] = *list;											\
+			*list = curr;													\
+			if (*threshold && ++length >= *threshold) {						\
+				if (!__lfsmr##w##_leave(hdr, vec, order, *smr, list, base,	\
+										check))								\
+					return false;											\
+				*threshold = 0;												\
+			}																\
+		}																	\
+	} while (curr != end);													\
+																			\
+	return true;															\
+}																			\
+																			\
+static inline void __lfsmr##w##_free(struct lfsmr##w * hdr, type_t list,	\
+	lfsmr##w##_free_t smr_free, const void * base)							\
+{																			\
+	struct lfsmr##w##_node * node;											\
+																			\
+	while (list != 0) {														\
+		node = lfsmr##w##_addr(list, base);									\
+		list = node->next[0];												\
+		smr_free(hdr, node);												\
+	}																		\
+}																			\
+																			\
+static inline bool lfsmr##w##_leave(struct lfsmr##w * hdr, size_t vec,		\
+	size_t order, lfsmr##w##_handle_t smr, lfsmr##w##_free_t smr_free,		\
+	const void * base, lf_check_t check)									\
+{																			\
+	type_t list = 0;														\
+	if (!__lfsmr##w##_leave(hdr, vec, order, smr, &list, base, check))		\
+		return false;														\
+	__lfsmr##w##_free(hdr, list, smr_free, base);							\
+	return true;															\
+}																			\
+																			\
+static inline bool lfsmr##w##_trim(struct lfsmr##w * hdr, size_t vec,		\
+	size_t order, lfsmr##w##_handle_t * smr, lfsmr##w##_free_t smr_free,	\
+	const void * base, lf_check_t check, size_t threshold)					\
+{																			\
+	struct lfsmr##w##_node * node;											\
+	lfsmr##w##_handle_t end = *smr;											\
+	size_t new_threshold = threshold;										\
+	type_t link = __lfsmr##w##_link(hdr, vec);								\
+	type_t list = 0;														\
+																			\
+	if (link != end) {														\
+		*smr = link;														\
+		node = lfsmr##w##_addr(link, base);									\
+		if (!check(hdr, node, sizeof(*node)))								\
+			return false;													\
+		link = node->next[vec];													\
+		if (!__lfsmr##w##_traverse(hdr, vec, order, smr, &list, base,		\
+				check, &new_threshold, link, (uintptr_t) end))				\
+			return false;													\
+		__lfsmr##w##_free(hdr, list, smr_free, base);						\
+		/* Leave was called, i.e. new_threshold = 0. */						\
+		if (threshold != new_threshold)										\
+			return lfsmr##w##_enter(hdr, vec, smr, base, check);			\
+	}																		\
+	return true;															\
+}																			\
+																			\
+static inline bool lfsmr##w##_retire(struct lfsmr##w * hdr, size_t order,	\
+		struct lfsmr##w##_node * node, lfsmr##w##_free_t smr_free,			\
+		const void * base)													\
+{																			\
+	type_t first;															\
+	first = (type_t) ((uintptr_t) node) - (type_t) ((uintptr_t) base);		\
+	return __lfsmr##w##_retire(hdr, order, first, smr_free, base);			\
+}
+
+/* vi: set tabstop=4: */
diff -urN linux-4.18.20/kernel/smr/bits/llvm_x86.h linux-4.18.20-new/kernel/smr/bits/llvm_x86.h
--- linux-4.18.20/kernel/smr/bits/llvm_x86.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/kernel/smr/bits/llvm_x86.h	2019-03-12 13:48:29.945873789 -0400
@@ -0,0 +1,167 @@
+/*
+ Copyright (c) 2018, Ruslan Nikolaev
+ All rights reserved.
+
+ Redistribution and use in source and binary forms, with or without
+ modification, are permitted provided that the following conditions are met:
+
+ 1. Redistributions of source code must retain the above copyright notice, this
+    list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright notice,
+    this list of conditions and the following disclaimer in the documentation
+    and/or other materials provided with the distribution.
+
+ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+*/
+
+#ifndef __LF_LLVM_X86_H
+#define __LF_LLVM_X86_H 1
+
+#include <stdatomic.h>
+
+#define LFATOMIC(x)				_Atomic(x)
+#define LFATOMIC_VAR_INIT(x)	ATOMIC_VAR_INIT(x)
+
+static inline void __lfaba_init(_Atomic(lfatomic_big_t) * obj,
+		lfatomic_big_t val)
+{
+	atomic_init(obj, val);
+}
+
+static inline lfatomic_big_t __lfaba_load(_Atomic(lfatomic_big_t) * obj,
+		memory_order order)
+{
+	return *((volatile lfatomic_big_t *) obj);
+}
+
+static inline bool __lfaba_cmpxchg_weak(_Atomic(lfatomic_big_t) * obj,
+	lfatomic_big_t * expected, lfatomic_big_t desired,
+	memory_order succ, memory_order fail)
+{
+	return atomic_compare_exchange_weak_explicit(obj, expected, desired,
+					succ, fail);
+}
+
+static inline bool __lfaba_cmpxchg_strong(_Atomic(lfatomic_big_t) * obj,
+	lfatomic_big_t * expected, lfatomic_big_t desired,
+	memory_order succ, memory_order fail)
+{
+	return atomic_compare_exchange_strong_explicit(obj, expected, desired,
+					succ, fail);
+}
+
+static inline void __lfepoch_init(_Atomic(lfepoch_t) * obj, lfepoch_t val)
+{
+	atomic_init(obj, val);
+}
+
+static inline lfepoch_t __lfepoch_load(_Atomic(lfepoch_t) * obj,
+		memory_order order)
+{
+#ifdef __i386__
+	/* Avoid using cmpxchg8b for atomic loads. */
+	lfepoch_t result;
+	__asm__ __volatile__ ("fildq	%1\n\t"
+						  "fistpq	%0"
+						  : "=m" (result)
+						  : "m" (*obj)
+						  : "st");
+	return result;
+#else
+	return atomic_load_explicit(obj, order);
+#endif
+}
+
+static inline bool __lfepoch_cmpxchg_weak(_Atomic(lfepoch_t) * obj,
+		lfepoch_t * expected, lfepoch_t desired,
+		memory_order succ, memory_order fail)
+{
+	return atomic_compare_exchange_weak_explicit(obj, expected, desired,
+				succ, fail);
+}
+
+static inline lfepoch_t __lfepoch_fetch_add(_Atomic(lfepoch_t) * obj,
+		lfepoch_t arg, memory_order order)
+{
+	return atomic_fetch_add_explicit(obj, arg, order);
+}
+
+#define __LFREF_CMPXCHG_FULL(dtype_t)	(1)
+
+#define __LFREF_ATOMICS_IMPL(w, type_t, dtype_t)							\
+static inline void __lfref_init##w(_Atomic(dtype_t) * obj, dtype_t val)		\
+{																			\
+	atomic_init(obj, val);													\
+}																			\
+																			\
+static inline dtype_t __lfref_load##w(_Atomic(dtype_t) * obj,				\
+		memory_order order)													\
+{																			\
+	if (sizeof(dtype_t) != sizeof(lfatomic_big_t)) {						\
+		return atomic_load_explicit(obj, order);							\
+	} else {																\
+		return *((volatile lfatomic_big_t *) obj);							\
+	}																		\
+}																			\
+																			\
+static inline type_t __lfref_link##w(_Atomic(dtype_t) * obj,				\
+		memory_order order)													\
+{																			\
+	if (sizeof(dtype_t) != sizeof(lfatomic_big_t)) {						\
+		return (atomic_load_explicit(obj, order) & ~__lfref_mask##w) >>		\
+					__lfrptr_shift##w;										\
+	} else {																\
+		return *((volatile type_t *) obj + __LFREF_LINK);					\
+	}																		\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgptr_weak##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	return atomic_compare_exchange_weak_explicit(obj, expected, desired,	\
+			succ, fail);													\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgptr_strong##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	return atomic_compare_exchange_strong_explicit(obj, expected, desired,	\
+			succ, fail);													\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgref_weak##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	return atomic_compare_exchange_weak_explicit(obj, expected, desired,	\
+			succ, fail);													\
+}																			\
+																			\
+static inline bool __lfref_cmpxchgref_strong##w(_Atomic(dtype_t) * obj,		\
+		dtype_t * expected, dtype_t desired,								\
+		memory_order succ, memory_order fail)								\
+{																			\
+	return atomic_compare_exchange_strong_explicit(obj, expected, desired,	\
+			succ, fail);													\
+}																			\
+																			\
+static inline dtype_t __lfref_fetch_add##w(_Atomic(dtype_t) * obj,			\
+		dtype_t arg, memory_order order)									\
+{																			\
+	return atomic_fetch_add_explicit(obj, arg, order);						\
+}
+
+#endif /* !__LF_LLVM_X86_H */
+
+/* vi: set tabstop=4: */
diff -urN linux-4.18.20/kernel/smr/lfsmr.h linux-4.18.20-new/kernel/smr/lfsmr.h
--- linux-4.18.20/kernel/smr/lfsmr.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/kernel/smr/lfsmr.h	2019-03-12 13:45:39.100126001 -0400
@@ -0,0 +1,65 @@
+/*
+ Copyright (c) 2017, Ruslan Nikolaev
+ All rights reserved.
+
+ Redistribution and use in source and binary forms, with or without
+ modification, are permitted provided that the following conditions are met:
+
+ 1. Redistributions of source code must retain the above copyright notice, this
+    list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright notice,
+    this list of conditions and the following disclaimer in the documentation
+    and/or other materials provided with the distribution.
+
+ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+*/
+
+#ifndef __LFSMR_H
+#define __LFSMR_H	1
+
+#include "bits/lfsmr_cas2.h"
+
+/* Available on all architectures. */
+#define LFSMR_ALIGN	(_Alignof(struct lfsmr))
+#define LFSMR_SIZE(x)		\
+	((x) * sizeof(struct lfsmr_vector) + offsetof(struct lfsmr, vector))
+#if LFATOMIC_BIG_WIDTH >= 2 * __LFPTR_WIDTH &&	\
+		!__LFCMPXCHG_SPLIT(2 * __LFPTR_WIDTH)
+__LFSMR_IMPL2(, uintptr_t, lfref_t)
+#else
+__LFSMR_IMPL1(, uintptr_t)
+#endif
+
+#define LFSMR32_ALIGN	(_Alignof(struct lfsmr32))
+#define LFSMR32_SIZE(x)		\
+	((x) * sizeof(struct lfsmr32_vector) + offsetof(struct lfsmr32, vector))
+#if LFATOMIC_BIG_WIDTH >= 64 && !__LFCMPXCHG_SPLIT(64)
+__LFSMR_IMPL2(32, uint32_t, uint64_t)
+#else
+__LFSMR_IMPL1(32, uint32_t)
+#endif
+
+/* Available on 64-bit architectures. */
+#if LFATOMIC_WIDTH >= 64
+# define LFSMR64_ALIGN	(_Alignof(struct lfsmr64))
+# define LFSMR64_SIZE(x)		\
+	((x) * sizeof(struct lfsmr64_vector) + offsetof(struct lfsmr64, vector))
+# if LFATOMIC_BIG_WIDTH >= 128 && !__LFCMPXCHG_SPLIT(128)
+__LFSMR_IMPL2(64, uint64_t, __uint128_t)
+# else
+__LFSMR_IMPL1(64, uint64_t)
+# endif
+#endif
+
+#endif	/* !__LFSMR_H */
+
+/* vi: set tabstop=4: */
diff -urN linux-4.18.20/kernel/smr.c linux-4.18.20-new/kernel/smr.c
--- linux-4.18.20/kernel/smr.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/kernel/smr.c	2019-03-12 13:45:17.815907573 -0400
@@ -0,0 +1,123 @@
+/*
+ Copyright (c) 2019, Ruslan Nikolaev
+ All rights reserved.
+
+ Redistribution and use in source and binary forms, with or without
+ modification, are permitted provided that the following conditions are met:
+
+ 1. Redistributions of source code must retain the above copyright notice, this
+    list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright notice,
+    this list of conditions and the following disclaimer in the documentation
+    and/or other materials provided with the distribution.
+
+ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+*/
+
+#include <linux/smp.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/module.h>
+
+#include <smr/smr.h>
+#include "smr/lfsmr.h"
+
+static struct workqueue_struct *smr_wq = NULL;
+
+static union {
+	_Alignas(LFSMR_ALIGN) char data[LFSMR_SIZE(SMR_NUM)];
+	struct lfsmr header;
+} smr;
+
+struct SMR_Manager {
+	struct work_struct my_work;
+    smr_header header;
+    void *address;
+};
+
+static struct SMR_Manager * make_manager(void *address)
+{
+	struct SMR_Manager *manager = kzalloc(sizeof(*manager), GFP_ATOMIC);
+	if(!manager)
+		return NULL;
+
+	manager->address = address;
+	return manager;
+}
+
+static void free_manager(struct SMR_Manager *manager)
+{
+	kfree(manager);
+}
+
+static void unmap_work_handler(struct work_struct *work)
+{
+	struct SMR_Manager *manager = (struct SMR_Manager *)work;
+
+//	printk("Memory Freed %lx\n", (unsigned long) manager->address);
+
+//	unmap_module(manager->address);
+	vfree(manager->address);
+	free_manager(manager);
+}
+
+void smr_init(void)
+{
+	lfsmr_init(&smr.header, SMR_ORDER);
+}
+
+static inline void smr_do_free(struct lfsmr * h, struct lfsmr_node * node)
+{
+	struct SMR_Manager *manager;
+	smr_header *header = (smr_header *) node;
+	manager = container_of(header, struct SMR_Manager, header);
+
+	INIT_WORK( (struct work_struct *)manager, unmap_work_handler );
+	queue_work( smr_wq, (struct work_struct *)manager);
+}
+
+smr_handle smr_enter(void)
+{
+	size_t vec = raw_smp_processor_id() % SMR_NUM;
+	smr_handle ret;
+	ret.vector = vec;
+	lfsmr_enter(&smr.header, vec, &ret.handle, 0, LF_DONTCHECK);
+	return ret;
+}
+
+void smr_leave(smr_handle handle)
+{
+	lfsmr_leave(&smr.header, handle.vector, SMR_ORDER, handle.handle,
+		smr_do_free, 0, LF_DONTCHECK);
+}
+
+int smr_retire(void *address)
+{
+	struct SMR_Manager *manager = make_manager(address);
+	if(!manager)
+		return -ENOMEM;
+
+	/* TODO: Can't do this smr_init. Investigate why */
+	if (!smr_wq)
+		smr_wq = create_workqueue("smr_wq");
+
+	lfsmr_retire(&smr.header, SMR_ORDER, (struct lfsmr_node *)(&manager->header),
+		smr_do_free, 0);
+
+	return 0;
+}
+
+
+EXPORT_SYMBOL(smr_init);
+EXPORT_SYMBOL(smr_enter);
+EXPORT_SYMBOL(smr_leave);
+EXPORT_SYMBOL(smr_retire);
diff -urN linux-4.18.20/mm/vmalloc.c linux-4.18.20-new/mm/vmalloc.c
--- linux-4.18.20/mm/vmalloc.c	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/mm/vmalloc.c	2019-03-12 13:37:16.962908667 -0400
@@ -2752,3 +2752,68 @@
 
 #endif
 
+
+// __vmalloc_area_node
+static void *remap_area(struct vm_struct *area, struct vm_struct *old_area, gfp_t gfp_mask,
+				 pgprot_t prot, int node)
+{
+	area->nr_pages = old_area->nr_pages;
+	area->pages = old_area->pages;
+
+	if (map_vm_area(area, prot, old_area->pages))
+		goto fail;
+	return area->addr;
+
+fail:
+	printk("ERR: Fail in remap_hxn_2\n");
+	return NULL;
+}
+
+// __vmalloc_node_range
+// TODO explore vmap function for better implementation
+void *remap_module(unsigned long addr, unsigned long size, unsigned long align,
+		unsigned long start, unsigned long end, gfp_t gfp_mask,
+		pgprot_t prot, unsigned long vm_flags, int node,
+		const void *caller)
+{
+	struct vmap_area *va;
+	struct vm_struct *area;
+	struct vm_struct *old_area;
+	void *addr_new;
+
+	va = find_vmap_area(addr);
+	if(va == NULL){
+		printk("vmap_area is null");
+		goto fail;
+	}
+	old_area = va->vm;
+
+	size = PAGE_ALIGN(size);
+	if (!size || (size >> PAGE_SHIFT) > totalram_pages)
+		goto fail;
+
+	area = __get_vm_area_node(size, align, VM_ALLOC | VM_UNINITIALIZED |
+				vm_flags, start, end, node, gfp_mask, caller);
+	if (!area)
+		goto fail;
+
+	addr_new = remap_area(area, old_area, gfp_mask, prot, node);
+	if (!addr_new)
+		goto fail;
+
+	clear_vm_uninitialized_flag(area);
+
+	return addr_new;
+
+	fail:
+		printk("ERR: Fail in remap_hxn\n");
+		return NULL;
+}
+EXPORT_SYMBOL(remap_module);
+
+// vfree()
+void unmap_module(const void *addr)
+{
+	vunmap(addr);
+}
+EXPORT_SYMBOL(unmap_module);
diff -urN linux-4.18.20/scripts/mod/modpost.c linux-4.18.20-new/scripts/mod/modpost.c
--- linux-4.18.20/scripts/mod/modpost.c	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/scripts/mod/modpost.c	2019-03-12 13:37:16.962908667 -0400
@@ -699,6 +699,8 @@
 			mod->has_init = 1;
 		if (strcmp(symname, "cleanup_module") == 0)
 			mod->has_cleanup = 1;
+		if (strcmp(symname, "randomize_module") == 0)
+			mod->has_randomize = 1;
 		break;
 	}
 }
@@ -2141,6 +2143,10 @@
 		buf_printf(b, "#ifdef CONFIG_MODULE_UNLOAD\n"
 			      "\t.exit = cleanup_module,\n"
 			      "#endif\n");
+	if (mod->has_randomize)
+		buf_printf(b, "#ifdef CONFIG_X86_MODULE_RERANDOMIZE\n"
+			      "\t.rerandomize = randomize_module,\n"
+			      "#endif\n");
 	buf_printf(b, "\t.arch = MODULE_ARCH_INIT,\n");
 	buf_printf(b, "};\n");
 }
@@ -2175,6 +2181,8 @@
 
 	for (s = mod->unres; s; s = s->next) {
 		exp = find_symbol(s->name);
+		if (strstarts(s->name, "__FIXED"))
+			continue;
 		if (!exp || exp->module == mod) {
 			if (have_vmlinux && !s->weak) {
 				if (warn_unresolved) {
diff -urN linux-4.18.20/scripts/mod/modpost.h linux-4.18.20-new/scripts/mod/modpost.h
--- linux-4.18.20/scripts/mod/modpost.h	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/scripts/mod/modpost.h	2019-03-12 13:37:16.962908667 -0400
@@ -118,6 +118,7 @@
 	int skip;
 	int has_init;
 	int has_cleanup;
+	int has_randomize;
 	struct buffer dev_table_buf;
 	char	     srcversion[25];
 	int is_dot_o;
diff -urN linux-4.18.20/scripts/recordmcount.h linux-4.18.20-new/scripts/recordmcount.h
--- linux-4.18.20/scripts/recordmcount.h	2018-11-21 03:22:14.000000000 -0500
+++ linux-4.18.20-new/scripts/recordmcount.h	2019-03-12 13:37:16.962908667 -0400
@@ -18,6 +18,15 @@
  *
  * Licensed under the GNU General Public License, version 2 (GPLv2).
  */
+
+#ifndef R_X86_64_REX_GOTPCRELX
+	#define R_X86_64_REX_GOTPCRELX	42
+#endif
+
+#ifndef R_X86_64_GOTPCRELX
+	#define R_X86_64_GOTPCRELX	41
+#endif
+
 #undef append_func
 #undef is_fake_mcount
 #undef fn_is_fake_mcount
