diff -urN linux-4.18.20/arch/x86/crypto/aes-x86_64-asm_64.S linux-4.18.20-new/arch/x86/crypto/aes-x86_64-asm_64.S
--- linux-4.18.20/arch/x86/crypto/aes-x86_64-asm_64.S	2018-12-10 01:04:12.992606671 -0500
+++ linux-4.18.20-new/arch/x86/crypto/aes-x86_64-asm_64.S	2018-12-23 13:02:22.469702762 -0500
@@ -17,6 +17,7 @@
 
 #include <linux/linkage.h>
 #include <asm/asm-offsets.h>
+#include <asm/asm.h>
 
 #define R1	%rax
 #define R1E	%eax
@@ -83,11 +84,11 @@
 	ENDPROC(FUNC);
 
 #define round_mov(tab_off, reg_i, reg_o) \
-	leaq	tab_off(%rip), RBASE; \
+	_ASM_LEA_RIP(tab_off, RBASE);	\
 	movl	(RBASE,reg_i,4), reg_o;
 
 #define round_xor(tab_off, reg_i, reg_o) \
-	leaq	tab_off(%rip), RBASE; \
+	_ASM_LEA_RIP(tab_off, RBASE);	\
 	xorl	(RBASE,reg_i,4), reg_o;
 
 #define round(TAB,OFFSET,r1,r2,r3,r4,r5,r6,r7,r8,ra,rb,rc,rd) \
diff -urN linux-4.18.20/arch/x86/crypto/cast5-avx-x86_64-asm_64.S linux-4.18.20-new/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
--- linux-4.18.20/arch/x86/crypto/cast5-avx-x86_64-asm_64.S	2018-12-10 01:04:12.992606671 -0500
+++ linux-4.18.20-new/arch/x86/crypto/cast5-avx-x86_64-asm_64.S	2018-12-23 13:02:22.469702762 -0500
@@ -25,6 +25,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/asm.h>
 
 .file "cast5-avx-x86_64-asm_64.S"
 
@@ -99,17 +100,17 @@
 
 #define lookup_32bit(src, dst, op1, op2, op3, interleave_op, il_reg) \
 	movzbl		src ## bh,       RID1d;    \
-	leaq		s1(%rip),        RID2;     \
+	_ASM_LEA_RIP(s1, RID2);                    \
 	movl		(RID2, RID1, 4), dst ## d; \
 	movzbl		src ## bl,       RID2d;    \
-	leaq		s2(%rip),        RID1;     \
+	_ASM_LEA_RIP(s2, RID1);                    \
 	op1		(RID1, RID2, 4), dst ## d; \
 	shrq $16,	src;                       \
 	movzbl		src ## bh,     RID1d;      \
-	leaq		s3(%rip),        RID2;     \
+	_ASM_LEA_RIP(s3, RID2);                    \
 	op2		(RID2, RID1, 4), dst ## d; \
 	movzbl		src ## bl,     RID2d;      \
-	leaq		s4(%rip),        RID1;     \
+	_ASM_LEA_RIP(s4, RID1);                    \
 	op3		(RID1, RID2, 4), dst ## d; \
 	interleave_op(il_reg);
 
diff -urN linux-4.18.20/arch/x86/crypto/cast6-avx-x86_64-asm_64.S linux-4.18.20-new/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
--- linux-4.18.20/arch/x86/crypto/cast6-avx-x86_64-asm_64.S	2018-12-10 01:04:12.992606671 -0500
+++ linux-4.18.20-new/arch/x86/crypto/cast6-avx-x86_64-asm_64.S	2018-12-23 13:02:22.469702762 -0500
@@ -25,6 +25,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/asm.h>
 #include "glue_helper-asm-avx.S"
 
 .file "cast6-avx-x86_64-asm_64.S"
@@ -99,17 +100,17 @@
 
 #define lookup_32bit(src, dst, op1, op2, op3, interleave_op, il_reg) \
 	movzbl		src ## bh,       RID1d;    \
-	leaq		s1(%rip),        RID2;     \
+	_ASM_LEA_RIP(s1, RID2);                    \
 	movl		(RID2, RID1, 4), dst ## d; \
 	movzbl		src ## bl,       RID2d;    \
-	leaq		s2(%rip),        RID1;     \
+	_ASM_LEA_RIP(s2, RID1);                    \
 	op1		(RID1, RID2, 4), dst ## d; \
 	shrq $16,	src;                       \
 	movzbl		src ## bh,     RID1d;      \
-	leaq		s3(%rip),        RID2;     \
+	_ASM_LEA_RIP(s3, RID2);                    \
 	op2		(RID2, RID1, 4), dst ## d; \
 	movzbl		src ## bl,     RID2d;      \
-	leaq		s4(%rip),        RID1;     \
+	_ASM_LEA_RIP(s4, RID1);                    \
 	op3		(RID1, RID2, 4), dst ## d; \
 	interleave_op(il_reg);
 
diff -urN linux-4.18.20/arch/x86/crypto/sha1-mb/sha1_mb_mgr_flush_avx2.S linux-4.18.20-new/arch/x86/crypto/sha1-mb/sha1_mb_mgr_flush_avx2.S
--- linux-4.18.20/arch/x86/crypto/sha1-mb/sha1_mb_mgr_flush_avx2.S	2018-12-10 01:04:12.992606671 -0500
+++ linux-4.18.20-new/arch/x86/crypto/sha1-mb/sha1_mb_mgr_flush_avx2.S	2018-12-23 13:02:22.469702762 -0500
@@ -53,6 +53,7 @@
  */
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/asm.h>
 #include "sha1_mb_mgr_datastruct.S"
 
 
@@ -183,7 +184,7 @@
 
 	# "state" and "args" are the same address, arg1
 	# len is arg2
-	call	sha1_x8_avx2
+	_ASM_CALL(sha1_x8_avx2)
 	# state and idx are intact
 
 
diff -urN linux-4.18.20/arch/x86/crypto/sha1-mb/sha1_mb_mgr_submit_avx2.S linux-4.18.20-new/arch/x86/crypto/sha1-mb/sha1_mb_mgr_submit_avx2.S
--- linux-4.18.20/arch/x86/crypto/sha1-mb/sha1_mb_mgr_submit_avx2.S	2018-12-10 01:04:12.992606671 -0500
+++ linux-4.18.20-new/arch/x86/crypto/sha1-mb/sha1_mb_mgr_submit_avx2.S	2018-12-23 13:02:22.473702887 -0500
@@ -54,6 +54,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/asm.h>
 #include "sha1_mb_mgr_datastruct.S"
 
 
@@ -163,7 +164,7 @@
 
 	# "state" and "args" are the same address, arg1
 	# len is arg2
-	call    sha1_x8_avx2
+	_ASM_CALL(sha1_x8_avx2)
 
 	# state and idx are intact
 
diff -urN linux-4.18.20/arch/x86/crypto/sha256-mb/sha256_mb_mgr_flush_avx2.S linux-4.18.20-new/arch/x86/crypto/sha256-mb/sha256_mb_mgr_flush_avx2.S
--- linux-4.18.20/arch/x86/crypto/sha256-mb/sha256_mb_mgr_flush_avx2.S	2018-12-10 01:04:12.992606671 -0500
+++ linux-4.18.20-new/arch/x86/crypto/sha256-mb/sha256_mb_mgr_flush_avx2.S	2018-12-23 13:02:22.473702887 -0500
@@ -52,6 +52,7 @@
  */
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/asm.h>
 #include "sha256_mb_mgr_datastruct.S"
 
 .extern sha256_x8_avx2
@@ -181,7 +182,7 @@
 
 	# "state" and "args" are the same address, arg1
 	# len is arg2
-	call	sha256_x8_avx2
+	_ASM_CALL(sha256_x8_avx2)
 	# state and idx are intact
 
 len_is_0:
diff -urN linux-4.18.20/arch/x86/crypto/sha256-mb/sha256_mb_mgr_submit_avx2.S linux-4.18.20-new/arch/x86/crypto/sha256-mb/sha256_mb_mgr_submit_avx2.S
--- linux-4.18.20/arch/x86/crypto/sha256-mb/sha256_mb_mgr_submit_avx2.S	2018-12-10 01:04:12.992606671 -0500
+++ linux-4.18.20-new/arch/x86/crypto/sha256-mb/sha256_mb_mgr_submit_avx2.S	2018-12-23 13:02:22.473702887 -0500
@@ -53,6 +53,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/asm.h>
 #include "sha256_mb_mgr_datastruct.S"
 
 .extern sha256_x8_avx2
@@ -164,7 +165,7 @@
 
 	# "state" and "args" are the same address, arg1
 	# len is arg2
-	call	sha256_x8_avx2
+	_ASM_CALL(sha256_x8_avx2)
 
 	# state and idx are intact
 
diff -urN linux-4.18.20/arch/x86/crypto/sha512-mb/sha512_mb_mgr_flush_avx2.S linux-4.18.20-new/arch/x86/crypto/sha512-mb/sha512_mb_mgr_flush_avx2.S
--- linux-4.18.20/arch/x86/crypto/sha512-mb/sha512_mb_mgr_flush_avx2.S	2018-12-10 01:04:12.992606671 -0500
+++ linux-4.18.20-new/arch/x86/crypto/sha512-mb/sha512_mb_mgr_flush_avx2.S	2018-12-23 13:02:22.473702887 -0500
@@ -53,6 +53,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/asm.h>
 #include "sha512_mb_mgr_datastruct.S"
 
 .extern sha512_x4_avx2
@@ -177,7 +178,7 @@
 
         # "state" and "args" are the same address, arg1
         # len is arg2
-        call    sha512_x4_avx2
+	_ASM_CALL(sha512_x4_avx2)
         # state and idx are intact
 
 len_is_0:
diff -urN linux-4.18.20/arch/x86/crypto/sha512-mb/sha512_mb_mgr_submit_avx2.S linux-4.18.20-new/arch/x86/crypto/sha512-mb/sha512_mb_mgr_submit_avx2.S
--- linux-4.18.20/arch/x86/crypto/sha512-mb/sha512_mb_mgr_submit_avx2.S	2018-12-10 01:04:12.992606671 -0500
+++ linux-4.18.20-new/arch/x86/crypto/sha512-mb/sha512_mb_mgr_submit_avx2.S	2018-12-23 13:02:22.473702887 -0500
@@ -53,6 +53,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/asm.h>
 #include "sha512_mb_mgr_datastruct.S"
 
 .extern sha512_x4_avx2
@@ -167,7 +168,7 @@
 
 	# "state" and "args" are the same address, arg1
 	# len is arg2
-	call    sha512_x4_avx2
+	_ASM_CALL(sha512_x4_avx2)
 	# state and idx are intact
 
 len_is_0:
diff -urN linux-4.18.20/arch/x86/include/asm/alternative.h linux-4.18.20-new/arch/x86/include/asm/alternative.h
--- linux-4.18.20/arch/x86/include/asm/alternative.h	2018-12-10 01:04:12.996606714 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/alternative.h	2018-12-23 13:02:22.477703012 -0500
@@ -207,8 +207,8 @@
 
 /* Like alternative_io, but for replacing a direct call with another one. */
 #define alternative_call(oldfunc, newfunc, feature, output, input...)	\
-	asm volatile (ALTERNATIVE("call %P[old]", "call %P[new]", feature) \
-		: output : [old] "i" (oldfunc), [new] "i" (newfunc), ## input)
+	asm volatile (ALTERNATIVE(_ASM_CALL(%p[old]), _ASM_CALL(%p[new]), feature) \
+		: output : [old] "X" (oldfunc), [new] "X" (newfunc), ## input)
 
 /*
  * Like alternative_call, but there are two features and respective functions.
@@ -218,11 +218,11 @@
  */
 #define alternative_call_2(oldfunc, newfunc1, feature1, newfunc2, feature2,   \
 			   output, input...)				      \
-	asm volatile (ALTERNATIVE_2("call %P[old]", "call %P[new1]", feature1,\
-		"call %P[new2]", feature2)				      \
+	asm volatile (ALTERNATIVE_2(_ASM_CALL(%p[old]), _ASM_CALL(%p[new1]), feature1,\
+		_ASM_CALL(%p[new2]), feature2)				      \
 		: output, ASM_CALL_CONSTRAINT				      \
-		: [old] "i" (oldfunc), [new1] "i" (newfunc1),		      \
-		  [new2] "i" (newfunc2), ## input)
+		: [old] "X" (oldfunc), [new1] "X" (newfunc1),		      \
+		  [new2] "X" (newfunc2), ## input)
 
 /*
  * use this macro(s) if you need more than one output parameter
diff -urN linux-4.18.20/arch/x86/include/asm/arch_hweight.h linux-4.18.20-new/arch/x86/include/asm/arch_hweight.h
--- linux-4.18.20/arch/x86/include/asm/arch_hweight.h	2018-12-10 01:04:12.996606714 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/arch_hweight.h	2018-12-23 13:02:22.477703012 -0500
@@ -3,6 +3,7 @@
 #define _ASM_X86_HWEIGHT_H
 
 #include <asm/cpufeatures.h>
+#include <asm/asm.h>
 
 #ifdef CONFIG_64BIT
 /* popcnt %edi, %eax */
@@ -24,7 +25,7 @@
 {
 	unsigned int res;
 
-	asm (ALTERNATIVE("call __sw_hweight32", POPCNT32, X86_FEATURE_POPCNT)
+	asm (ALTERNATIVE(_ASM_CALL(__sw_hweight32), POPCNT32, X86_FEATURE_POPCNT)
 			 : "="REG_OUT (res)
 			 : REG_IN (w));
 
@@ -52,7 +53,7 @@
 {
 	unsigned long res;
 
-	asm (ALTERNATIVE("call __sw_hweight64", POPCNT64, X86_FEATURE_POPCNT)
+	asm (ALTERNATIVE(_ASM_CALL(__sw_hweight64), POPCNT64, X86_FEATURE_POPCNT)
 			 : "="REG_OUT (res)
 			 : REG_IN (w));
 
diff -urN linux-4.18.20/arch/x86/include/asm/asm.h linux-4.18.20-new/arch/x86/include/asm/asm.h
--- linux-4.18.20/arch/x86/include/asm/asm.h	2018-12-10 01:04:13.000606757 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/asm.h	2018-12-23 13:02:22.477703012 -0500
@@ -2,6 +2,30 @@
 #ifndef _ASM_X86_ASM_H
 #define _ASM_X86_ASM_H
 
+/* PIC modules require an indirection through GOT for
+   external symbols. _ASM_CALL() for internal functions
+   is optimized by replacing indirect calls with direct ones
+   preceded by 1-byte NOP paddings per a call site;
+   Similarly, _ASM_LEA_RIP() is optimized by replacing MOV
+   to LEA and is used to load symbol addresses on x86-64. */
+#if defined(MODULE) && defined(CONFIG_X86_PIC)
+# ifdef __ASSEMBLY__
+#  define _ASM_CALL(f)		call *##f##@GOTPCREL(%rip)
+#  define _ASM_LEA_RIP(v,a)	movq v##@GOTPCREL(%rip), a
+# else
+#  define _ASM_CALL(f)		"call *" #f "@GOTPCREL(%%rip)"
+#  define _ASM_LEA_RIP(v,a)	"movq " #v "@GOTPCREL(%%rip), " #a
+# endif
+#else
+# ifdef __ASSEMBLY__
+#  define _ASM_CALL(f)		call f
+#  define _ASM_LEA_RIP(v,a)	leaq v##(%rip), a
+# else
+#  define _ASM_CALL(f)		"call " #f
+#  define _ASM_LEA_RIP(v,a)	"leaq " #v "(%%rip), " #a
+# endif
+#endif
+
 #ifdef __ASSEMBLY__
 # define __ASM_FORM(x)	x
 # define __ASM_FORM_RAW(x)     x
@@ -122,10 +146,10 @@
 #ifdef __ASSEMBLY__
 # define _ASM_EXTABLE_HANDLE(from, to, handler)			\
 	.pushsection "__ex_table","a" ;				\
-	.balign 4 ;						\
+	_ASM_ALIGN ;						\
 	.long (from) - . ;					\
 	.long (to) - . ;					\
-	.long (handler) - . ;					\
+	_ASM_PTR (handler) - . ;				\
 	.popsection
 
 # define _ASM_EXTABLE(from, to)					\
@@ -174,10 +198,10 @@
 # define _EXPAND_EXTABLE_HANDLE(x) #x
 # define _ASM_EXTABLE_HANDLE(from, to, handler)			\
 	" .pushsection \"__ex_table\",\"a\"\n"			\
-	" .balign 4\n"						\
+	_ASM_ALIGN "\n"						\
 	" .long (" #from ") - .\n"				\
 	" .long (" #to ") - .\n"				\
-	" .long (" _EXPAND_EXTABLE_HANDLE(handler) ") - .\n"	\
+	_ASM_PTR " (" _EXPAND_EXTABLE_HANDLE(handler) ") - .\n"	\
 	" .popsection\n"
 
 # define _ASM_EXTABLE(from, to)					\
diff -urN linux-4.18.20/arch/x86/include/asm/elf.h linux-4.18.20-new/arch/x86/include/asm/elf.h
--- linux-4.18.20/arch/x86/include/asm/elf.h	2018-12-10 01:04:12.996606714 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/elf.h	2018-12-23 13:02:22.477703012 -0500
@@ -63,7 +63,12 @@
 #define R_X86_64_8		14	/* Direct 8 bit sign extended  */
 #define R_X86_64_PC8		15	/* 8 bit sign extended pc relative */
 
-#define R_X86_64_NUM		16
+#define R_X86_64_PC64		24	/* PC relative 64 bit */
+
+#define R_X86_64_GOTPCRELX	41
+#define R_X86_64_REX_GOTPCRELX	42
+
+#define R_X86_64_NUM		43
 
 /*
  * These are used to set parameters in the core dumps.
diff -urN linux-4.18.20/arch/x86/include/asm/extable.h linux-4.18.20-new/arch/x86/include/asm/extable.h
--- linux-4.18.20/arch/x86/include/asm/extable.h	2018-12-10 01:04:12.996606714 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/extable.h	2018-12-23 13:02:22.477703012 -0500
@@ -15,7 +15,8 @@
  */
 
 struct exception_table_entry {
-	int insn, fixup, handler;
+	int insn, fixup;
+	int handler, pad;
 };
 struct pt_regs;
 
diff -urN linux-4.18.20/arch/x86/include/asm/jump_label.h linux-4.18.20-new/arch/x86/include/asm/jump_label.h
--- linux-4.18.20/arch/x86/include/asm/jump_label.h	2018-12-10 01:04:13.000606757 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/jump_label.h	2018-12-23 13:02:22.477703012 -0500
@@ -37,7 +37,7 @@
 		".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t"
 		".pushsection __jump_table,  \"aw\" \n\t"
 		_ASM_ALIGN "\n\t"
-		_ASM_PTR "1b, %l[l_yes], %P0 \n\t"
+		_ASM_PTR "1b, %l[l_yes], %p0 \n\t"
 		".popsection \n\t"
 		: :  "X" (&((char *)key)[branch]) : : l_yes);
 
@@ -53,7 +53,7 @@
 		"2:\n\t"
 		".pushsection __jump_table,  \"aw\" \n\t"
 		_ASM_ALIGN "\n\t"
-		_ASM_PTR "1b, %l[l_yes], %P0 \n\t"
+		_ASM_PTR "1b, %l[l_yes], %p0 \n\t"
 		".popsection \n\t"
 		: :  "X" (&((char *)key)[branch]) : : l_yes);
 
diff -urN linux-4.18.20/arch/x86/include/asm/kvm_host.h linux-4.18.20-new/arch/x86/include/asm/kvm_host.h
--- linux-4.18.20/arch/x86/include/asm/kvm_host.h	2018-12-10 01:04:13.000606757 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/kvm_host.h	2018-12-23 13:02:22.477703012 -0500
@@ -1394,20 +1394,31 @@
  */
 asmlinkage void kvm_spurious_fault(void);
 
+#if defined(MODULE) && defined(CONFIG_X86_PIC)
+# define ___kvm_check_rebooting					\
+	"pushq %%rax \n\t"					\
+	"movq kvm_rebooting@GOTPCREL(%%rip), %%rax \n\t"	\
+	"cmpb $0, (%%rax) \n\t"					\
+	"popq %%rax \n\t"
+#else
+# define ___kvm_check_rebooting					\
+	"cmpb $0, kvm_rebooting" __ASM_SEL(,(%%rip)) " \n\t"
+#endif
+
 #define ____kvm_handle_fault_on_reboot(insn, cleanup_insn)	\
 	"666: " insn "\n\t" \
 	"668: \n\t"                           \
 	".pushsection .fixup, \"ax\" \n" \
 	"667: \n\t" \
 	cleanup_insn "\n\t"		      \
-	"cmpb $0, kvm_rebooting" __ASM_SEL(,(%%rip)) " \n\t" \
+	___kvm_check_rebooting			\
 	"jne 668b \n\t"      		      \
 	__ASM_SIZE(push) "$0 \n\t"		\
 	__ASM_SIZE(push) "%%" _ASM_AX " \n\t"		\
 	_ASM_MOVABS " $666b, %%" _ASM_AX "\n\t"	\
 	_ASM_MOV " %%" _ASM_AX ", " __ASM_SEL(4,8) "(%%" _ASM_SP ") \n\t" \
 	__ASM_SIZE(pop) "%%" _ASM_AX " \n\t"		\
-	"call kvm_spurious_fault \n\t"	      \
+	_ASM_CALL(kvm_spurious_fault) " \n\t"	      \
 	".popsection \n\t" \
 	_ASM_EXTABLE(666b, 667b)
 
diff -urN linux-4.18.20/arch/x86/include/asm/module.h linux-4.18.20-new/arch/x86/include/asm/module.h
--- linux-4.18.20/arch/x86/include/asm/module.h	2018-12-10 01:04:13.000606757 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/module.h	2018-12-23 13:02:22.477703012 -0500
@@ -5,13 +5,11 @@
 #include <asm-generic/module.h>
 #include <asm/orc_types.h>
 
-#ifdef CONFIG_X86_PIE
 struct mod_got_sec {
 	struct elf64_shdr	*got;
 	int			got_num_entries;
 	int			got_max_entries;
 };
-#endif
 
 struct mod_arch_specific {
 #ifdef CONFIG_UNWINDER_ORC
@@ -19,9 +17,7 @@
 	int *orc_unwind_ip;
 	struct orc_entry *orc_unwind;
 #endif
-#ifdef CONFIG_X86_PIE
 	struct mod_got_sec	core;
-#endif
 };
 
 #ifdef CONFIG_X86_64
diff -urN linux-4.18.20/arch/x86/include/asm/paravirt_types.h linux-4.18.20-new/arch/x86/include/asm/paravirt_types.h
--- linux-4.18.20/arch/x86/include/asm/paravirt_types.h	2018-12-10 01:04:12.996606714 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/paravirt_types.h	2018-12-23 13:02:22.481703137 -0500
@@ -337,7 +337,7 @@
 #define PARAVIRT_PATCH(x)					\
 	(offsetof(struct paravirt_patch_template, x) / sizeof(void *))
 
-#ifdef CONFIG_X86_PIE
+#if defined(CONFIG_X86_PIE) || (defined(MODULE) && defined(CONFIG_X86_PIC))
 #define paravirt_opptr_call "a"
 #define paravirt_opptr_type "p"
 #else
@@ -355,7 +355,11 @@
  * Generate some code, and mark it as patchable by the
  * apply_paravirt() alternate instruction patcher.
  */
-#define _paravirt_alt(insn_string, type, clobber)	\
+#if defined(MODULE) && defined(CONFIG_X86_PIC)
+# define _paravirt_alt(insn_string, type, clobber)	\
+	insn_string "\n"
+#else
+# define _paravirt_alt(insn_string, type, clobber)	\
 	"771:\n\t" insn_string "\n" "772:\n"		\
 	".pushsection .parainstructions,\"a\"\n"	\
 	_ASM_ALIGN "\n"					\
@@ -364,6 +368,7 @@
 	"  .byte 772b-771b\n"				\
 	"  .short " clobber "\n"			\
 	".popsection\n"
+#endif
 
 /* Generate patchable code, with the default asm parameters. */
 #define paravirt_alt(insn_string)					\
diff -urN linux-4.18.20/arch/x86/include/asm/percpu.h linux-4.18.20-new/arch/x86/include/asm/percpu.h
--- linux-4.18.20/arch/x86/include/asm/percpu.h	2018-12-10 01:04:12.996606714 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/percpu.h	2018-12-23 13:02:22.481703137 -0500
@@ -216,7 +216,7 @@
 })
 
 /* Position Independent code uses relative addresses only */
-#ifdef CONFIG_X86_PIE
+#if defined(CONFIG_X86_PIE) || (defined(MODULE) && defined(CONFIG_X86_PIC))
 #define __percpu_stable_arg __percpu_arg(a1)
 #else
 #define __percpu_stable_arg __percpu_arg(P1)
diff -urN linux-4.18.20/arch/x86/include/asm/uaccess.h linux-4.18.20-new/arch/x86/include/asm/uaccess.h
--- linux-4.18.20/arch/x86/include/asm/uaccess.h	2018-12-10 01:04:13.000606757 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/uaccess.h	2018-12-23 13:02:22.481703137 -0500
@@ -174,7 +174,7 @@
 	register __inttype(*(ptr)) __val_gu asm("%"_ASM_DX);		\
 	__chk_user_ptr(ptr);						\
 	might_fault();							\
-	asm volatile("call __get_user_%P4"				\
+	asm volatile(_ASM_CALL(__get_user_%P4)				\
 		     : "=a" (__ret_gu), "=r" (__val_gu),		\
 			ASM_CALL_CONSTRAINT				\
 		     : "0" (ptr), "i" (sizeof(*(ptr))));		\
@@ -183,7 +183,7 @@
 })
 
 #define __put_user_x(size, x, ptr, __ret_pu)			\
-	asm volatile("call __put_user_" #size : "=a" (__ret_pu)	\
+	asm volatile(_ASM_CALL(__put_user_##size) : "=a" (__ret_pu)	\
 		     : "0" ((typeof(*(ptr)))(x)), "c" (ptr) : "ebx")
 
 
@@ -213,7 +213,7 @@
 		     : : "A" (x), "r" (addr))
 
 #define __put_user_x8(x, ptr, __ret_pu)				\
-	asm volatile("call __put_user_8" : "=a" (__ret_pu)	\
+	asm volatile(_ASM_CALL(__put_user_8) : "=a" (__ret_pu)	\
 		     : "A" ((typeof(*(ptr)))(x)), "c" (ptr) : "ebx")
 #else
 #define __put_user_asm_u64(x, ptr, retval, errret) \
diff -urN linux-4.18.20/arch/x86/include/asm/xen/hypercall.h linux-4.18.20-new/arch/x86/include/asm/xen/hypercall.h
--- linux-4.18.20/arch/x86/include/asm/xen/hypercall.h	2018-12-10 01:04:13.000606757 -0500
+++ linux-4.18.20-new/arch/x86/include/asm/xen/hypercall.h	2018-12-23 13:10:04.423442394 -0500
@@ -88,9 +88,14 @@
 
 extern struct { char _entry[32]; } hypercall_page[];
 
-#define __HYPERCALL		"call hypercall_page+%c[offset]"
-#define __HYPERCALL_ENTRY(x)						\
+#if defined(MODULE) && defined(CONFIG_X86_PIC)
+# define __HYPERCALL(x)	"call *xen_hypercall_" #x "@GOTPCREL(%%rip)"
+# define __HYPERCALL_ENTRY(x)
+#else
+# define __HYPERCALL(x)	"call hypercall_page+%c[offset]"
+# define __HYPERCALL_ENTRY(x)						\
 	[offset] "i" (__HYPERVISOR_##x * sizeof(hypercall_page[0]))
+#endif
 
 #ifdef CONFIG_X86_32
 #define __HYPERCALL_RETREG	"eax"
@@ -146,7 +151,7 @@
 ({									\
 	__HYPERCALL_DECLS;						\
 	__HYPERCALL_0ARG();						\
-	asm volatile (__HYPERCALL					\
+	asm volatile (__HYPERCALL(name)					\
 		      : __HYPERCALL_0PARAM				\
 		      : __HYPERCALL_ENTRY(name)				\
 		      : __HYPERCALL_CLOBBER0);				\
@@ -157,7 +162,7 @@
 ({									\
 	__HYPERCALL_DECLS;						\
 	__HYPERCALL_1ARG(a1);						\
-	asm volatile (__HYPERCALL					\
+	asm volatile (__HYPERCALL(name)					\
 		      : __HYPERCALL_1PARAM				\
 		      : __HYPERCALL_ENTRY(name)				\
 		      : __HYPERCALL_CLOBBER1);				\
@@ -168,7 +173,7 @@
 ({									\
 	__HYPERCALL_DECLS;						\
 	__HYPERCALL_2ARG(a1, a2);					\
-	asm volatile (__HYPERCALL					\
+	asm volatile (__HYPERCALL(name)					\
 		      : __HYPERCALL_2PARAM				\
 		      : __HYPERCALL_ENTRY(name)				\
 		      : __HYPERCALL_CLOBBER2);				\
@@ -179,7 +184,7 @@
 ({									\
 	__HYPERCALL_DECLS;						\
 	__HYPERCALL_3ARG(a1, a2, a3);					\
-	asm volatile (__HYPERCALL					\
+	asm volatile (__HYPERCALL(name)					\
 		      : __HYPERCALL_3PARAM				\
 		      : __HYPERCALL_ENTRY(name)				\
 		      : __HYPERCALL_CLOBBER3);				\
@@ -190,7 +195,7 @@
 ({									\
 	__HYPERCALL_DECLS;						\
 	__HYPERCALL_4ARG(a1, a2, a3, a4);				\
-	asm volatile (__HYPERCALL					\
+	asm volatile (__HYPERCALL(name)					\
 		      : __HYPERCALL_4PARAM				\
 		      : __HYPERCALL_ENTRY(name)				\
 		      : __HYPERCALL_CLOBBER4);				\
@@ -201,7 +206,7 @@
 ({									\
 	__HYPERCALL_DECLS;						\
 	__HYPERCALL_5ARG(a1, a2, a3, a4, a5);				\
-	asm volatile (__HYPERCALL					\
+	asm volatile (__HYPERCALL(name)					\
 		      : __HYPERCALL_5PARAM				\
 		      : __HYPERCALL_ENTRY(name)				\
 		      : __HYPERCALL_CLOBBER5);				\
diff -urN linux-4.18.20/arch/x86/Kconfig linux-4.18.20-new/arch/x86/Kconfig
--- linux-4.18.20/arch/x86/Kconfig	2018-12-10 01:04:12.996606714 -0500
+++ linux-4.18.20-new/arch/x86/Kconfig	2018-12-23 13:02:22.481703137 -0500
@@ -2238,9 +2238,18 @@
 	select DYNAMIC_MODULE_BASE
 	select MODULE_REL_CRCS if MODVERSIONS
 
+config X86_PIC
+	bool
+	depends on X86_64
+	default y
+	select MODULE_REL_CRCS if MODVERSIONS
+	---help---
+	  Compile position-independent DLL modules which can
+          be placed anywhere in the 64-bit address space.
+
 config RANDOMIZE_BASE_LARGE
 	bool "Increase the randomization range of the kernel image"
-	depends on X86_64 && RANDOMIZE_BASE
+	depends on X86_64 && RANDOMIZE_BASE && X86_PIC
 	select X86_PIE
 	select X86_MODULE_PLTS if MODULES
 	default n
diff -urN linux-4.18.20/arch/x86/kernel/ftrace.c linux-4.18.20-new/arch/x86/kernel/ftrace.c
--- linux-4.18.20/arch/x86/kernel/ftrace.c	2018-12-10 01:04:13.008606842 -0500
+++ linux-4.18.20-new/arch/x86/kernel/ftrace.c	2018-12-23 13:02:22.481703137 -0500
@@ -144,13 +144,6 @@
 {
 	unsigned char replaced[MCOUNT_INSN_SIZE + 1];
 
-	/*
-	 * If PIE is not enabled default to the original approach to code
-	 * modification.
-	 */
-	if (!IS_ENABLED(CONFIG_X86_PIE))
-		return ftrace_modify_code_direct(ip, old_code, new_code);
-
 	ftrace_expected = old_code;
 
 	/* Ensure the instructions point to a call to the GOT */
@@ -159,9 +152,12 @@
 		return -EFAULT;
 	}
 
+	/*
+	 * For non-PIC code, default to the original approach to code
+	 * modification.
+	 */
 	if (memcmp(replaced, got_call_preinsn, sizeof(got_call_preinsn))) {
-		WARN_ONCE(1, "invalid function call");
-		return -EINVAL;
+		return ftrace_modify_code_direct(ip, old_code, new_code);
 	}
 
 	/*
diff -urN linux-4.18.20/arch/x86/kernel/machine_kexec_64.c linux-4.18.20-new/arch/x86/kernel/machine_kexec_64.c
--- linux-4.18.20/arch/x86/kernel/machine_kexec_64.c	2018-12-10 01:04:13.004606798 -0500
+++ linux-4.18.20-new/arch/x86/kernel/machine_kexec_64.c	2018-12-23 13:02:22.481703137 -0500
@@ -496,6 +496,10 @@
 			value -= (u64)address;
 			*(u32 *)location = value;
 			break;
+		case R_X86_64_PC64:
+			value -= (u64)address;
+			*(u64 *)location = value;
+			break;
 		default:
 			pr_err("Unknown rela relocation: %llu\n",
 			       ELF64_R_TYPE(rel[i].r_info));
diff -urN linux-4.18.20/arch/x86/kernel/module.c linux-4.18.20-new/arch/x86/kernel/module.c
--- linux-4.18.20/arch/x86/kernel/module.c	2018-12-10 01:04:13.004606798 -0500
+++ linux-4.18.20-new/arch/x86/kernel/module.c	2018-12-23 13:05:59.980604384 -0500
@@ -90,6 +90,12 @@
 
 	return 0;
 }
+#else
+static u64 find_got_kernel_entry(Elf64_Sym *sym, const Elf64_Rela *rela)
+{
+	return 0;
+}
+#endif
 
 static u64 module_emit_got_entry(struct module *mod, void *loc,
 				 const Elf64_Rela *rela, Elf64_Sym *sym)
@@ -111,7 +117,7 @@
 	 * relocations are sorted, this will be the last entry we allocated.
 	 * (if one exists).
 	 */
-	if (i > 0 && got[i] == got[i - 2]) {
+	if (i > 0 && got[i] == got[i - 1]) {
 		ret = (u64)&got[i - 1];
 	} else {
 		gotsec->got_num_entries++;
@@ -156,6 +162,8 @@
 
 	for (i = 0; i < num; i++) {
 		switch (ELF64_R_TYPE(rela[i].r_info)) {
+		case R_X86_64_REX_GOTPCRELX:
+		case R_X86_64_GOTPCRELX:
 		case R_X86_64_GOTPCREL:
 			s = syms + ELF64_R_SYM(rela[i].r_info);
 
@@ -243,7 +251,6 @@
 	}
 	return 0;
 }
-#endif
 
 void *module_alloc(unsigned long size)
 {
@@ -358,11 +365,11 @@
 			if ((s64)val != *(s32 *)loc)
 				goto overflow;
 			break;
-#ifdef CONFIG_X86_PIE
+		case R_X86_64_REX_GOTPCRELX:
+		case R_X86_64_GOTPCRELX:
 		case R_X86_64_GOTPCREL:
 			val = module_emit_got_entry(me, loc, rel + i, sym);
 			/* fallthrough */
-#endif
 		case R_X86_64_PC32:
 		case R_X86_64_PLT32:
 			if (*(u32 *)loc != 0)
@@ -373,6 +380,12 @@
 			    (s64)val != *(s32 *)loc)
 				goto overflow;
 			break;
+		case R_X86_64_PC64:
+			if (*(u64 *)loc != 0)
+				goto invalid_relocation;
+			val -= (u64)loc;
+			*(u64 *)loc = val;
+			break;
 		default:
 			pr_err("%s: Unknown rela relocation: %llu\n",
 			       me->name, ELF64_R_TYPE(rel[i].r_info));
diff -urN linux-4.18.20/arch/x86/kvm/emulate.c linux-4.18.20-new/arch/x86/kvm/emulate.c
--- linux-4.18.20/arch/x86/kvm/emulate.c	2018-12-10 01:04:13.008606842 -0500
+++ linux-4.18.20-new/arch/x86/kvm/emulate.c	2018-12-23 13:02:22.485703262 -0500
@@ -428,7 +428,6 @@
 	FOP_RET
 
 asm(".pushsection .fixup, \"ax\"\n"
-    ".global kvm_fastop_exception \n"
     "kvm_fastop_exception: xor %esi, %esi; ret\n"
     ".popsection");
 
diff -urN linux-4.18.20/arch/x86/Makefile linux-4.18.20-new/arch/x86/Makefile
--- linux-4.18.20/arch/x86/Makefile	2018-12-10 01:04:12.996606714 -0500
+++ linux-4.18.20-new/arch/x86/Makefile	2018-12-23 13:02:22.485703262 -0500
@@ -136,6 +136,12 @@
         KBUILD_CFLAGS += $(cflags-y)
 
         KBUILD_CFLAGS += -mno-red-zone
+
+ifdef CONFIG_X86_PIC
+        KBUILD_CFLAGS_MODULE += -fPIC -fno-plt -mcmodel=small -fno-stack-protector -fvisibility=hidden
+endif
+        KBUILD_LDFLAGS_MODULE += -T $(srctree)/arch/x86/kernel/module.lds
+
 ifdef CONFIG_X86_PIE
         KBUILD_CFLAGS += -fPIE
         KBUILD_LDFLAGS_MODULE += -T $(srctree)/arch/x86/kernel/module.lds
diff -urN linux-4.18.20/arch/x86/tools/relocs.c linux-4.18.20-new/arch/x86/tools/relocs.c
--- linux-4.18.20/arch/x86/tools/relocs.c	2018-12-10 01:04:13.012606883 -0500
+++ linux-4.18.20-new/arch/x86/tools/relocs.c	2018-12-23 13:02:22.485703262 -0500
@@ -203,6 +203,7 @@
 		REL_TYPE(R_X86_64_NONE),
 		REL_TYPE(R_X86_64_64),
 		REL_TYPE(R_X86_64_PC32),
+		REL_TYPE(R_X86_64_PC64),
 		REL_TYPE(R_X86_64_GOT32),
 		REL_TYPE(R_X86_64_PLT32),
 		REL_TYPE(R_X86_64_COPY),
@@ -210,6 +211,8 @@
 		REL_TYPE(R_X86_64_JUMP_SLOT),
 		REL_TYPE(R_X86_64_RELATIVE),
 		REL_TYPE(R_X86_64_GOTPCREL),
+		REL_TYPE(R_X86_64_REX_GOTPCRELX),
+		REL_TYPE(R_X86_64_GOTPCRELX),
 		REL_TYPE(R_X86_64_32),
 		REL_TYPE(R_X86_64_32S),
 		REL_TYPE(R_X86_64_16),
@@ -866,6 +869,8 @@
 		offset += per_cpu_load_addr;
 
 	switch (r_type) {
+	case R_X86_64_REX_GOTPCRELX:
+	case R_X86_64_GOTPCRELX:
 	case R_X86_64_GOTPCREL:
 	case R_X86_64_NONE:
 		/* NONE can be ignored. */
@@ -881,6 +886,14 @@
 		 */
 		if (is_percpu_sym(sym, symname))
 			add_reloc(&relocs32neg, offset);
+
+		/* fallthrough */
+
+	case R_X86_64_PC64:
+		/*
+		 * PC64 relocations are currently found only in
+		 * __ex_table and don't need to be adjusted.
+		 */
 		break;
 
 	case R_X86_64_32:
diff -urN linux-4.18.20/arch/x86/xen/xen-head.S linux-4.18.20-new/arch/x86/xen/xen-head.S
--- linux-4.18.20/arch/x86/xen/xen-head.S	2018-12-10 01:04:13.016606927 -0500
+++ linux-4.18.20-new/arch/x86/xen/xen-head.S	2018-12-23 13:11:59.957110663 -0500
@@ -13,6 +13,7 @@
 #include <asm/page_types.h>
 #include <asm/percpu.h>
 #include <asm/unwind_hints.h>
+#include <asm/export.h>
 
 #include <xen/interface/elfnote.h>
 #include <xen/interface/features.h>
@@ -66,6 +67,7 @@
 	.endr
 
 #define HYPERCALL(n) \
+	EXPORT_SYMBOL_GPL(xen_hypercall_##n); \
 	.equ xen_hypercall_##n, hypercall_page + __HYPERVISOR_##n * 32; \
 	.type xen_hypercall_##n, @function; .size xen_hypercall_##n, 32
 #include <asm/xen-hypercalls.h>
diff -urN linux-4.18.20/lib/zstd/entropy_common_dec.c linux-4.18.20-new/lib/zstd/entropy_common_dec.c
--- linux-4.18.20/lib/zstd/entropy_common_dec.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/lib/zstd/entropy_common_dec.c	2018-12-23 13:02:22.485703262 -0500
@@ -0,0 +1 @@
+#include "entropy_common.c"
diff -urN linux-4.18.20/lib/zstd/fse_decompress_dec.c linux-4.18.20-new/lib/zstd/fse_decompress_dec.c
--- linux-4.18.20/lib/zstd/fse_decompress_dec.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/lib/zstd/fse_decompress_dec.c	2018-12-23 13:02:22.485703262 -0500
@@ -0,0 +1 @@
+#include "fse_decompress.c"
diff -urN linux-4.18.20/lib/zstd/Makefile linux-4.18.20-new/lib/zstd/Makefile
--- linux-4.18.20/lib/zstd/Makefile	2018-12-10 01:04:12.516601611 -0500
+++ linux-4.18.20-new/lib/zstd/Makefile	2018-12-23 13:02:22.485703262 -0500
@@ -6,4 +6,4 @@
 zstd_compress-y := fse_compress.o huf_compress.o compress.o \
 		   entropy_common.o fse_decompress.o zstd_common.o
 zstd_decompress-y := huf_decompress.o decompress.o \
-		     entropy_common.o fse_decompress.o zstd_common.o
+		     entropy_common_dec.o fse_decompress_dec.o zstd_common_dec.o
diff -urN linux-4.18.20/lib/zstd/zstd_common_dec.c linux-4.18.20-new/lib/zstd/zstd_common_dec.c
--- linux-4.18.20/lib/zstd/zstd_common_dec.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-4.18.20-new/lib/zstd/zstd_common_dec.c	2018-12-23 13:02:22.485703262 -0500
@@ -0,0 +1 @@
+#include "zstd_common.c"
diff -urN linux-4.18.20/scripts/recordmcount.c linux-4.18.20-new/scripts/recordmcount.c
--- linux-4.18.20/scripts/recordmcount.c	2018-12-10 01:04:12.568602166 -0500
+++ linux-4.18.20-new/scripts/recordmcount.c	2018-12-23 13:02:22.489703387 -0500
@@ -453,7 +453,8 @@
 /* Swap the stub and nop for a got call if the binary is built with PIE */
 static int is_fake_mcount_x86_x64(Elf64_Rel const *rp)
 {
-	if (ELF64_R_TYPE(rp->r_info) == R_X86_64_GOTPCREL) {
+	if (ELF64_R_TYPE(rp->r_info) == R_X86_64_GOTPCREL ||
+	    ELF64_R_TYPE(rp->r_info) == R_X86_64_GOTPCRELX) {
 		ideal_nop = ideal_nop6_x86_64;
 		ideal_nop_x86_size = sizeof(ideal_nop6_x86_64);
 		stub_x86 = stub_got_x86;
diff -urN linux-4.18.20/scripts/sortextable.c linux-4.18.20-new/scripts/sortextable.c
--- linux-4.18.20/scripts/sortextable.c	2018-12-10 01:04:12.568602166 -0500
+++ linux-4.18.20-new/scripts/sortextable.c	2018-12-23 13:02:22.489703387 -0500
@@ -238,6 +238,37 @@
 	}
 }
 
+static void x86_64_sort_relative_table(char *extab_image, int image_size)
+{
+	int i;
+
+	i = 0;
+	while (i < image_size) {
+		uint32_t *loc = (uint32_t *)(extab_image + i);
+
+		w(r(loc) + i, loc);
+		w(r(loc + 1) + i + 4, loc + 1);
+		w(r(loc + 2) + i + 8, loc + 2);
+		w(r(loc + 3) + i + 12, loc + 3);
+
+		i += sizeof(uint32_t) * 4;
+	}
+
+	qsort(extab_image, image_size / 16, 16, compare_relative_table);
+
+	i = 0;
+	while (i < image_size) {
+		uint32_t *loc = (uint32_t *)(extab_image + i);
+
+		w(r(loc) - i, loc);
+		w(r(loc + 1) - (i + 4), loc + 1);
+		w(r(loc + 2) - (i + 8), loc + 2);
+		w(r(loc + 3) - (i + 12), loc + 3);
+
+		i += sizeof(uint32_t) * 4;
+	}
+}
+
 static void sort_relative_table(char *extab_image, int image_size)
 {
 	int i;
@@ -309,9 +340,11 @@
 		fail_file();
 		break;
 	case EM_386:
-	case EM_X86_64:
 		custom_sort = x86_sort_relative_table;
 		break;
+	case EM_X86_64:
+		custom_sort = x86_64_sort_relative_table;
+		break;
 
 	case EM_S390:
 	case EM_AARCH64:
diff -urN linux-4.18.20/tools/objtool/check.c linux-4.18.20-new/tools/objtool/check.c
--- linux-4.18.20/tools/objtool/check.c	2018-12-10 01:04:12.624602762 -0500
+++ linux-4.18.20-new/tools/objtool/check.c	2018-12-23 13:02:22.489703387 -0500
@@ -179,7 +179,7 @@
 		return 0;
 
 	insn = find_insn(file, func->sec, func->offset);
-	if (!insn->func)
+	if (!insn || !insn->func)
 		return 0;
 
 	func_for_each_insn_all(file, func, insn) {
@@ -233,6 +233,8 @@
 
 static int dead_end_function(struct objtool_file *file, struct symbol *func)
 {
+	if (!func)
+		return 0;
 	return __dead_end_function(file, func, 0);
 }
 
@@ -581,7 +583,7 @@
 	struct rela *rela;
 
 	for_each_insn(file, insn) {
-		if (insn->type != INSN_CALL)
+		if (insn->type != INSN_CALL && insn->type != INSN_CALL_DYNAMIC)
 			continue;
 
 		rela = find_rela_by_dest_range(insn->sec, insn->offset,
@@ -590,8 +592,8 @@
 			dest_off = insn->offset + insn->len + insn->immediate;
 			insn->call_dest = find_symbol_by_offset(insn->sec,
 								dest_off);
-
-			if (!insn->call_dest && !insn->ignore) {
+			if (!insn->call_dest && !insn->ignore &&
+			    insn->type != INSN_CALL_DYNAMIC) {
 				WARN_FUNC("unsupported intra-function call",
 					  insn->sec, insn->offset);
 				if (retpoline)
@@ -602,8 +604,9 @@
 		} else if (rela->sym->type == STT_SECTION) {
 			insn->call_dest = find_symbol_by_offset(rela->sym->sec,
 								rela->addend+4);
-			if (!insn->call_dest ||
-			    insn->call_dest->type != STT_FUNC) {
+			if ((!insn->call_dest ||
+			     insn->call_dest->type != STT_FUNC) &&
+			    insn->type != INSN_CALL_DYNAMIC) {
 				WARN_FUNC("can't find call dest symbol at %s+0x%x",
 					  insn->sec, insn->offset,
 					  rela->sym->sec->name,
@@ -836,6 +839,11 @@
 	struct symbol *pfunc = insn->func->pfunc;
 	unsigned int prev_offset = 0;
 
+	/* If PC32 relocations are used (as in PIC), the following logic
+	   can be broken in many ways. */
+	if (file->ignore_unreachables)
+		return 0;
+
 	list_for_each_entry_from(rela, &file->rodata->rela->rela_list, list) {
 		if (rela == next_table)
 			break;
@@ -1244,7 +1252,7 @@
 
 static bool is_fentry_call(struct instruction *insn)
 {
-	if (insn->type == INSN_CALL &&
+	if (insn->call_dest &&
 	    insn->call_dest->type == STT_NOTYPE &&
 	    !strcmp(insn->call_dest->name, "__fentry__"))
 		return true;
@@ -1889,6 +1897,7 @@
 			return 0;
 
 		case INSN_CALL:
+		case INSN_CALL_DYNAMIC:
 			if (is_fentry_call(insn))
 				break;
 
@@ -1898,8 +1907,6 @@
 			if (ret == -1)
 				return 1;
 
-			/* fallthrough */
-		case INSN_CALL_DYNAMIC:
 			if (!no_fp && func && !has_valid_stack_frame(&state)) {
 				WARN_FUNC("call without frame pointer save/setup",
 					  sec, insn->offset);
@@ -1929,12 +1936,15 @@
 			break;
 
 		case INSN_JUMP_DYNAMIC:
+			/* XXX: Does not work properly with PIC code. */
+#if 0
 			if (func && list_empty(&insn->alts) &&
 			    has_modified_stack_frame(&state)) {
 				WARN_FUNC("sibling call from callable instruction with modified stack frame",
 					  sec, insn->offset);
 				return 1;
 			}
+#endif
 
 			return 0;
 
@@ -2015,6 +2025,11 @@
 		if (!strcmp(insn->sec->name, ".init.text") && !module)
 			continue;
 
+		/* ignore ftrace calls in PIC code */
+		if (!insn->call_dest ||
+		    !strcmp(insn->call_dest->name, "__fentry__"))
+			continue;
+
 		WARN_FUNC("indirect %s found in RETPOLINE build",
 			  insn->sec, insn->offset,
 			  insn->type == INSN_JUMP_DYNAMIC ? "jump" : "call");
@@ -2027,13 +2042,15 @@
 
 static bool is_kasan_insn(struct instruction *insn)
 {
-	return (insn->type == INSN_CALL &&
+	return ((insn->type == INSN_CALL || insn->type == INSN_CALL_DYNAMIC) &&
+		insn->call_dest &&
 		!strcmp(insn->call_dest->name, "__asan_handle_no_return"));
 }
 
 static bool is_ubsan_insn(struct instruction *insn)
 {
-	return (insn->type == INSN_CALL &&
+	return ((insn->type == INSN_CALL || insn->type == INSN_CALL_DYNAMIC) &&
+		insn->call_dest &&
 		!strcmp(insn->call_dest->name,
 			"__ubsan_handle_builtin_unreachable"));
 }
diff -urN linux-4.18.20/tools/objtool/special.c linux-4.18.20-new/tools/objtool/special.c
--- linux-4.18.20/tools/objtool/special.c	2018-12-10 01:04:12.624602762 -0500
+++ linux-4.18.20-new/tools/objtool/special.c	2018-12-23 13:06:35.557749951 -0500
@@ -27,6 +27,7 @@
 #include "warn.h"
 
 #define EX_ENTRY_SIZE		12
+#define EX_ENTRY_SIZE64		16
 #define EX_ORIG_OFFSET		0
 #define EX_NEW_OFFSET		4
 
@@ -78,6 +79,33 @@
 	{},
 };
 
+struct special_entry entries64[] = {
+	{
+		.sec = ".altinstructions",
+		.group = true,
+		.size = ALT_ENTRY_SIZE,
+		.orig = ALT_ORIG_OFFSET,
+		.orig_len = ALT_ORIG_LEN_OFFSET,
+		.new = ALT_NEW_OFFSET,
+		.new_len = ALT_NEW_LEN_OFFSET,
+		.feature = ALT_FEATURE_OFFSET,
+	},
+	{
+		.sec = "__jump_table",
+		.jump_or_nop = true,
+		.size = JUMP_ENTRY_SIZE,
+		.orig = JUMP_ORIG_OFFSET,
+		.new = JUMP_NEW_OFFSET,
+	},
+	{
+		.sec = "__ex_table",
+		.size = EX_ENTRY_SIZE64,
+		.orig = EX_ORIG_OFFSET,
+		.new = EX_NEW_OFFSET,
+	},
+	{},
+};
+
 static int get_alt_entry(struct elf *elf, struct special_entry *entry,
 			 struct section *sec, int idx,
 			 struct special_alt *alt)
@@ -152,7 +180,7 @@
  */
 int special_get_alts(struct elf *elf, struct list_head *alts)
 {
-	struct special_entry *entry;
+	struct special_entry *entry = entries;
 	struct section *sec;
 	unsigned int nr_entries;
 	struct special_alt *alt;
@@ -160,7 +188,11 @@
 
 	INIT_LIST_HEAD(alts);
 
-	for (entry = entries; entry->sec; entry++) {
+	/* ex_table entry size is different for x86-64 */
+	if (elf->ehdr.e_machine == EM_X86_64)
+		entry = entries64;
+
+	for (; entry->sec; entry++) {
 		sec = find_section_by_name(elf, entry->sec);
 		if (!sec)
 			continue;
